{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Case Study\n",
    "\n",
    "\n",
    "#### Steven Hayden, Kevin Mendonsa, Joe Schueder, Nicole Wittlin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary Framework\n",
    "\n",
    "Intro?\n",
    "Business Imperative?\n",
    "Put those here??\n",
    "\n",
    "\n",
    "Key Findings About Data Set/Observations \n",
    "- Info about \n",
    "- Wed most observations 100K, Tues & Thurs approx 30K\n",
    "- July most observations, close second in June; August and May close. Dec, Jan, Feb minimal. possible seaonal pattern\n",
    "- Asia 139K observations, \n",
    "- x32 a continuous varible as a percentage but low cardinality with 11 unique values, opted to treat as categorical rather than continuous\n",
    "- Distribution \n",
    "\n",
    "Baseline Model \n",
    "- Log Reg, what were the criteria, results\n",
    "\n",
    "Other Models Run\n",
    "\n",
    "Summary Chart of Costs from Excel\n",
    "\n",
    "Conclusion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction \n",
    "\n",
    "We have been provided a dataset comprising 50 explanatory variables and one response variable with the objective of minimizing cost of the predicted response using provided cost penalties to the business. The dataset contains 160,000 observations. Given the lack of domain knowledge and intrinsic information about the dataset, we will conduct a comprehensive exploratory data analysis (EDA) to understand the charactersitics of the data and the variables. The response variable \"y\" is binary (1,0). \n",
    "\n",
    "No additional information has been provided and therefore the data analysis and processing of the information is critical for delivering optimal results.  We will clearly articulate any assumptions we make in order to reach our conclusions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Imperative\n",
    "The business imperative for this analysis is to reduce False Negatives as a priority as they appear to have a greater adverse impact to the business in question.  False Positives while important are 50 times less impactful than False Negatives.\n",
    "\n",
    "We have therefore been informed that each False Positive prediction will cost the business \\\\$10 and each False Negative prediction will cost the business \\\\$500. True Positives and True Negatives have no effect on the business cost. Given that False Negatives have a higher cost penalty of (\\\\$500), Recall scores are critical in this analysis. Precision is also important with a cost penalty of \\\\$10. Correct predictions are \"zero cost\". \n",
    "\n",
    "The delivered model must be able to be deployed in a production environment and be generalized for future application of new data.  We will methodically step through various steps from cursory data review to an in-dept data analysis, baselining, modeling, dimensionality reduction, optimization and tuning and conclusions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and organization of our analysis\n",
    "Our analysis will step through the following stages building upon the conclusions and decisions of previous findings as we transition to an optimal conclusion to support the business imperative for this analysis.\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "- Data classes of features\n",
    "- Validate Normality assumptions\n",
    "- Check Cardinality\n",
    "- Missing Data\n",
    "- Validate Independence assumptions\n",
    "    - Pearson's Correlation\n",
    "    - Phi K Correlation\n",
    "- Outliers\n",
    "\n",
    "\n",
    "### Preprocessing\n",
    "- **Data Cleanup**\n",
    "    - Renaming values\n",
    "    - Stripping out special characters\n",
    "    - Conversion of data classes - String to float\n",
    "  \n",
    "\n",
    "- **Imputation Strategy to address missing data**\n",
    "    - Impute or drop\n",
    "    \n",
    "    \n",
    "- **One-hot Encoding of categorical variables**\n",
    "\n",
    "\n",
    "- **Standardization**\n",
    "    - MinMax scaling\n",
    "    \n",
    "    \n",
    "- **Balancing**\n",
    "    \n",
    "    \n",
    "- **Sampling strategy for testing and training splits**\n",
    "\n",
    "\n",
    "### Establish a Baseline\n",
    "- Modeling using diverse machine learning model algorithms (rationale for models selected)\n",
    "- Develop a baseline Cost-Benefit Matrix based on provided cost penalties\n",
    " \n",
    "###  Dimensionality Reduction (Feature Reduction)\n",
    "- Recursive feature extraction (Dimensionality Reduction - Occams's Razor)\n",
    " \n",
    "###  Cross Validation using K-fold (Model generalization and optimization)\n",
    " \n",
    "###  Parameter Tuning and Optimization\n",
    "- RandomizedSearchCV to choose best parameters\n",
    "\n",
    "### Modeling with Reduced Features\n",
    " \n",
    "###  Ensembling\n",
    "- Method using multiple learning algorithms to derive improved predictive performance than a single algorithm.\n",
    " \n",
    "###  Error Metrics\n",
    "- ROC-AUC\n",
    "- Recall\n",
    "- Precision\n",
    "- F1 scores\n",
    "- Confusion Matrix - Model accuracy and predictive power\n",
    "\n",
    "###   Conclusion and findings\n",
    "- Cost/Benefits Matrix - Translate into business terms/discovery\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFECV\n",
    "from vecstack import stacking\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.plotting import plot_learning_curves, plot_decision_regions\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Load the data to the workspace\n",
    "\n",
    "- Load the data as a data frame and conduct cursory analysis to review\n",
    "    - Shape (rows and columns)\n",
    "    - Variable Classes (datatypes)\n",
    "    - Missingness of data (NULL data)\n",
    "    - Unique values for Cardinality\n",
    "\n",
    "For clarity and consistency we will refer to Explanatory variables as Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "%time Business_Data = pd.read_csv('final_project.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "Business_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check data type and not null counts of all the columns\n",
    "Business_Data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 categorical features and 45 continuous features of float64 data type.  The response variable is binary and classed as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas Profiling to do a quick EDA \n",
    "# (Executing Mini version due to performance and memory constraints)\n",
    "profileMin = ProfileReport(Business_Data, minimal=True)\n",
    "profileMin.to_notebook_iframe()\n",
    "# profileMin.to_file(output_file=\"output_min.html\") # save profiling report to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Normality Assumption\n",
    "In reviewing the results of the Pandas profiling report above, we can see that the continuous features meet the assumptions for normality with well defined Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Missing data (NULLs) - Analysis by feature for determining imputation strategy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get NULL counts by column\n",
    "Business_Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- TOTAL NULL counts of all features: **1608**\n",
    "- Percentage of NULL values against total data: 1608/160000 => **1.005%**\n",
    "\n",
    "#### The Imputation strategy and actions are listed further below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inspect all features for Cardinality (Unique classes per variable)\n",
    "This will enable the identification of cardinality in features and also identify candidates for conversion to a categorical data class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine Unique values by feature\n",
    "Business_Data.nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closer review of the categorical features identified above\n",
    "- A closer analysis reveals that features **x32** and **x37** have special characters. \n",
    "- Additionally, feature **x32** has **low cardinality** with only 13 unique classes in 160,000 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review and analyze the Categorical features and response as indicated above\n",
    "print('Unique classes of feature x24:: \\n',Business_Data.x24.value_counts(dropna = False))\n",
    "print('\\nUnique values of feature x29:: \\n',Business_Data.x29.value_counts(dropna = False))\n",
    "print('\\nUnique values of feature x30:: \\n',Business_Data.x30.value_counts(dropna = False))\n",
    "print('\\nUnique values of feature x32:: \\n',Business_Data.x32.value_counts(dropna = False))\n",
    "print('\\nUnique values of response y:: \\n',Business_Data.y.value_counts(dropna = False))\n",
    "\n",
    "Business_Data.x37.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "We used Pearson's correlation to ensure the assumptions of independence of features are met. The correlation plot identifies attributes in the original dataset that are highly collinear. In other words, one predictor feature in the regression model can be linearly predicted from the others with a substantial degree of accuracy.\n",
    "\n",
    "\n",
    "### Pearson's Correlation\n",
    "\n",
    "The Pearson correlation coefficient is used to measure the strength of a linear association between two variables, where the value r = 1 means a perfect positive correlation and the value r = -1 means a perfect negataive correlation. It inofrms whether a statistically significant linear relationship exists between two continuous variables. The strength of a linear relationship (i.e., how close the relationship is to being a perfectly straight line) \n",
    "\n",
    "(Reference: https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/)\n",
    "\n",
    "Interpretation of Pearson's coefficient.\n",
    "* Exactly –1. A perfect downhill (negative) linear relationship\n",
    "\n",
    "* –0.70. A strong downhill (negative) linear relationship\n",
    "\n",
    "* –0.50. A moderate downhill (negative) relationship\n",
    "\n",
    "* –0.30. A weak downhill (negative) linear relationship\n",
    "\n",
    "* 0. No linear relationship\n",
    "\n",
    "* +0.30. A weak uphill (positive) linear relationship\n",
    "\n",
    "* +0.50. A moderate uphill (positive) relationship\n",
    "\n",
    "* +0.70. A strong uphill (positive) linear relationship\n",
    "\n",
    "* Exactly +1. A perfect uphill (positive) linear relationship\n",
    "\n",
    "![title](img/PearsonCorrelation2.png) \n",
    "\n",
    "Based on the analysis of the correlation matrix above, features $x6$ and $x41$ were excluded as they were highly correlated with $x2$ and $x38$ respectively with a correleation value = 1. Therefore, we will drop these variables from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features 'x6', 'x41' due to high correlation as identified above\n",
    "Project_Data_final = Business_Data.drop(['x6', 'x41'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "### Clean-up of categorical features - x32 and x37\n",
    "As identified above, we need to clean feature x37 by stripping it off special characters and converting it to a class \"float\".\n",
    "\n",
    "#### Actions:\n",
    "- Strip feature **x37** off all special characters \n",
    "- Convert feature **x37** to a float data class\n",
    "- Retain feature **x32** as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip special characters in feature x37\n",
    "# Convert feature x37 to float data class\n",
    "Business_Data['x37'] = Business_Data['x37'].str.replace('$','').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Business_Data.x32.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature x32 class distribution\n",
    "# Business_Data['x32'] = Business_Data['x32'].astype(str)\n",
    "Business_Data.x32.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Business_Data.x37.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation strategy\n",
    "\n",
    "Impute the nulls with appropriate values after determining if they are \n",
    "- Missing Completely At Random (MCAR)\n",
    "- Missing At Random (MAR)\n",
    "- Missing Not At Random (MNAR)\n",
    "\n",
    "After discussions with the business, it has been determined that the missing data is to be treated as Missing At Random (MAR).  Furthermore, since almost all the continuous features have a gaussian distribution and meet the assumptions for normality (refer to Pandas Profiling report for additional details), it was agreed to replace NULL values in continuous features with the \"median\" of the respective feature.  For missing values or NANs in categorical values we will proceed with dropping the records as they represent just **0.07% of the 160,000** records. \n",
    "\n",
    "#### Analysis:\n",
    "- TOTAL NULL counts of all features: **1608**\n",
    "- Percentage of total data: 1608/160000 => **1.005%**\n",
    "\n",
    "#### Actions:\n",
    "- Impute missing values of continuous features with the **median**\n",
    "- Drop missing values for categorical features given the large dataset of 160000 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame and replace null values as \n",
    "# per the imputation strategy agreed with the business.\n",
    "# Replace missing values in continuous features using the median\n",
    "Business_Data_imputed = Business_Data.apply(lambda x: x if x.dtype == 'object' else x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Business_Data_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NULL observations in categorical features\n",
    "\n",
    "Given the extremely low count of NULLs in the categorical features, dropping these observations still retain 159912 of the 160000 rows. This is approximately **0.07%** of the original data set.  It should likely have little to no impact on the overall analysis.\n",
    "\n",
    "- x24 - $28$\n",
    "- x29 - $30$\n",
    "- x30 - $30$\n",
    "- x32 - $31$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NULLS in categorical features\n",
    "Business_Data_imputed.dropna(inplace=True)\n",
    "\n",
    "# Review the data post dropping the NaNs in categorical features\n",
    "Business_Data_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up classes in some categorical features for consistency and spelling errors\n",
    "Business_Data_imputed['x24'] = Business_Data_imputed['x24'].replace(['asia', 'euorpe','america'], ['Asia', 'Europe','America'])\n",
    "Business_Data_imputed['x29'] = Business_Data_imputed['x29'].replace(['sept.', 'January','Dev','July'], ['Sep', 'Jan','Dec','Jul'])\n",
    "Business_Data_imputed['x30'] = Business_Data_imputed['x30'].replace(['monday', 'tuesday','wednesday','thurday','friday'], ['Mon', 'Tue','Wed','Thu','Fri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique classes and counts for categorical variables\n",
    "print('Unique values of Business_Data x24:: \\n',Business_Data_imputed.x24.value_counts(dropna = False))\n",
    "print('\\nUnique values of Business_Data x29:: \\n',Business_Data_imputed.x29.value_counts(dropna = False))\n",
    "print('\\nUnique values of Business_Data x30:: \\n',Business_Data_imputed.x30.value_counts(dropna = False))\n",
    "print('\\nUnique values of Business_Data x32:: \\n',Business_Data_imputed.x32.value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot encoding for Categorical Features\n",
    "\n",
    "One Hot Encoding (ohe) is a popular approach in which a categorical feature is converted to a format that enhances the predictive capability of ML algorithms. It works best when the cardinality of the categorical feature is low but is not advisable for features where there are more than 15 different classes. \n",
    "\n",
    "\n",
    "![title](img/OneHotEncoding2.png)\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "\n",
    "#### Parameter options used for pd.get_dummies\n",
    "- data - Data of which to get dummy indicators \n",
    "    - Parameter setting: **Project_Data_imputed**\n",
    "- prefix_sep - If appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix.\n",
    "    - Parameter setting: **\"_\"**\n",
    "- drop_first - Whether to get k-1 dummies out of k categorical levels by removing the first level\n",
    "    - Parameter setting: **True**\n",
    "- columns - Names as list of Categorical features for one hot encoding.\n",
    "    - Parameter setting: **cat_columns** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable \"cat_columns\" to include all features to be encoded\n",
    "cat_columns = ['x24', 'x29', 'x30','x32']\n",
    "\n",
    "# Use the pandas get_dummies function to encode the selected categorical features\n",
    "Business_Data_ohe = pd.get_dummies(Business_Data_imputed, prefix_sep=\"_\", drop_first=True, columns=cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the results of the encoding operation\n",
    "Business_Data_ohe.isnull().sum()\n",
    "Business_Data_ohe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the columns in the dataset\n",
    "Business_Data_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot of variables\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = Business_Data_ohe[['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
    "       'x11', 'x12', 'x13', 'x14', 'x15']].boxplot()\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "ax = Business_Data_ohe[['x16', 'x17', 'x18', 'x19', 'x20',\n",
    "       'x21', 'x22', 'x23', 'x25', 'x26', 'x27', 'x28', 'x31', 'x33', 'x34']].boxplot()\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "ax = Business_Data_ohe[['x37']].boxplot()\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "ax = Business_Data_ohe[['x35', 'x36', 'x38', 'x39', 'x40', 'x41', 'x42', 'x43', 'x44',\n",
    "       'x45', 'x46', 'x47', 'x48', 'x49']].boxplot()\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the box plots above we see very little skewness in the data and they appear to be normally distributed.  However, there are outliers that need to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Standardization - Scaling of the data\n",
    "\n",
    "A pre-processing step applied to independent features in a dataset. Primarily it normalizes the data within a particular range. It may also aid in accelerating the computations of an algorithm. The Features have been scaled to a mean of 0 and variance of 1 to improve the accuracy of the classification models.\n",
    "\n",
    "- *fit_transform* within MinMaxScaler() function fits the data and then transforms it.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the MinMaxScaler to the variable scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Perform the scaling using the scaler\n",
    "Business_Data_ohe_scaled = pd.DataFrame(scaler.fit_transform(Business_Data_ohe), columns=Business_Data_ohe.columns)\n",
    "\n",
    "# Review the data post scaling\n",
    "Business_Data_ohe_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Balancing\n",
    "\n",
    "Looking at the distribution of the outcomes (response variable) in our dataset, we can see that almost 60% of the data in our set has a response of \"0\" while the outcome 1 makes up 40%. When we run our classifier, we will need to be careful to account for the slightly unbalanced nature of our classifiers to help ensure we aren't overclassifying the majority labels.\n",
    "\n",
    "![title](img/ResponseVariable.png)\n",
    "\n",
    "We will address the balancing of the test and train datasets in our sampling below.  We will split the the test and train datasets by using the \"stratify\" option and will validate that they are balanced as part of the pre-processing of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sampling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test Split\n",
    "\n",
    "This technique provides us a way of voluntarily holding back part of the data to test whether the model works. If we use our entire dataset to train the model, then the model will alway predict the correct category for any entry in the data set. This process of setting aside a part of the dataset voluntarily to evaluate the model stops the model from being too optimistic when predicting the outcome. Training the model on the entire data set could also lead to data snooping bias. This kind of bias results from refining too many parameters to improve the model's performance on a specific data set. In our analysis, we will split the data into training and test data sets (75:25) randomly. We will use the \"stratify\" option while splitting the data to ensure a well balanced test and train set as regards the response variable or outcome.\n",
    "\n",
    "Reference: https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the Explanatory features and Response variable to variables X and y respectively  \n",
    "X = Business_Data_ohe_scaled.drop(columns=['y'], axis=1)\n",
    "y = Business_Data_ohe_scaled.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data but use the stratify option to ensure a \n",
    "# balanced split of the outcome \"y\" across the test and train data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the ratio of the response variable in the Train-Test split to ensure it is balanced\n",
    "\n",
    "Let's check the ratio of the response variable and compare it with that of the Train and Test dataset. The below results confirm that the ratio is retained and hence we can proceed with K-Fold Cross-Validation(CV) instead of Stratifid K-Fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Business_Data_ohe_scaled['y']==0)/sum(Business_Data_ohe_scaled['y']==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test==0)/sum(y_test==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train==0)/sum(y_train==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Baseline Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Establish the baseline model and Cost-Benefit matrix before tuning and optimizations\n",
    "\n",
    "- Use the complete data set and select diverse models with default parameters.\n",
    "- Establish a confusion matrix with classification results and model accuracy scores. \n",
    "- Create a custom function **confusion_mat** to capture, display and compare the confusion and classification results. \n",
    "- Use the true False Positives(FP) and False Negatives(FN) for the case counts to calculate the resulting business cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MODEL EVALUATION METRICS \n",
    "\n",
    "In order to evaluate the model, we reviewed several standard metrics, which are defined below. The aim is to maximize our precision, recall, and accuracy scores in our models.\n",
    "\n",
    "\n",
    "The model had the following evaluation profile calcualated based on averages of the metrics by evaluating them on 10 times cross validation:  \n",
    " * Accuracy: \n",
    " * Precsion: \n",
    " * Recall: \n",
    " * F1:   \n",
    "\n",
    "\n",
    "* **Confusion Matrix** - The confusion matrix shows that this model categorized XX incorrectly as 1 when they were not and xx emails as 0 when they actually were 1.  \n",
    "\n",
    "\n",
    "  \n",
    "* **ACCURACY** - total number of correct predictions (True Positives/TP; True Negatives/TN) over total number of predictions made. <br>\n",
    "Accuracy = (TP + TN)/(TP + FP + FN + TN)\n",
    "\n",
    "\n",
    "* **PRECISION** - proportion of true positives over total number of positive outcomes, whether accurately predicted (TP) or inaccurately predicted (FP); helps illuminate which model is accurately picking correct classes or correctly classifying observations.<br>\n",
    "Precision = (TP) / (TP + FP)\n",
    "\n",
    "\n",
    "* **RECALL** - proportion of positive outcomes that were correctly classified by model; tells how many values were incorrectly predicted; a good pair with precision to determine if modeling is overfitting or selecting a single class; also known as sensitivity.  \n",
    "Recall/Sensitivity = (TP) / (TP + FN) \n",
    "\n",
    "\n",
    "* **F1 SCORE** - measure of accuracy that accounts for true negatives and false positives.<br>\n",
    "F1 score = 2(True Positive Rate * True Negatives)/(True Positives + True Negatives) \n",
    "\n",
    "\n",
    "We further plotted an **ROC curve** to visualize the Decision Tree's performance. An ROC curve, also known as a Reciever Operation Characteristic Curve, plots and compares classifiers based on the True Positive Rates (TPR) and False Positive Rates (FPR) for each classifier. An AUC (Area Under the Curve) score of 1.0 denotes a perfect classifier and an area of 0.5 represents a model which is no better than a random guess. Higher the AUC the better the classifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the default parameters for the models to be used below\n",
    "rand_state = 101 # set a seed for reproducibility\n",
    "n_iterations = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### K-fold Cross-Validation\n",
    "\n",
    "- This statistical method is used to evaluate model generalization performance for future applicability\n",
    "- Provides greater stability and is more robust than using a simple split of a dataset into train and test sets.\n",
    "- The algorithm splits the dataset multiple times training the model repeatedly on each of the splits.\n",
    "- KFold divides the dataset into groups of equal sized samples (if possible), called folds.\n",
    "- The prediction function uses these folds for learning, with the fold left out used for testing.\n",
    "\n",
    "![title](img/KFoldCV.png) \n",
    "\n",
    "Reference: https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538\n",
    "\n",
    "We adopted the k-fold cross-validation approach for our analysis with the following parameters.\n",
    "\n",
    "Parameters for k-fold CV:\n",
    "- **n_splitsint** - Number of folds\n",
    "    - Parameter selected: **5**\n",
    "- **random_state** - random_state is the seed used by the random number generator\n",
    "    - Parameter selected: **10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the cross validation parameters to be used in the models\n",
    "cv = KFold(n_splits=n_iterations, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to capture the results of the baseline models below\n",
    "results_df = pd.DataFrame(columns=['Classifier', 'Features', 'Accuracy', 'Parameters', 'Recall', \n",
    "                                   'Precision', 'FPR', 'TPR', 'AUC', 'True_Positives', \n",
    "                                   'True_Negatives', 'False_Positives', 'False_Negatives', 'Cost(1000s)'])\n",
    "\n",
    "# Review the dataframe\n",
    "results_df\n",
    "\n",
    "# Create a reduced report of selected columns\n",
    "results_df_reduced = results_df[['Classifier', 'Features', 'Parameters', 'Accuracy', 'Recall',\n",
    "            'Precision', 'AUC', 'True_Positives', 'True_Negatives', \n",
    "            'False_Positives', 'False_Negatives', 'Cost(1000s)']].sort_values(by = 'Cost(1000s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom function to create a confusion matrix and print the results of the base models\n",
    "def confusion_mat(y_test, y_pred):\n",
    "    print('----------------------------------------------------------')\n",
    "    print('Confusion Matrix::\\n')\n",
    "    print(pd.crosstab(y_test, y_pred))\n",
    "    print()\n",
    "    print('Classification Report::')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    TN = pd.crosstab(y_test, y_pred).iloc[0, 0]\n",
    "    FP = pd.crosstab(y_test, y_pred).iloc[0, 1]\n",
    "    FN = pd.crosstab(y_test, y_pred).iloc[1, 0]\n",
    "    TP = pd.crosstab(y_test, y_pred).iloc[1, 1]\n",
    "    Cost = ((FP*10)+(FN*500))/1000\n",
    "    Recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    Precision = precision_score(y_test, y_pred)\n",
    "    Accuracy = accuracy_score(y_test, y_pred)\n",
    "    return(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier dictionary\n",
    "clfsDict = {'Logistic Regression': LogisticRegression(solver = 'lbfgs', random_state=rand_state),\n",
    "            'Gaussian Naive-Bayes': GaussianNB(),\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=rand_state),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=rand_state),\n",
    "            'XGB': XGBClassifier(random_state=rand_state)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the baseline models and capture the performance metrics\n",
    "start = time()\n",
    "\n",
    "# Initialize a dataframe to capture the model stats\n",
    "model_run_stats = []\n",
    "\n",
    "# Loop through the models in the classifier dictionary\n",
    "for mdl, clf in clfsDict.items():\n",
    "    start_1 = time()\n",
    "    acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_full_data = clf.predict(X_test)\n",
    "    y_pred_proba_full_data = clf.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba_full_data)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba_full_data)\n",
    "    (FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_full_data)\n",
    "    results_df = results_df.append({'Classifier':mdl\n",
    "                                    , 'Features':'Full'\n",
    "                                    , 'Parameters': 'Default'\n",
    "                                    , 'Accuracy': Accuracy\n",
    "                                    , 'Recall': Recall\n",
    "                                    , 'Precision': Precision\n",
    "                                    , 'FPR': fpr\n",
    "                                    , 'TPR': tpr\n",
    "                                    , 'AUC': auc\n",
    "                                    , 'True_Positives': TP\n",
    "                                    , 'True_Negatives': TN\n",
    "                                    , 'False_Positives': FP\n",
    "                                    , 'False_Negatives': FN\n",
    "                                    , 'Cost(1000s)': Cost\n",
    "                                   }\n",
    "                                   , ignore_index=True\n",
    "                                  )\n",
    "    print()\n",
    "    print('Process Time:: %0f' %(time()-start_1))\n",
    "    print()\n",
    "    \n",
    "# get the feature importance from Random Forest and store in variable fi\n",
    "    if mdl=='Random Forest':\n",
    "        fi = pd.DataFrame(clf.feature_importances_,columns = ['featimp'])\n",
    "\n",
    "# display the model stats\n",
    "model_run_stats\n",
    "\n",
    "print('Wall Time:: %0f' %(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the combined results of the baselining \n",
    "# using all features and default parameters\n",
    "results_df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dimensionality Reduction (Feature Reduction)\n",
    "---\n",
    "\n",
    "In the provided dataset we have 50 explanatory features.  It is essential to understand and determine the importance of each feature in model contribution to enhance estimator results and improve performance. This is an important concept in machine learning and centered around simplicity and optimization.  Using the sklearn.feature_selection module that examines the classes, we can determine the features having the greatest impact and contribution to the selected models. \n",
    "\n",
    "Reference: https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15\n",
    "\n",
    "\n",
    "### Benefits of Dimensionality reduction\n",
    "- Contributes to increasing the goodness of fit metric - **R2**\n",
    "- Aligns with the principle of **Occam's razor or the law of parsimony** - simpler is better and more likely to be correct\n",
    "- Improve an estimators accuracy scores\n",
    "- Boost performance especially on high-dimensional datasets\n",
    "\n",
    "We used **Recursive Feature Elimination with Cross-Validation (RFECV)** to restrict the features used in the model.\n",
    "Recursive feature elimination (RFE) selects features by recursively considering smaller and smaller sets of features by an external estimator based on the assignment of weights against the features (e.g. the coefficients of a linear model).\n",
    "- Train the estimator on the baseline or complete set of features\n",
    "- Importance of each feature is determined by the coef_ attribute or a feature_importances_ attribute. \n",
    "- Least important features are trimmed from the original feature set.\n",
    "- The process is recursively repeated on the trimmed dataset until the optimal features desired are reached.\n",
    "\n",
    "\n",
    "### Parameters for RFECV: \n",
    "RFECV performs RFE in a cross-validation loop to identify the optimal features.\n",
    "- **estimator**: A supervised learning estimator with a fit method that provides information about feature importance.\n",
    "    - Parameter selected: **RandomForestClassifier**\n",
    "    \n",
    "    \n",
    "- **step** - If greater than or equal to 1, then step corresponds to the number of features to remove at each iteration.\n",
    "    - Parameter selected: **4**\n",
    "    \n",
    "    \n",
    "- **cv**: Determines the cross-validation splitting strategy. Integer value specifies the number of folds.\n",
    "    - Parameter selected: **3**\n",
    "    \n",
    "    \n",
    "- **scoring**: A scoring method to determine which score to maximize\n",
    "    - Parameter selected: **recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Importance Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joe - Does it make sense to just list and plot the top features versus plotting all the features - Thoughts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge field names and feature importance to display them together\n",
    "featuresnames = pd.DataFrame(Business_Data_ohe_scaled.columns.values.tolist(), columns =['fields'])\n",
    "featimpdf = pd.merge(featuresnames, fi, left_index=True, right_index=True)\n",
    "featimpdf = featimpdf.sort_values(by='featimp', ascending=False)\n",
    "featimpdf\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(25,8))\n",
    "ax = sns.barplot(x=featimpdf.fields, y=featimpdf.featimp)\n",
    "ax.set_title('FEATURE IMPORTANCE')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the parameters for RFECV using the Random Forest estimator\n",
    "start = time()\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=rand_state)\n",
    "selector = RFECV(estimator=clf, step = 4, cv=3, scoring='recall')\n",
    "\n",
    "feature_fit = selector.fit(X_train, y_train)\n",
    "\n",
    "print()\n",
    "print('Process Time:: %0f' %(time()-start_1))\n",
    "print()\n",
    "\n",
    "print('Optimal number of features : ', feature_fit.n_features_)\n",
    "print('Best features : ', X_train.columns[selector.support_])\n",
    "\n",
    "print('Wall Time:: %0f' %(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features selected against the model score\n",
    "plt.figure()\n",
    "plt.title('Recursive Feature CV score versus Feature count')\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFECV process identified the following features as being the most important and best in contributing to the predictive modeling.  \n",
    "\n",
    "The most important features are:\n",
    "\n",
    "###  x7' ,\t 'x12' ,\t'x20',\t 'x23',\t 'x27',\t 'x28',\t 'x37',\t 'x38',\t 'x40',\t 'x41',\t'x42',\t 'x46',\t 'x48',\t 'x49'\n",
    "\n",
    "##### The business should pay greater attention to these features given their importance and adopt strategies as appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training and test sets using just the reduced features \n",
    "# identified from the RFECV operation conducted above\n",
    "X_train_reduced_features = X_train[X_train.columns[selector.support_]]\n",
    "X_test_reduced_features = X_test[X_test.columns[selector.support_]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the reduced features for the training data\n",
    "X_train_reduced_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the reduced features for the testing data\n",
    "X_test_reduced_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train the models using the reduced features\n",
    "---\n",
    "\n",
    "Using default parameters to establish a baseline with reduced features before proceeding to the **parameter tuning phase** using a **Randomized Grid Search** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decision Tree Classifier\n",
    "\n",
    "Decision trees are primarily leveraged to address both regression and classification cases. \n",
    "- **Classification tree models are used to predict a qualitative response**\n",
    "- **Regression tree models are used to predict a quantitative response**\n",
    "\n",
    "The algorithm constructs a tree using a training dataset where each node is an attribute and the branches are the corresponding values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DECISION TREE classifier \n",
    "clf_DT_default = DecisionTreeClassifier(random_state=rand_state)\n",
    "\n",
    "# Fit the model\n",
    "clf_DT_default.fit(X_train_reduced_features, y_train)\n",
    "\n",
    "# Do the prediction and capture the error/performance metrics\n",
    "y_pred = clf_DT_default.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_DT_default.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred)\n",
    "\n",
    "# Append the results to the \"results_df\" dataframe for compare and contrast\n",
    "results_df = results_df.append({'Classifier':'Decision Tree'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Default'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                                }\n",
    "                                , ignore_index=True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random Forest Classifier\n",
    "\n",
    "Random Forests are primarily based on Decision trees. \n",
    "- Are an ensemble of decision trees trained using the bagging methodology. \n",
    "- Leverages randomness when growing trees rather than just searching for the best feature when splitting a node.\n",
    "- Seeks the best feature among a random subset of features. \n",
    "- Delivers an enhanced model by trading higher bias for lower variance. \n",
    "\n",
    "Random Forests are useful in gaining a good understanding of feature importance especially when performing feature selection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RANDOM FOREST classifier \n",
    "clf_RF_default = RandomForestClassifier(random_state=rand_state)\n",
    "\n",
    "# Fit the model\n",
    "clf_RF_default.fit(X_train_reduced_features, y_train)\n",
    "\n",
    "# Do the prediction and capture the error/performance metrics\n",
    "y_pred = clf_RF_default.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_RF_default.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred)\n",
    "\n",
    "# Append the results to the \"results_df\" dataframe for compare and contrast\n",
    "results_df = results_df.append({'Classifier':'Random Forest'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Default'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### XGBoost Classifier\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed for high efficiency, flexibility and portability. \n",
    "- Machine learning algorithms are deployed leveraging the Gradient Boosting framework. \n",
    "- It provides a parallel tree boosting approach (GBDT, GBM) which is fast and accurate for solving data science problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBOOST Classifier\n",
    "clf_XGB_default = XGBClassifier(random_state=rand_state)\n",
    "\n",
    "# Fit the model\n",
    "clf_XGB_default.fit(X_train_reduced_features, y_train)\n",
    "\n",
    "# Do the prediction and capture the error/performance metrics\n",
    "y_pred = clf_XGB_default.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_XGB_default.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred)\n",
    "\n",
    "# Append the results to the \"results_df\" dataframe for compare and contrast\n",
    "results_df = results_df.append({'Classifier':'XGB'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Default'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the combined results matrix\n",
    "results_df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RandomizedSearchCV to choose best parameters\n",
    "---\n",
    "\n",
    "In RandomizedSearchCV all hyperparameter values are not tested. Instead, a fixed number of hyperparameter settings are sampled from specified probability distributions.\n",
    "\n",
    "### Options for RandomizedSearchCV\n",
    "- estimator - A object of that type is instantiated for each grid point\n",
    "    - Assigned as per algorithm being tuned\n",
    "    \n",
    "    \n",
    "- param_distributions - Dictionary with parameters names (string) as keys and distributions or lists of parameters to try\n",
    "    - Assigned as per algorithm being tuned\n",
    "    \n",
    "    \n",
    "- scoring - A single string to evaluate the predictions on the test set\n",
    "    - Parameter setting: **['precision' , 'recall' , 'accuracy']**\n",
    "    \n",
    "    \n",
    "- cv - Determines the cross-validation splitting strategy\n",
    "    - Parameter setting: **5**\n",
    "    \n",
    "    \n",
    "- refit - Refit an estimator using the best found parameters on the whole dataset\n",
    "    - Parameter setting: **recall**\n",
    "    \n",
    "    \n",
    "- n_iterint - Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution\n",
    "    - Parameter setting: **10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"scores\" variable with metrics\n",
    "scores = ['precision'\n",
    "          , 'recall'\n",
    "          , 'accuracy'\n",
    "         ]\n",
    "\n",
    "# Review the variable\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to capture the performance metrics\n",
    "def report_perf(clf, n_top = 3, clf_name = \"\", verbose=True):\n",
    "    if(verbose):\n",
    "        for n in range(1, n_top+1):\n",
    "            candidates = np.flatnonzero(clf.cv_results_['rank_test_recall'] == n)\n",
    "            for candidate in candidates:\n",
    "                print('Model with rank: {0}'.format(n))\n",
    "                print('Mean validation score (Recall on Test): {0:.3f} (std: {1:.3f})'.format(\n",
    "                    clf.cv_results_['mean_test_recall'][candidate], clf.cv_results_['std_test_recall'][candidate]))\n",
    "                print('ParametersL {0}',format(clf.cv_results_['params'][candidate]))\n",
    "    return{'Classifier': clf_name\n",
    "           , 'Best_Parameters': str(clf.best_params_)\n",
    "           , 'Best_Estimator': str(clf.best_estimator_)\n",
    "           , 'Precision_Mean':clf.cv_results_['mean_test_precision'][clf.best_index_]\n",
    "           , 'Precision_std':clf.cv_results_['std_test_precision'][clf.best_index_]\n",
    "           , 'Recall_Mean':clf.cv_results_['mean_test_recall'][clf.best_index_]\n",
    "           , 'Recall_std':clf.cv_results_['std_test_recall'][clf.best_index_]\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Decision Tree\n",
    "\n",
    "Tuning the *Decision Tree Classifier*:\n",
    "- max_depth: The maximum depth of the tree.\n",
    "    - Parameter setting: **[10, 20, 30, None]**\n",
    "    \n",
    "    \n",
    "- max_features: The number of features to consider when looking for the best split\n",
    "    - Parameter setting: **['auto', 'sqrt']**\n",
    "    \n",
    "    \n",
    "- criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "    - Parameter setting: **['gini','entropy']**\n",
    "    \n",
    "    \n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node.  \n",
    "    - Parameter setting: **[1, 2, 4]**\n",
    "    \n",
    "    \n",
    "- min_samples_split: The minimum number of samples required to split an internal node\n",
    "    - Parameter setting: **[2, 5, 10]**\n",
    "    \n",
    "    \n",
    "- random_state - If int, random_state is the seed used by the random number generator\n",
    "    - Parameter setting: **10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree Classifier\n",
    "clf_DT = DecisionTreeClassifier(random_state=rand_state)\n",
    "\n",
    "# Set the tuning parameters\n",
    "tuned_parameters_DT={'max_depth': [10, 20, 30, None]\n",
    "                     , 'max_features': ['auto', 'sqrt']\n",
    "                     , 'criterion':['gini','entropy']\n",
    "                     , 'min_samples_leaf': [1, 2, 4]\n",
    "                     , 'min_samples_split': [2, 5, 10]\n",
    "                    }\n",
    "\n",
    "random_search_DT = RandomizedSearchCV(estimator=clf_DT\n",
    "                                      , param_distributions=tuned_parameters_DT\n",
    "                                      , cv = 5\n",
    "                                      , scoring = scores\n",
    "                                      , refit ='recall'\n",
    "                                      , n_iter=10\n",
    "                                     )\n",
    "\n",
    "# Run the RandomizedSearchCV for the Classifier\n",
    "random_search_DT.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Capture the performance metrics for the Decision Tree tuning\n",
    "DT_report = report_perf(random_search_DT, n_top = 3, clf_name='Decision_Tree')\n",
    "\n",
    "# Display the report\n",
    "DT_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random Forest\n",
    "\n",
    "Tuning the parameters for the *Random Forest Classifier:\n",
    "- n_estimators: The number of trees in the forest.\n",
    "    - Parameter setting: **[100, 200]**\n",
    "    \n",
    "    \n",
    "- max_depth: The maximum depth of the tree.\n",
    "    - Parameter setting: **[25, 50, None]**\n",
    "    \n",
    "    \n",
    "- criterion: function to measure the quality of a split. Supported criteria are \"gini\" for Gini impurity and \"entropy\" for information gain.\n",
    "    - Parameter setting: **['gini','entropy']**\n",
    "    \n",
    "    \n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "    - Parameter setting: **[1, 25, 50]**\n",
    "    \n",
    "    \n",
    "- max_features: The number of features to consider when looking for the best split.\n",
    "    - Parameter setting: **[.2, .3, 'auto']**\n",
    "    \n",
    "    \n",
    "- min_samples_split: The minimum number of samples required to split an internal node.\n",
    "    - Parameter setting: **[2, 4, 8, 10, 12]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "clf_RF = RandomForestClassifier(random_state=rand_state)\n",
    "\n",
    "# Set the tuning parameters\n",
    "tuned_parameters_RF={'n_estimators':[100, 200]\n",
    "                     , 'max_depth':[25, 50, None]\n",
    "                     , 'criterion':['gini','entropy']\n",
    "                     , 'min_samples_leaf': [1, 25, 50]\n",
    "                     , 'max_features':[.2, .3, 'auto']\n",
    "                     , 'min_samples_split':[2, 4, 8, 10, 12]\n",
    "                    }\n",
    "\n",
    "random_search_RF = RandomizedSearchCV(estimator=clf_RF\n",
    "                                      , param_distributions=tuned_parameters_RF\n",
    "                                      , cv = 5\n",
    "                                      , scoring = scores\n",
    "                                      , refit = 'recall'\n",
    "                                      , n_iter=10\n",
    "                                     )\n",
    "\n",
    "# Run the RandomizedSearchCV for the Classifier\n",
    "random_search_RF.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Capture the performance metrics for the Random Forest tuning\n",
    "RF_report = report_perf(random_search_RF, n_top = 3, clf_name='Random_Forest')\n",
    "\n",
    "# Display the report\n",
    "RF_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### XGBoost Classifier\n",
    "\n",
    "Tuning the parameters for XGBoost:\n",
    "- n_estimators: Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "    - Parameter setting: **[100, 150, 1000]**\n",
    "    \n",
    "    \n",
    "- learning_rate: Boosting learning rate (xgb’s “eta”)\n",
    "    - Parameter setting: **[0.01, 0.6, None]**\n",
    "    \n",
    "    \n",
    "- subsample: Subsample ratio of the training instances.\n",
    "    - Parameter setting: **[0.3, 0.9, None]**\n",
    "    \n",
    "    \n",
    "- max_depth: Maximum depth of a tree.\n",
    "    - Parameter setting: **[3, 4, 5, 6, 7, 8, 9, None]**\n",
    "    \n",
    "    \n",
    "- colsample_bytree: The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "    - Parameter setting: **[0.5, 0.9, None]**\n",
    "    \n",
    "    \n",
    "- min_child_weight: Minimum sum of instance weight (hessian) needed in a child.\n",
    "    - Parameter setting: **[1, 2, 3, 4, None]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost Classifier\n",
    "clf_XGB = XGBClassifier(random_state=rand_state)\n",
    "\n",
    "# Set the tuning parameters\n",
    "tuned_parameters_XGB={'n_estimators': [100, 150, 1000]\n",
    "                      , 'learning_rate': [0.01, 0.6, None]\n",
    "                      , 'subsample': [0.3, 0.9, None]\n",
    "                      , 'max_depth': [3, 4, 5, 6, 7, 8, 9, None]\n",
    "                      , 'colsample_bytree': [0.5, 0.9, None]\n",
    "                      , 'min_child_weight': [1, 2, 3, 4, None]\n",
    "                     }\n",
    "\n",
    "random_search_XGB = RandomizedSearchCV(estimator=clf_XGB\n",
    "                                      , param_distributions=tuned_parameters_XGB\n",
    "                                      , cv = 5\n",
    "                                      , scoring = scores\n",
    "                                      , refit ='recall'\n",
    "                                      , n_iter=10\n",
    "                                     )\n",
    "# Run the RandomizedSearchCV for the XGBoost Classifier\n",
    "random_search_XGB.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Capture the performance metrics for the XGBoost tuning\n",
    "XGB_report = report_perf(random_search_XGB, n_top = 3, clf_name='XGBoost')\n",
    "\n",
    "# Display the report\n",
    "XGB_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append random_state to the best parameters\n",
    "\n",
    "Assign the best parameters from each Random Search model to a variable and add the random state parameter to it. These variables are then used to train the models with **reduced features and best parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_best_params = random_search_DT.best_params_\n",
    "DT_best_params['random_state'] = rand_state\n",
    "\n",
    "RF_best_params = random_search_RF.best_params_\n",
    "RF_best_params['random_state'] = rand_state\n",
    "\n",
    "XGB_best_params = random_search_XGB.best_params_\n",
    "XGB_best_params['random_state'] = rand_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train the models with Reduced Features and Best Parameters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "- The best parameter (with random_state) from the previous step is chosen as classifier\n",
    "- Confusion Matrix and Classification Reports are displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_DT_best = DecisionTreeClassifier(**DT_best_params)\n",
    "clf_DT_best.fit(X_train_reduced_features, y_train)\n",
    "\n",
    "y_pred_DT = clf_DT_best.predict(X_test_reduced_features)\n",
    "\n",
    "y_pred_proba = clf_DT_best.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_DT)\n",
    "results_df = results_df.append({'Classifier':'Decision Tree'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "- The best estimator from the previous step is chosen as classifier\n",
    "- Confusion Matrix and Classification Reports are displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF_best = RandomForestClassifier(**RF_best_params)\n",
    "clf_RF_best.fit(X_train_reduced_features, y_train)\n",
    "\n",
    "y_pred_RF = clf_RF_best.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_RF_best.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_RF)\n",
    "\n",
    "results_df = results_df.append({'Classifier':'Random Forest'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "- The best estimator from the previous step is chosen as classifier\n",
    "- Confusion Matrix and Classification Reports are displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB_Best = XGBClassifier(**XGB_best_params)\n",
    "clf_XGB_Best.fit(X_train_reduced_features, y_train)\n",
    "\n",
    "y_pred_XGB = clf_XGB_Best.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_XGB_Best.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_XGB)\n",
    "\n",
    "results_df = results_df.append({'Classifier':'XGB'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators in order to improve generalizability and robustness versus a single estimator.\n",
    "\n",
    "Stacking, also called Super Learning or Stacked Regression, is a class of algorithms that involves training a second-level \"metalearner\" to find the optimal combination of the base learners. Unlike bagging and boosting, the goal in stacking is to ensemble strong and diverse sets of learners together.\n",
    "\n",
    "![title](img/EnsembleModels3.png) \n",
    "\n",
    "References: https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model 1 - STACKING\n",
    "\n",
    "In this ensemble technique we leverage the vecstack package for stacking. It is compatible with the scikit-learn API to automate OOF computation, prediction and bagging using multiple models and stages.\n",
    "\n",
    "![title](img/EnsembleModels.png) \n",
    "\n",
    "Reference: https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\n",
    "\n",
    "We use the \"BEST\" Decision Tree and Random Forest classifiers from above in Stage 1. Stage 1 output is then passed on to Stage 2 which uses the XGBoost classifier with the best parameters. Finally, Logistic Regression is used as the Meta Classifier to predict the outcome and compare against the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# Initialize Level-1 models\n",
    "#-------------------------------------------------------\n",
    "models_1 = [clf_DT_best\n",
    "            , clf_RF_best]\n",
    "\n",
    "# Get out-of-fold predictions from Level-1 models\n",
    "S_1_train, S_1_test = stacking(models_1\n",
    "                               , X_train_reduced_features\n",
    "                               , y_train\n",
    "                               , X_test_reduced_features\n",
    "                               , regression = True\n",
    "                               , metric=recall_score\n",
    "                               , verbose = 2)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Initialize Level-2 models\n",
    "#-------------------------------------------------------\n",
    "models_2 = [clf_XGB_Best]\n",
    "\n",
    "# Get out-of-fold predictions from Level-2 models\n",
    "S_2_train, S_2_test = stacking(models_2\n",
    "                               , S_1_train\n",
    "                               , y_train\n",
    "                               , S_1_test\n",
    "                               , regression = True\n",
    "                               , metric=recall_score\n",
    "                               , verbose = 2)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Initialize the Level-3 model\n",
    "#-------------------------------------------------------\n",
    "model = LogisticRegression(random_state=rand_state)\n",
    "\n",
    "# Fit\n",
    "model.fit(S_2_train, y_train)\n",
    "\n",
    "# Get final prediction\n",
    "y_pred_ensemble = model.predict(S_2_test)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "model.predict_proba(S_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(S_2_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_ensemble)\n",
    "results_df = results_df.append({'Classifier':'Ensemble_1'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model 2\n",
    "\n",
    "The second ensemble technique uses the StackingCVClassifier from the mlxtend package. \n",
    "\n",
    "![title](img/Ensemble_StackingCVClassifier.png) \n",
    "\n",
    "Reference: http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/\n",
    "\n",
    "In this model, we pass all our 3 best models (Decision Tree, Random Forest and XGBoost) together in Stage 1. Similar to the previous ensemble model, we use Logistic Regression as our Meta-Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingCVClassifier(classifiers=[clf_DT_best, clf_RF_best, clf_XGB_Best]\n",
    "                            , use_probas=True\n",
    "                            , random_state= rand_state\n",
    "                            , meta_classifier=LogisticRegression(random_state=rand_state))\n",
    "\n",
    "label = ['Decision Tree', 'Random Forest', 'XGBoost', 'Stacking Classifier']\n",
    "clf_list = [clf_DT_best, clf_RF_best, clf_XGB_Best, sclf]\n",
    "    \n",
    "sclf.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sclf = sclf.predict(X_test_reduced_features)\n",
    "y_pred_proba = sclf.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_sclf)\n",
    "results_df = results_df.append({'Classifier':'Ensemble_2'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model 3\n",
    "\n",
    "Stack of estimators with a final classifier. Stacked generalization consists of stacking the output of individual estimators and using a classifier to compute the final prediction. Stacking leverages the strength of each individual estimator by using their outputs as input to the final estimator.\n",
    "\n",
    "![title](img/StackOfEstimatorsWithFinalClassifier.png)\n",
    "\n",
    "Reference: https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840\n",
    "\n",
    "\n",
    "In this step, we use multi-layer classsifier technique. In this technique, we assign the final estimator to a stacking classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = StackingClassifier(estimators=[('rf', clf_RF_best)\n",
    "                                            , ('xgb', clf_XGB_Best)]\n",
    "                                , final_estimator=LogisticRegression(random_state=rand_state)\n",
    "                               )\n",
    "\n",
    "multi_layer_regressor = StackingClassifier(estimators=[('dt', clf_DT_best)]\n",
    "                                          , final_estimator=final_layer\n",
    "                                         )\n",
    "\n",
    "multi_layer_regressor.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multi = multi_layer_regressor.predict(X_test_reduced_features)\n",
    "y_pred_proba = multi_layer_regressor.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_multi)\n",
    "results_df = results_df.append({'Classifier':'Ensemble_3'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model 4\n",
    "\n",
    "This Ensemble Model is similar to Ensemble Model 3. However, in this step, we use a single stacking layer with Decision Tree, Random Forest and XGBoost and the best parameters passed as ensemble estimators. \n",
    "\n",
    "![title](img/EnsembleModels2.png)\n",
    "\n",
    "Reference: https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\n",
    "\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is chosen as the final estimator or meta-classifier as discussed in Ensemble Model 1 & 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_estimators = [('rf', clf_RF_best)\n",
    "                       , ('xgb', clf_XGB_Best)\n",
    "                       , ('dt', clf_DT_best)\n",
    "                      ]\n",
    "\n",
    "clf_ensemble = StackingClassifier(estimators=ensemble_estimators\n",
    "                                  , final_estimator=CalibratedClassifierCV(SGDClassifier(random_state=rand_state))\n",
    "                                 )\n",
    "\n",
    "clf_ensemble.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ens = clf_ensemble.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_ensemble.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_ens)\n",
    "results_df = results_df.append({'Classifier':'Ensemble_4'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model 5\n",
    "\n",
    "Ensemble model 5 is the same as Ensemble Model 4. The only difference is that we are using SMV (SVC) as the final estimator here instead of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_estimators_5 = [('rf', clf_RF_best)\n",
    "                       , ('xgb', clf_XGB_Best)\n",
    "                       , ('dt', clf_DT_best)\n",
    "                      ]\n",
    "\n",
    "clf_ensemble_5 = StackingClassifier(estimators=ensemble_estimators_5\n",
    "                                    , final_estimator=CalibratedClassifierCV(SVC(gamma = 0.5, random_state=rand_state))\n",
    "                                 )\n",
    "\n",
    "clf_ensemble_5.fit(X_train_reduced_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ens_5 = clf_ensemble_5.predict(X_test_reduced_features)\n",
    "y_pred_proba = clf_ensemble_5.predict_proba(X_test_reduced_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "(FP, FN, TP, TN, Cost, Accuracy, Recall, Precision) = confusion_mat(y_test, y_pred_ens_5)\n",
    "results_df = results_df.append({'Classifier':'Ensemble_5'\n",
    "                                , 'Features':'Reduced'\n",
    "                                , 'Parameters': 'Best'\n",
    "                                , 'Accuracy': Accuracy\n",
    "                                , 'Recall': Recall\n",
    "                                , 'Precision': Precision\n",
    "                                , 'FPR': fpr\n",
    "                                , 'TPR': tpr\n",
    "                                , 'AUC': auc\n",
    "                                , 'True_Positives': TP\n",
    "                                , 'True_Negatives': TN\n",
    "                                , 'False_Positives': FP\n",
    "                                , 'False_Negatives': FN\n",
    "                                , 'Cost(1000s)': Cost\n",
    "                               }\n",
    "                               , ignore_index=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['Classifier', 'Features', 'Parameters', 'Accuracy', 'Recall',\n",
    "            'Precision', 'AUC', 'True_Positives', 'True_Negatives', \n",
    "            'False_Positives', 'False_Negatives', 'Cost(1000s)']].sort_values(by = 'Cost(1000s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_plot_df = results_df[['Classifier', 'Features', 'Parameters', \n",
    "                          'AUC', 'FPR', 'TPR', 'Cost(1000s)']].sort_values(by = 'Cost(1000s)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_plot_df['Clf'] = roc_plot_df[['Classifier', 'Features', 'Parameters']].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "roc_plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_plot_df.set_index('Clf', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "for i in roc_plot_df.index:\n",
    "    plt.plot(roc_plot_df.loc[i]['FPR'], \n",
    "             roc_plot_df.loc[i]['TPR'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, roc_plot_df.loc[i]['AUC']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Code adapted from Capstone Project work by Kevin Mendonsa, Vivek Viswanathan, and Maureen Stolberg: \"TITLE HERE\"\n",
    "\n",
    "- https://jamesrledoux.com/code/imputation\n",
    "- https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "- https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "- https://stackoverflow.com/questions/60393024/rfecv-for-classification-giving-keyerror-weight\n",
    "- https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
    "- https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n",
    "- https://towardsdatascience.com/automate-stacking-in-python-fc3e7834772e\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
