{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTFj8ft5dlbS"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "lzyBOpYMdp3F"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "m_x4KfSJ7Vt7"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9HmC2T4ld5B"
   },
   "source": [
    "# Overfit and underfit -- NW Q: do we ned this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRTxFhXAlnl1"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19rPukKZsPG6"
   },
   "source": [
    "As always, the code in this example will use the `tf.keras` API, which you can learn more about in the TensorFlow [Keras guide](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "In both of the previous examples—[classifying text](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub) and [predicting fuel efficiency](https://www.tensorflow.org/tutorials/keras/regression) — we saw that the accuracy of our model on the validation data would peak after training for a number of epochs, and would then stagnate or start decreasing.\n",
    "\n",
    "In other words, our model would *overfit* to the training data. Learning how to deal with overfitting is important. Although it's often possible to achieve high accuracy on the *training set*, what we really want is to develop models that generalize well to a *testing set* (or data they haven't seen before).\n",
    "\n",
    "The opposite of overfitting is *underfitting*. Underfitting occurs when there is still room for improvement on the test data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data.\n",
    "\n",
    "If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. We need to strike a balance. Understanding how to train for an appropriate number of epochs as we'll explore below is a useful skill.\n",
    "\n",
    "To prevent overfitting, the best solution is to use more complete training data. The dataset should cover the full range of inputs that the model is expected to handle. Additional data may only be useful if it covers new and interesting cases.\n",
    "\n",
    "A model trained on more complete data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization. These place constraints on the quantity and type of information your model can store.  If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
    "\n",
    "In this notebook, we'll explore several common regularization techniques, and use them to improve on a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: \n",
    "Steven Hayden, Kevin Mendonsa, Joe Schueder, Nicole Wittlin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WL8UoOTmGGsL"
   },
   "source": [
    "## Data Preparation and Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NW Q: what else do we need here? Most of this was pulled from the case, what from the notebook below do we need to highlight? Any key packages? Etc. \n",
    "SH: we did not really do any data exploration. We changed how the data was imported from a tensor to a df. This made it more readable. This could have a negative impact on perform. \n",
    "\n",
    "For example, though the Large Hadron Collider produces approximately 10^11 collisions per hour, approximately 300 of these collisions result in a Higgs boson, on average. Therefore, good data analysis depends on distinguishing collisions which produce particles of interest (signal) from those producing other particles (background).\n",
    "\n",
    "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "Published dataset containing 11 million simulated collision events for benchmarking ML classification algorithms, found on UCI Machine Learning Repository\n",
    "\n",
    "Data sets were nearly balanced with 53% positive examples in Higgs data \n",
    "\n",
    "https://www.tensorflow.org/datasets/catalog/higgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "### Package Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pZ8A2liqvgk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnAtAjqRYVXe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs\n",
    "#!pip install git+https://github.com/tensorflow/docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3S2qD9OrICX"
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pnOU-ctX27Q"
   },
   "outputs": [],
   "source": [
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jj6I4dvTtbUe"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPjAvwb-6dFd"
   },
   "outputs": [],
   "source": [
    "#gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
    "#gz = 'C:/Users/kevinm/Documents/SMU/MSDS7337_NaturalLanguageProcessing/Week11/HIGGS.csv.gz'\n",
    "#gz = 'C:/Users/shayden/Downloads/HIGGS.csv.gz'\n",
    "#gz = tf.keras.utils.get_file('HIGGS.csv.gz', '/home/jjschued/HIGGS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "proxy = 'http://proxy.rockwellcollins.com:9090'\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/17vHho4WOidi1xsU5nuYIxvgaxKTK2L75/view?usp=sharing\n",
    "file_id = '17vHho4WOidi1xsU5nuYIxvgaxKTK2L75'\n",
    "#url_dataset = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "#url_dataset = '/user/jjschued/HIGGS.csv'\n",
    "#url_dataset = 'https://drive.google.com/u/1/uc?export=download&confirm=cg02&id=17vHho4WOidi1xsU5nuYIxvgaxKTK2L75'\n",
    "#url_dataset = \"https://doc-0g-4c-docs.googleusercontent.com/docs/securesc/aplji487l37jpmjrtc3fc7mkdck4ah8r/qd2imheo9cvohf9r82e90395lm9o7bva/1595540025000/05402358165231425872/05402358165231425872/17vHho4WOidi1xsU5nuYIxvgaxKTK2L75?e=download&authuser=1\"\n",
    "#url = requests.get(url_dataset).text\n",
    "#csv_raw = StringIO(url)\n",
    "url_dataset = '/home/jjschued/HIGGS.csv'\n",
    "#df = pd.read_csv('C:/Users/shayden/Downloads/HIGGS.csv.gz', compression = 'gzip',nrows=11000, header = None)\n",
    "#url_dataset = 'C:/Users/shayden/Downloads/HIGGS.csv'\n",
    "#df = pd.read_csv(url_dataset, nrows=1000000)\n",
    "df = pd.read_csv(url_dataset, nrows=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jjschued/HIGGS.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.000000000000000000e+00</th>\n",
       "      <th>8.692932128906250000e-01</th>\n",
       "      <th>-6.350818276405334473e-01</th>\n",
       "      <th>2.256902605295181274e-01</th>\n",
       "      <th>3.274700641632080078e-01</th>\n",
       "      <th>-6.899932026863098145e-01</th>\n",
       "      <th>7.542022466659545898e-01</th>\n",
       "      <th>-2.485731393098831177e-01</th>\n",
       "      <th>-1.092063903808593750e+00</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>...</th>\n",
       "      <th>-1.045456994324922562e-02</th>\n",
       "      <th>-4.576716944575309753e-02</th>\n",
       "      <th>3.101961374282836914e+00</th>\n",
       "      <th>1.353760004043579102e+00</th>\n",
       "      <th>9.795631170272827148e-01</th>\n",
       "      <th>9.780761599540710449e-01</th>\n",
       "      <th>9.200048446655273438e-01</th>\n",
       "      <th>7.216574549674987793e-01</th>\n",
       "      <th>9.887509346008300781e-01</th>\n",
       "      <th>8.766783475875854492e-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>1.084947</td>\n",
       "      <td>0.299487</td>\n",
       "      <td>0.511042</td>\n",
       "      <td>-0.374340</td>\n",
       "      <td>0.774997</td>\n",
       "      <td>-0.218866</td>\n",
       "      <td>-0.547110</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377029</td>\n",
       "      <td>-1.557350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.077976</td>\n",
       "      <td>1.028331</td>\n",
       "      <td>0.984388</td>\n",
       "      <td>1.004476</td>\n",
       "      <td>0.925660</td>\n",
       "      <td>1.012436</td>\n",
       "      <td>1.057740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306174</td>\n",
       "      <td>-1.808714</td>\n",
       "      <td>-1.240359</td>\n",
       "      <td>1.365489</td>\n",
       "      <td>-1.740858</td>\n",
       "      <td>1.695741</td>\n",
       "      <td>0.731754</td>\n",
       "      <td>0.156495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.232118</td>\n",
       "      <td>-0.143431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886896</td>\n",
       "      <td>0.935988</td>\n",
       "      <td>0.973462</td>\n",
       "      <td>0.879262</td>\n",
       "      <td>0.605018</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>1.018208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794809</td>\n",
       "      <td>-0.059466</td>\n",
       "      <td>0.526425</td>\n",
       "      <td>0.736569</td>\n",
       "      <td>1.662992</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>-1.320002</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.238036</td>\n",
       "      <td>-1.050715</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.443730</td>\n",
       "      <td>1.199071</td>\n",
       "      <td>0.990352</td>\n",
       "      <td>0.697507</td>\n",
       "      <td>0.799412</td>\n",
       "      <td>1.002388</td>\n",
       "      <td>0.846745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.839097</td>\n",
       "      <td>0.202531</td>\n",
       "      <td>0.619642</td>\n",
       "      <td>0.497209</td>\n",
       "      <td>-1.177904</td>\n",
       "      <td>0.544788</td>\n",
       "      <td>1.262518</td>\n",
       "      <td>0.130439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.053149</td>\n",
       "      <td>-0.947502</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.806380</td>\n",
       "      <td>1.075156</td>\n",
       "      <td>0.987699</td>\n",
       "      <td>0.624265</td>\n",
       "      <td>0.887372</td>\n",
       "      <td>0.718604</td>\n",
       "      <td>0.641227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.265836</td>\n",
       "      <td>-0.984737</td>\n",
       "      <td>0.296158</td>\n",
       "      <td>1.097784</td>\n",
       "      <td>1.345723</td>\n",
       "      <td>0.668183</td>\n",
       "      <td>-0.723884</td>\n",
       "      <td>-0.941828</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112803</td>\n",
       "      <td>1.656251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704588</td>\n",
       "      <td>0.594713</td>\n",
       "      <td>1.772387</td>\n",
       "      <td>1.243505</td>\n",
       "      <td>0.677384</td>\n",
       "      <td>0.990824</td>\n",
       "      <td>0.782038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1.000000000000000000e+00  8.692932128906250000e-01  \\\n",
       "0                           1.0                  0.907542   \n",
       "1                           1.0                  0.798835   \n",
       "2                           0.0                  1.344385   \n",
       "3                           1.0                  1.105009   \n",
       "4                           0.0                  1.595839   \n",
       "...                         ...                       ...   \n",
       "10995                       1.0                  0.767357   \n",
       "10996                       0.0                  0.306174   \n",
       "10997                       1.0                  0.794809   \n",
       "10998                       0.0                  0.839097   \n",
       "10999                       1.0                  2.265836   \n",
       "\n",
       "       -6.350818276405334473e-01  2.256902605295181274e-01  \\\n",
       "0                       0.329147                  0.359412   \n",
       "1                       1.470639                 -1.635975   \n",
       "2                      -0.876626                  0.935913   \n",
       "3                       0.321356                  1.522401   \n",
       "4                      -0.607811                  0.007075   \n",
       "...                          ...                       ...   \n",
       "10995                   1.084947                  0.299487   \n",
       "10996                  -1.808714                 -1.240359   \n",
       "10997                  -0.059466                  0.526425   \n",
       "10998                   0.202531                  0.619642   \n",
       "10999                  -0.984737                  0.296158   \n",
       "\n",
       "       3.274700641632080078e-01  -6.899932026863098145e-01  \\\n",
       "0                      1.497970                  -0.313010   \n",
       "1                      0.453773                   0.425629   \n",
       "2                      1.992050                   0.882454   \n",
       "3                      0.882808                  -1.205349   \n",
       "4                      1.818450                  -0.111906   \n",
       "...                         ...                        ...   \n",
       "10995                  0.511042                  -0.374340   \n",
       "10996                  1.365489                  -1.740858   \n",
       "10997                  0.736569                   1.662992   \n",
       "10998                  0.497209                  -1.177904   \n",
       "10999                  1.097784                   1.345723   \n",
       "\n",
       "       7.542022466659545898e-01  -2.485731393098831177e-01  \\\n",
       "0                      1.095531                  -0.557525   \n",
       "1                      1.104875                   1.282322   \n",
       "2                      1.786066                  -1.646778   \n",
       "3                      0.681466                  -1.070464   \n",
       "4                      0.847550                  -0.566437   \n",
       "...                         ...                        ...   \n",
       "10995                  0.774997                  -0.218866   \n",
       "10996                  1.695741                   0.731754   \n",
       "10997                  0.614593                  -1.320002   \n",
       "10998                  0.544788                   1.262518   \n",
       "10999                  0.668183                  -0.723884   \n",
       "\n",
       "       -1.092063903808593750e+00  0.000000000000000000e+00  ...  \\\n",
       "0                      -1.588230                  2.173076  ...   \n",
       "1                       1.381664                  0.000000  ...   \n",
       "2                      -0.942383                  0.000000  ...   \n",
       "3                      -0.921871                  0.000000  ...   \n",
       "4                       1.581239                  2.173076  ...   \n",
       "...                          ...                       ...  ...   \n",
       "10995                  -0.547110                  1.086538  ...   \n",
       "10996                   0.156495                  0.000000  ...   \n",
       "10997                  -0.025997                  0.000000  ...   \n",
       "10998                   0.130439                  0.000000  ...   \n",
       "10999                  -0.941828                  2.173076  ...   \n",
       "\n",
       "       -1.045456994324922562e-02  -4.576716944575309753e-02  \\\n",
       "0                      -1.138930                  -0.000819   \n",
       "1                       1.128848                   0.900461   \n",
       "2                      -0.678379                  -1.360356   \n",
       "3                      -0.373566                   0.113041   \n",
       "4                      -0.654227                  -1.274345   \n",
       "...                          ...                        ...   \n",
       "10995                   1.377029                  -1.557350   \n",
       "10996                   1.232118                  -0.143431   \n",
       "10997                  -1.238036                  -1.050715   \n",
       "10998                  -1.053149                  -0.947502   \n",
       "10999                   0.112803                   1.656251   \n",
       "\n",
       "       3.101961374282836914e+00  1.353760004043579102e+00  \\\n",
       "0                      0.000000                  0.302220   \n",
       "1                      0.000000                  0.909753   \n",
       "2                      0.000000                  0.946652   \n",
       "3                      0.000000                  0.755856   \n",
       "4                      3.101961                  0.823761   \n",
       "...                         ...                       ...   \n",
       "10995                  0.000000                  1.077976   \n",
       "10996                  0.000000                  0.886896   \n",
       "10997                  3.101961                  1.443730   \n",
       "10998                  3.101961                  0.806380   \n",
       "10999                  0.000000                  0.704588   \n",
       "\n",
       "       9.795631170272827148e-01  9.780761599540710449e-01  \\\n",
       "0                      0.833048                  0.985700   \n",
       "1                      1.108330                  0.985692   \n",
       "2                      1.028704                  0.998656   \n",
       "3                      1.361057                  0.986610   \n",
       "4                      0.938191                  0.971758   \n",
       "...                         ...                       ...   \n",
       "10995                  1.028331                  0.984388   \n",
       "10996                  0.935988                  0.973462   \n",
       "10997                  1.199071                  0.990352   \n",
       "10998                  1.075156                  0.987699   \n",
       "10999                  0.594713                  1.772387   \n",
       "\n",
       "       9.200048446655273438e-01  7.216574549674987793e-01  \\\n",
       "0                      0.978098                  0.779732   \n",
       "1                      0.951331                  0.803252   \n",
       "2                      0.728281                  0.869200   \n",
       "3                      0.838085                  1.133295   \n",
       "4                      0.789176                  0.430553   \n",
       "...                         ...                       ...   \n",
       "10995                  1.004476                  0.925660   \n",
       "10996                  0.879262                  0.605018   \n",
       "10997                  0.697507                  0.799412   \n",
       "10998                  0.624265                  0.887372   \n",
       "10999                  1.243505                  0.677384   \n",
       "\n",
       "       9.887509346008300781e-01  8.766783475875854492e-01  \n",
       "0                      0.992356                  0.798343  \n",
       "1                      0.865924                  0.780118  \n",
       "2                      1.026736                  0.957904  \n",
       "3                      0.872245                  0.808487  \n",
       "4                      0.961357                  0.957818  \n",
       "...                         ...                       ...  \n",
       "10995                  1.012436                  1.057740  \n",
       "10996                  0.853274                  1.018208  \n",
       "10997                  1.002388                  0.846745  \n",
       "10998                  0.718604                  0.641227  \n",
       "10999                  0.990824                  0.782038  \n",
       "\n",
       "[11000 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000 entries, 0 to 10999\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   1.000000000000000000e+00    11000 non-null  float64\n",
      " 1   8.692932128906250000e-01    11000 non-null  float64\n",
      " 2   -6.350818276405334473e-01   11000 non-null  float64\n",
      " 3   2.256902605295181274e-01    11000 non-null  float64\n",
      " 4   3.274700641632080078e-01    11000 non-null  float64\n",
      " 5   -6.899932026863098145e-01   11000 non-null  float64\n",
      " 6   7.542022466659545898e-01    11000 non-null  float64\n",
      " 7   -2.485731393098831177e-01   11000 non-null  float64\n",
      " 8   -1.092063903808593750e+00   11000 non-null  float64\n",
      " 9   0.000000000000000000e+00    11000 non-null  float64\n",
      " 10  1.374992132186889648e+00    11000 non-null  float64\n",
      " 11  -6.536741852760314941e-01   11000 non-null  float64\n",
      " 12  9.303491115570068359e-01    11000 non-null  float64\n",
      " 13  1.107436060905456543e+00    11000 non-null  float64\n",
      " 14  1.138904333114624023e+00    11000 non-null  float64\n",
      " 15  -1.578198313713073730e+00   11000 non-null  float64\n",
      " 16  -1.046985387802124023e+00   11000 non-null  float64\n",
      " 17  0.000000000000000000e+00.1  11000 non-null  float64\n",
      " 18  6.579295396804809570e-01    11000 non-null  float64\n",
      " 19  -1.045456994324922562e-02   11000 non-null  float64\n",
      " 20  -4.576716944575309753e-02   11000 non-null  float64\n",
      " 21  3.101961374282836914e+00    11000 non-null  float64\n",
      " 22  1.353760004043579102e+00    11000 non-null  float64\n",
      " 23  9.795631170272827148e-01    11000 non-null  float64\n",
      " 24  9.780761599540710449e-01    11000 non-null  float64\n",
      " 25  9.200048446655273438e-01    11000 non-null  float64\n",
      " 26  7.216574549674987793e-01    11000 non-null  float64\n",
      " 27  9.887509346008300781e-01    11000 non-null  float64\n",
      " 28  8.766783475875854492e-01    11000 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkiyUdaWIrww"
   },
   "outputs": [],
   "source": [
    "FEATURES = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFggl9gYKKRJ"
   },
   "source": [
    "The `tf.data.experimental.CsvDataset` class can be used to read csv records directly from a gzip file with no intermediate decompression step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICKZRY7gN-QM"
   },
   "source": [
    "To keep this tutorial relatively short use just the first 1000 samples for validation, and the next 10 000 for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmk49OqZIFZP"
   },
   "outputs": [],
   "source": [
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FP3M9DmvON32"
   },
   "source": [
    "The `Dataset.skip` and `Dataset.take` methods make this easy.\n",
    "\n",
    "At the same time, use the `Dataset.cache` method to ensure that the loader doesn't need to re-read the data form the file on each epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating the Original Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters chosen with subset of Higgs data consisting of 2.6M training examples and 100,000 validation examples. Computational costs did not allow for thorough optimization but included combinations of pre-training methods, network architectures, initial learning rates, regularization methods. Hyperparameter optimization was performed using the full set of Higgs features…classifiers were tested on 500,000 simulated examples generated from same Monte Carlo procedures as training sets. \n",
    "\n",
    "Optimal Model from Paper\n",
    "- Five-layer Neural Network with 300 hidden units in each layer\n",
    "- Learning Rate of 0.05\n",
    "- Weight Decay Coefficient 1 x 10-5\n",
    "- Predetermined without Optimization  \n",
    "    - Hidden units all used tahn activation function\n",
    "    - Weights initialized from normal distribution with 0 mean and standard deviation 0.1 in first layer, 0.001 in output layer, and 0.05in all other hidden layers\n",
    "    - Mini-batches of size 100 to compute gradients\n",
    "    - Momentum increased linearly over first 200 epochs from 0.9 to 0.99, then remained constant\n",
    "    - Learning Rate decayed by 1.0000002 every batch update until it reached a minimum of 10-6\n",
    "- Training ended when momentum reached maximum value and minimum error on validation set of 500,000 examples had not decreased by more than a factor of 0.00001 over 10 epochs. Early stopping prevented overfitting and each NN trained over 200-1000 epochs.\n",
    "- When training with dropout, increased learning rate decay to 1.0000003 and ended training when momentum reached maximum value and error on validation set had not decreased for 40 epochs\n",
    "- Inputs standardized over entire train/test set with mean 0 and standard deviation of 1, except for features with values strictly greater than 0 (scaled for mean value of 1)\n",
    "- An additional boost in performance is obtained by using the dropout training algorithm, in which we stochastically drop neurons in the top hidden layer with 50% probability during training.\n",
    "\n",
    "**Original model built with PyLearn2; NW will write up few sentences about PyLearn2**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>1.084947</td>\n",
       "      <td>0.299487</td>\n",
       "      <td>0.511042</td>\n",
       "      <td>-0.374340</td>\n",
       "      <td>0.774997</td>\n",
       "      <td>-0.218866</td>\n",
       "      <td>-0.547110</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377029</td>\n",
       "      <td>-1.557350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.077976</td>\n",
       "      <td>1.028331</td>\n",
       "      <td>0.984388</td>\n",
       "      <td>1.004476</td>\n",
       "      <td>0.925660</td>\n",
       "      <td>1.012436</td>\n",
       "      <td>1.057740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306174</td>\n",
       "      <td>-1.808714</td>\n",
       "      <td>-1.240359</td>\n",
       "      <td>1.365489</td>\n",
       "      <td>-1.740858</td>\n",
       "      <td>1.695741</td>\n",
       "      <td>0.731754</td>\n",
       "      <td>0.156495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.232118</td>\n",
       "      <td>-0.143431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886896</td>\n",
       "      <td>0.935988</td>\n",
       "      <td>0.973462</td>\n",
       "      <td>0.879262</td>\n",
       "      <td>0.605018</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>1.018208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794809</td>\n",
       "      <td>-0.059466</td>\n",
       "      <td>0.526425</td>\n",
       "      <td>0.736569</td>\n",
       "      <td>1.662992</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>-1.320002</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.238036</td>\n",
       "      <td>-1.050715</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.443730</td>\n",
       "      <td>1.199071</td>\n",
       "      <td>0.990352</td>\n",
       "      <td>0.697507</td>\n",
       "      <td>0.799412</td>\n",
       "      <td>1.002388</td>\n",
       "      <td>0.846745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.839097</td>\n",
       "      <td>0.202531</td>\n",
       "      <td>0.619642</td>\n",
       "      <td>0.497209</td>\n",
       "      <td>-1.177904</td>\n",
       "      <td>0.544788</td>\n",
       "      <td>1.262518</td>\n",
       "      <td>0.130439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.053149</td>\n",
       "      <td>-0.947502</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.806380</td>\n",
       "      <td>1.075156</td>\n",
       "      <td>0.987699</td>\n",
       "      <td>0.624265</td>\n",
       "      <td>0.887372</td>\n",
       "      <td>0.718604</td>\n",
       "      <td>0.641227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.265836</td>\n",
       "      <td>-0.984737</td>\n",
       "      <td>0.296158</td>\n",
       "      <td>1.097784</td>\n",
       "      <td>1.345723</td>\n",
       "      <td>0.668183</td>\n",
       "      <td>-0.723884</td>\n",
       "      <td>-0.941828</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112803</td>\n",
       "      <td>1.656251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704588</td>\n",
       "      <td>0.594713</td>\n",
       "      <td>1.772387</td>\n",
       "      <td>1.243505</td>\n",
       "      <td>0.677384</td>\n",
       "      <td>0.990824</td>\n",
       "      <td>0.782038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6   \\\n",
       "0      1.0  0.907542  0.329147  0.359412  1.497970 -0.313010  1.095531   \n",
       "1      1.0  0.798835  1.470639 -1.635975  0.453773  0.425629  1.104875   \n",
       "2      0.0  1.344385 -0.876626  0.935913  1.992050  0.882454  1.786066   \n",
       "3      1.0  1.105009  0.321356  1.522401  0.882808 -1.205349  0.681466   \n",
       "4      0.0  1.595839 -0.607811  0.007075  1.818450 -0.111906  0.847550   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "10995  1.0  0.767357  1.084947  0.299487  0.511042 -0.374340  0.774997   \n",
       "10996  0.0  0.306174 -1.808714 -1.240359  1.365489 -1.740858  1.695741   \n",
       "10997  1.0  0.794809 -0.059466  0.526425  0.736569  1.662992  0.614593   \n",
       "10998  0.0  0.839097  0.202531  0.619642  0.497209 -1.177904  0.544788   \n",
       "10999  1.0  2.265836 -0.984737  0.296158  1.097784  1.345723  0.668183   \n",
       "\n",
       "             7         8         9   ...        19        20        21  \\\n",
       "0     -0.557525 -1.588230  2.173076  ... -1.138930 -0.000819  0.000000   \n",
       "1      1.282322  1.381664  0.000000  ...  1.128848  0.900461  0.000000   \n",
       "2     -1.646778 -0.942383  0.000000  ... -0.678379 -1.360356  0.000000   \n",
       "3     -1.070464 -0.921871  0.000000  ... -0.373566  0.113041  0.000000   \n",
       "4     -0.566437  1.581239  2.173076  ... -0.654227 -1.274345  3.101961   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10995 -0.218866 -0.547110  1.086538  ...  1.377029 -1.557350  0.000000   \n",
       "10996  0.731754  0.156495  0.000000  ...  1.232118 -0.143431  0.000000   \n",
       "10997 -1.320002 -0.025997  0.000000  ... -1.238036 -1.050715  3.101961   \n",
       "10998  1.262518  0.130439  0.000000  ... -1.053149 -0.947502  3.101961   \n",
       "10999 -0.723884 -0.941828  2.173076  ...  0.112803  1.656251  0.000000   \n",
       "\n",
       "             22        23        24        25        26        27        28  \n",
       "0      0.302220  0.833048  0.985700  0.978098  0.779732  0.992356  0.798343  \n",
       "1      0.909753  1.108330  0.985692  0.951331  0.803252  0.865924  0.780118  \n",
       "2      0.946652  1.028704  0.998656  0.728281  0.869200  1.026736  0.957904  \n",
       "3      0.755856  1.361057  0.986610  0.838085  1.133295  0.872245  0.808487  \n",
       "4      0.823761  0.938191  0.971758  0.789176  0.430553  0.961357  0.957818  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10995  1.077976  1.028331  0.984388  1.004476  0.925660  1.012436  1.057740  \n",
       "10996  0.886896  0.935988  0.973462  0.879262  0.605018  0.853274  1.018208  \n",
       "10997  1.443730  1.199071  0.990352  0.697507  0.799412  1.002388  0.846745  \n",
       "10998  0.806380  1.075156  0.987699  0.624265  0.887372  0.718604  0.641227  \n",
       "10999  0.704588  0.594713  1.772387  1.243505  0.677384  0.990824  0.782038  \n",
       "\n",
       "[11000 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate** – a configurable hyperparameter that controls how quickly/slowly a neural network learns a problem, more specifically it controls how much to change the weights to correct for error during each iteration; a large learning rate allows model to train faster but a cost, where smaller learning rates may yield a better model, requiring more training epochs and smaller batch sizes. \n",
    "- Math: gradient descent algorithm multiples learning rate by gradient, for example value of 0.1 will update weight 10% of the amount it could be updated\n",
    "- Range: 0.0 to 1.0, traditional default 0.1 or 0.01\n",
    "\n",
    "**Learning Rate Decay** – how learning rate changes over training epochs; learning rate decay can be designed where large weight changes happen at the beginning of the process and smaller, fine-tune changes toward the end; another strategy is to decay over a fixed number of training epochs at a small, constant value.\n",
    "\n",
    "**Drop Out** – form of regularization to minimize overfitting; technique that randomly removes/inactivates neurons at each training step, which forces remaining neurons to be more independent because they learn rated not in conjunction/cooperation with neighboring neuron; roughly doubles the number of iterations required for convergence but training time for each epoch is less. **NW N: have picture of Drop Out if we want to include**\n",
    " \n",
    "**Activation Functions** – aka transfer functions; functions that take a weighted sum of all inputs from previous layer and generates an output value for the next layer; for each node, it defines the output of the node given an input or set of inputs. \n",
    "- Tanh activation – a non-linear activation function that outputs values between -1.0 and 1.0 and the center falls around 0; limitations are that it can have limited sensitivity and is prone to saturation in larger, more layered networks due to vanishing gradients. **NW N: have picture of Tanh if we want to include**\n",
    "- Others include: Sigmoid, Softmax, ELU, ReLU, Leaky ReLU\n",
    "\n",
    "**Momentum** – improves the speed of optimization in concert with step size by helping SGD algorithm navigate in relevant/optimal directions; in other words, it adds inertia to the algorithm update process to continue moving in the optimal direction; best to begin with smaller momentum and then increase after passing through larger gradients – momentum can cause learning process to miss or oscillate around the minima. \n",
    "- Math: adds a fraction of the direction of the previous step to a current step\n",
    "- Range: 0.0 to 1.0, traditional default 0.9, 0.99 or 0.5\n",
    "\n",
    "**Types of Gradient Descent** \n",
    "- **Batch Gradient Descent** – batch size is set to total number of examples in the training dataset\n",
    "- **Stochastic Gradient Descent** – batch size is set to one\n",
    "- **Minibatch Gradient Descent** – batch size is set to more than one and less than the total number of examples in the training dataset. \n",
    "\n",
    "**Batch/batch size** – the number of data points/observations used in one iteration (one gradient update) of model training and dictates the number of training observations to be “learned” before updating internal parameters; generally, a larger batch involves more training examples, thus yielding a more stable learning process and accurate estimate. For example, a batch size of 32 means that 32 samples from the training dataset will be used to estimate error gradient before the model weights are updated. *NOTE: batch size and number of batches are different.* \n",
    "\n",
    "**Epoch** – represents the number of completes passes through the training dataset during the learning process, where the learning algorithm loops through a fixed number of epochs and within each, updates the network for each row in the training data; one epoch means that each sample in the training dataset has updated internal parameters; calculated as N / batch size training iterations, where N is the total number of examples. \n",
    "\n",
    "*“You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.”* **NW Q: does this explanation help? If so, I'll figure out where the quote is from to cite.**  **SH this helps the the story**\n",
    "\n",
    "**Iteration** – number of batches needed to complete on epoch\n",
    "\n",
    "**Understanding Iterations, Batches, and Epochs**\n",
    "\n",
    "Dataset with 200 samples\n",
    "Batch size = 5\n",
    "Epochs = 1000\n",
    "- Dataset will be divided into 40 batches, each with 5 samples, model weights will update after each batch of 5 samples\n",
    "- One epoch will involve 40 batches/40 updates to model\n",
    "- 1000 epochs, model will be exposed/passed through whole data 1000 times, total of 40,000 batches during entire training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def schedule( lr):\n",
    "#   if lr > 0.000001:\n",
    "#       return float(lr)\n",
    "#   else:\n",
    "#       return float(0.000001)\n",
    "\n",
    "    \n",
    "# learning rate schedule\n",
    "def schedule(epoch,lr):\n",
    "    initial_learning_rate=0.05\n",
    "    decay = 1.0000002\n",
    "    decay_steps= float(STEPS_PER_EPOCH*1000)\n",
    "    if lr > 0.000001:\n",
    "        LearningRate = initial_learning_rate / (1 + decay * epoch/ decay_steps)  \n",
    "                      \n",
    "        return float(LearningRate)\n",
    "    else:\n",
    "        LearningRate=float(0.000001)\n",
    "        return LearningRate\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_callbacks():\n",
    "    \n",
    "  return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "    tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \n",
    "  return tf.keras.optimizers.SGD(learning_rate=0.05,momentum=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7700 samples, validate on 1650 samples\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch: 0, accuracy:0.5070,  auc_16:0.5658,  loss:0.6988,  val_accuracy:0.4703,  val_auc_16:0.6110,  val_binary_crossentropy:0.6988,  val_loss:0.6823,  val_val_binary_crossentropy:0.6818,  \n",
      ".7700/7700 - 2s - loss: 0.6988 - accuracy: 0.5070 - val_binary_crossentropy: 0.6988 - auc_16: 0.5658 - val_loss: 0.6823 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6818 - val_auc_16: 0.6110\n",
      "Epoch 2/200\n",
      ".7700/7700 - 1s - loss: 0.6730 - accuracy: 0.5581 - val_binary_crossentropy: 0.6730 - auc_16: 0.6342 - val_loss: 0.6778 - val_accuracy: 0.4891 - val_val_binary_crossentropy: 0.6779 - val_auc_16: 0.6204\n",
      "Epoch 3/200\n",
      ".7700/7700 - 1s - loss: 0.6700 - accuracy: 0.5614 - val_binary_crossentropy: 0.6700 - auc_16: 0.6401 - val_loss: 0.6771 - val_accuracy: 0.4915 - val_val_binary_crossentropy: 0.6771 - val_auc_16: 0.6233\n",
      "Epoch 4/200\n",
      ".7700/7700 - 1s - loss: 0.6681 - accuracy: 0.5766 - val_binary_crossentropy: 0.6681 - auc_16: 0.6443 - val_loss: 0.6774 - val_accuracy: 0.4945 - val_val_binary_crossentropy: 0.6771 - val_auc_16: 0.6293\n",
      "Epoch 5/200\n",
      ".7700/7700 - 1s - loss: 0.6687 - accuracy: 0.5726 - val_binary_crossentropy: 0.6687 - auc_16: 0.6456 - val_loss: 0.6749 - val_accuracy: 0.4988 - val_val_binary_crossentropy: 0.6751 - val_auc_16: 0.6352\n",
      "Epoch 6/200\n",
      ".7700/7700 - 1s - loss: 0.6674 - accuracy: 0.5766 - val_binary_crossentropy: 0.6674 - auc_16: 0.6488 - val_loss: 0.6759 - val_accuracy: 0.5030 - val_val_binary_crossentropy: 0.6759 - val_auc_16: 0.6245\n",
      "Epoch 7/200\n",
      ".7700/7700 - 1s - loss: 0.6653 - accuracy: 0.5916 - val_binary_crossentropy: 0.6653 - auc_16: 0.6551 - val_loss: 0.6744 - val_accuracy: 0.5085 - val_val_binary_crossentropy: 0.6744 - val_auc_16: 0.6338\n",
      "Epoch 8/200\n",
      ".7700/7700 - 1s - loss: 0.6652 - accuracy: 0.5938 - val_binary_crossentropy: 0.6652 - auc_16: 0.6575 - val_loss: 0.6719 - val_accuracy: 0.5182 - val_val_binary_crossentropy: 0.6721 - val_auc_16: 0.6305\n",
      "Epoch 9/200\n",
      ".7700/7700 - 1s - loss: 0.6660 - accuracy: 0.5827 - val_binary_crossentropy: 0.6660 - auc_16: 0.6518 - val_loss: 0.6731 - val_accuracy: 0.5109 - val_val_binary_crossentropy: 0.6732 - val_auc_16: 0.6366\n",
      "Epoch 10/200\n",
      ".7700/7700 - 1s - loss: 0.6652 - accuracy: 0.5900 - val_binary_crossentropy: 0.6652 - auc_16: 0.6533 - val_loss: 0.6711 - val_accuracy: 0.5194 - val_val_binary_crossentropy: 0.6713 - val_auc_16: 0.6338\n",
      "Epoch 11/200\n",
      ".7700/7700 - 1s - loss: 0.6647 - accuracy: 0.5917 - val_binary_crossentropy: 0.6647 - auc_16: 0.6555 - val_loss: 0.6735 - val_accuracy: 0.5042 - val_val_binary_crossentropy: 0.6736 - val_auc_16: 0.6357\n",
      "Epoch 12/200\n",
      ".7700/7700 - 1s - loss: 0.6648 - accuracy: 0.5888 - val_binary_crossentropy: 0.6648 - auc_16: 0.6578 - val_loss: 0.6747 - val_accuracy: 0.5079 - val_val_binary_crossentropy: 0.6747 - val_auc_16: 0.6250\n",
      "Epoch 13/200\n",
      ".7700/7700 - 2s - loss: 0.6632 - accuracy: 0.5944 - val_binary_crossentropy: 0.6632 - auc_16: 0.6576 - val_loss: 0.6744 - val_accuracy: 0.5103 - val_val_binary_crossentropy: 0.6744 - val_auc_16: 0.6351\n",
      "Epoch 14/200\n",
      ".7700/7700 - 2s - loss: 0.6630 - accuracy: 0.5974 - val_binary_crossentropy: 0.6630 - auc_16: 0.6617 - val_loss: 0.6764 - val_accuracy: 0.4976 - val_val_binary_crossentropy: 0.6765 - val_auc_16: 0.6326\n",
      "Epoch 15/200\n",
      ".7700/7700 - 2s - loss: 0.6640 - accuracy: 0.5932 - val_binary_crossentropy: 0.6640 - auc_16: 0.6616 - val_loss: 0.6726 - val_accuracy: 0.5152 - val_val_binary_crossentropy: 0.6726 - val_auc_16: 0.6368\n",
      "Epoch 16/200\n",
      ".7700/7700 - 2s - loss: 0.6631 - accuracy: 0.6013 - val_binary_crossentropy: 0.6631 - auc_16: 0.6592 - val_loss: 0.6722 - val_accuracy: 0.5103 - val_val_binary_crossentropy: 0.6722 - val_auc_16: 0.6428\n",
      "Epoch 17/200\n",
      ".7700/7700 - 2s - loss: 0.6628 - accuracy: 0.6012 - val_binary_crossentropy: 0.6628 - auc_16: 0.6626 - val_loss: 0.6695 - val_accuracy: 0.5382 - val_val_binary_crossentropy: 0.6694 - val_auc_16: 0.6381\n",
      "Epoch 18/200\n",
      ".7700/7700 - 2s - loss: 0.6622 - accuracy: 0.6026 - val_binary_crossentropy: 0.6622 - auc_16: 0.6624 - val_loss: 0.6698 - val_accuracy: 0.5255 - val_val_binary_crossentropy: 0.6699 - val_auc_16: 0.6397\n",
      "Epoch 19/200\n",
      ".7700/7700 - 2s - loss: 0.6611 - accuracy: 0.6034 - val_binary_crossentropy: 0.6611 - auc_16: 0.6674 - val_loss: 0.6717 - val_accuracy: 0.5188 - val_val_binary_crossentropy: 0.6714 - val_auc_16: 0.6349\n",
      "Epoch 20/200\n",
      ".7700/7700 - 2s - loss: 0.6625 - accuracy: 0.5999 - val_binary_crossentropy: 0.6625 - auc_16: 0.6643 - val_loss: 0.6706 - val_accuracy: 0.5255 - val_val_binary_crossentropy: 0.6702 - val_auc_16: 0.6399\n",
      "Epoch 21/200\n",
      ".7700/7700 - 2s - loss: 0.6596 - accuracy: 0.6074 - val_binary_crossentropy: 0.6596 - auc_16: 0.6674 - val_loss: 0.6705 - val_accuracy: 0.5200 - val_val_binary_crossentropy: 0.6705 - val_auc_16: 0.6409\n",
      "Epoch 22/200\n",
      ".7700/7700 - 2s - loss: 0.6596 - accuracy: 0.6121 - val_binary_crossentropy: 0.6596 - auc_16: 0.6722 - val_loss: 0.6727 - val_accuracy: 0.5048 - val_val_binary_crossentropy: 0.6724 - val_auc_16: 0.6367\n",
      "Epoch 23/200\n",
      ".7700/7700 - 1s - loss: 0.6591 - accuracy: 0.6084 - val_binary_crossentropy: 0.6591 - auc_16: 0.6714 - val_loss: 0.6752 - val_accuracy: 0.4994 - val_val_binary_crossentropy: 0.6750 - val_auc_16: 0.6377\n",
      "Epoch 24/200\n",
      ".7700/7700 - 1s - loss: 0.6594 - accuracy: 0.6100 - val_binary_crossentropy: 0.6594 - auc_16: 0.6719 - val_loss: 0.6707 - val_accuracy: 0.5212 - val_val_binary_crossentropy: 0.6705 - val_auc_16: 0.6368\n",
      "Epoch 25/200\n",
      ".7700/7700 - 1s - loss: 0.6573 - accuracy: 0.6123 - val_binary_crossentropy: 0.6573 - auc_16: 0.6732 - val_loss: 0.6741 - val_accuracy: 0.4988 - val_val_binary_crossentropy: 0.6740 - val_auc_16: 0.6359\n",
      "Epoch 26/200\n",
      ".7700/7700 - 1s - loss: 0.6576 - accuracy: 0.6157 - val_binary_crossentropy: 0.6576 - auc_16: 0.6723 - val_loss: 0.6758 - val_accuracy: 0.4927 - val_val_binary_crossentropy: 0.6756 - val_auc_16: 0.6304\n",
      "Epoch 27/200\n",
      ".7700/7700 - 1s - loss: 0.6568 - accuracy: 0.6135 - val_binary_crossentropy: 0.6568 - auc_16: 0.6746 - val_loss: 0.6755 - val_accuracy: 0.4873 - val_val_binary_crossentropy: 0.6753 - val_auc_16: 0.6371\n",
      "Epoch 28/200\n",
      ".7700/7700 - 1s - loss: 0.6549 - accuracy: 0.6257 - val_binary_crossentropy: 0.6549 - auc_16: 0.6779 - val_loss: 0.6792 - val_accuracy: 0.4812 - val_val_binary_crossentropy: 0.6792 - val_auc_16: 0.6317\n",
      "Epoch 29/200\n",
      ".7700/7700 - 1s - loss: 0.6542 - accuracy: 0.6260 - val_binary_crossentropy: 0.6542 - auc_16: 0.6817 - val_loss: 0.6748 - val_accuracy: 0.4958 - val_val_binary_crossentropy: 0.6744 - val_auc_16: 0.6254\n",
      "Epoch 30/200\n",
      ".7700/7700 - 1s - loss: 0.6538 - accuracy: 0.6243 - val_binary_crossentropy: 0.6538 - auc_16: 0.6834 - val_loss: 0.6764 - val_accuracy: 0.4897 - val_val_binary_crossentropy: 0.6759 - val_auc_16: 0.6435\n",
      "Epoch 31/200\n",
      ".7700/7700 - 1s - loss: 0.6537 - accuracy: 0.6252 - val_binary_crossentropy: 0.6537 - auc_16: 0.6837 - val_loss: 0.6784 - val_accuracy: 0.4867 - val_val_binary_crossentropy: 0.6781 - val_auc_16: 0.6269\n",
      "Epoch 32/200\n",
      ".7700/7700 - 1s - loss: 0.6519 - accuracy: 0.6321 - val_binary_crossentropy: 0.6519 - auc_16: 0.6843 - val_loss: 0.6855 - val_accuracy: 0.4648 - val_val_binary_crossentropy: 0.6855 - val_auc_16: 0.6181\n",
      "Epoch 33/200\n",
      ".7700/7700 - 1s - loss: 0.6515 - accuracy: 0.6300 - val_binary_crossentropy: 0.6515 - auc_16: 0.6900 - val_loss: 0.6806 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6805 - val_auc_16: 0.6190\n",
      "Epoch 34/200\n",
      ".7700/7700 - 1s - loss: 0.6494 - accuracy: 0.6381 - val_binary_crossentropy: 0.6494 - auc_16: 0.6955 - val_loss: 0.6803 - val_accuracy: 0.4824 - val_val_binary_crossentropy: 0.6801 - val_auc_16: 0.6122\n",
      "Epoch 35/200\n",
      ".7700/7700 - 1s - loss: 0.6508 - accuracy: 0.6316 - val_binary_crossentropy: 0.6508 - auc_16: 0.6899 - val_loss: 0.6812 - val_accuracy: 0.4818 - val_val_binary_crossentropy: 0.6811 - val_auc_16: 0.5962\n",
      "Epoch 36/200\n",
      ".7700/7700 - 1s - loss: 0.6499 - accuracy: 0.6364 - val_binary_crossentropy: 0.6499 - auc_16: 0.6948 - val_loss: 0.6835 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6834 - val_auc_16: 0.5853\n",
      "Epoch 37/200\n",
      ".7700/7700 - 1s - loss: 0.6487 - accuracy: 0.6401 - val_binary_crossentropy: 0.6487 - auc_16: 0.6997 - val_loss: 0.6872 - val_accuracy: 0.4697 - val_val_binary_crossentropy: 0.6872 - val_auc_16: 0.5793\n",
      "Epoch 38/200\n",
      ".7700/7700 - 1s - loss: 0.6480 - accuracy: 0.6394 - val_binary_crossentropy: 0.6480 - auc_16: 0.7005 - val_loss: 0.6866 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6866 - val_auc_16: 0.5924\n",
      "Epoch 39/200\n",
      ".7700/7700 - 1s - loss: 0.6456 - accuracy: 0.6468 - val_binary_crossentropy: 0.6456 - auc_16: 0.7042 - val_loss: 0.6842 - val_accuracy: 0.4745 - val_val_binary_crossentropy: 0.6840 - val_auc_16: 0.5969\n",
      "Epoch 40/200\n",
      ".7700/7700 - 1s - loss: 0.6472 - accuracy: 0.6431 - val_binary_crossentropy: 0.6472 - auc_16: 0.7025 - val_loss: 0.6784 - val_accuracy: 0.4994 - val_val_binary_crossentropy: 0.6781 - val_auc_16: 0.6072\n",
      "Epoch 41/200\n",
      ".7700/7700 - 1s - loss: 0.6460 - accuracy: 0.6427 - val_binary_crossentropy: 0.6460 - auc_16: 0.7023 - val_loss: 0.6874 - val_accuracy: 0.4709 - val_val_binary_crossentropy: 0.6874 - val_auc_16: 0.5768\n",
      "Epoch 42/200\n",
      ".7700/7700 - 1s - loss: 0.6438 - accuracy: 0.6544 - val_binary_crossentropy: 0.6438 - auc_16: 0.7136 - val_loss: 0.6888 - val_accuracy: 0.4661 - val_val_binary_crossentropy: 0.6889 - val_auc_16: 0.5442\n",
      "Epoch 43/200\n",
      ".7700/7700 - 1s - loss: 0.6435 - accuracy: 0.6540 - val_binary_crossentropy: 0.6435 - auc_16: 0.7131 - val_loss: 0.6848 - val_accuracy: 0.4800 - val_val_binary_crossentropy: 0.6848 - val_auc_16: 0.5828\n",
      "Epoch 44/200\n",
      ".7700/7700 - 1s - loss: 0.6412 - accuracy: 0.6569 - val_binary_crossentropy: 0.6412 - auc_16: 0.7179 - val_loss: 0.6881 - val_accuracy: 0.4691 - val_val_binary_crossentropy: 0.6881 - val_auc_16: 0.5611\n",
      "Epoch 45/200\n",
      ".7700/7700 - 1s - loss: 0.6408 - accuracy: 0.6583 - val_binary_crossentropy: 0.6408 - auc_16: 0.7150 - val_loss: 0.6827 - val_accuracy: 0.4970 - val_val_binary_crossentropy: 0.6828 - val_auc_16: 0.5820\n",
      "Epoch 46/200\n",
      ".7700/7700 - 1s - loss: 0.6418 - accuracy: 0.6588 - val_binary_crossentropy: 0.6418 - auc_16: 0.7136 - val_loss: 0.6870 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6869 - val_auc_16: 0.5469\n",
      "Epoch 47/200\n",
      ".7700/7700 - 1s - loss: 0.6409 - accuracy: 0.6575 - val_binary_crossentropy: 0.6409 - auc_16: 0.7169 - val_loss: 0.6858 - val_accuracy: 0.4855 - val_val_binary_crossentropy: 0.6859 - val_auc_16: 0.5664\n",
      "Epoch 48/200\n",
      ".7700/7700 - 1s - loss: 0.6416 - accuracy: 0.6556 - val_binary_crossentropy: 0.6416 - auc_16: 0.7202 - val_loss: 0.6858 - val_accuracy: 0.4873 - val_val_binary_crossentropy: 0.6860 - val_auc_16: 0.5792\n",
      "Epoch 49/200\n",
      ".7700/7700 - 1s - loss: 0.6392 - accuracy: 0.6653 - val_binary_crossentropy: 0.6392 - auc_16: 0.7247 - val_loss: 0.6858 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6860 - val_auc_16: 0.5871\n",
      "Epoch 50/200\n",
      ".7700/7700 - 1s - loss: 0.6378 - accuracy: 0.6648 - val_binary_crossentropy: 0.6378 - auc_16: 0.7241 - val_loss: 0.6833 - val_accuracy: 0.4915 - val_val_binary_crossentropy: 0.6833 - val_auc_16: 0.5897\n",
      "Epoch 51/200\n",
      ".7700/7700 - 1s - loss: 0.6362 - accuracy: 0.6661 - val_binary_crossentropy: 0.6362 - auc_16: 0.7276 - val_loss: 0.6792 - val_accuracy: 0.5170 - val_val_binary_crossentropy: 0.6789 - val_auc_16: 0.6066\n",
      "Epoch 52/200\n",
      ".7700/7700 - 1s - loss: 0.6369 - accuracy: 0.6701 - val_binary_crossentropy: 0.6369 - auc_16: 0.7265 - val_loss: 0.6823 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6824 - val_auc_16: 0.6041\n",
      "Epoch 53/200\n",
      ".7700/7700 - 1s - loss: 0.6369 - accuracy: 0.6687 - val_binary_crossentropy: 0.6369 - auc_16: 0.7276 - val_loss: 0.6855 - val_accuracy: 0.4861 - val_val_binary_crossentropy: 0.6855 - val_auc_16: 0.5927\n",
      "Epoch 54/200\n",
      ".7700/7700 - 1s - loss: 0.6368 - accuracy: 0.6692 - val_binary_crossentropy: 0.6368 - auc_16: 0.7267 - val_loss: 0.6822 - val_accuracy: 0.5000 - val_val_binary_crossentropy: 0.6821 - val_auc_16: 0.6082\n",
      "Epoch 55/200\n",
      ".7700/7700 - 1s - loss: 0.6363 - accuracy: 0.6731 - val_binary_crossentropy: 0.6363 - auc_16: 0.7332 - val_loss: 0.6861 - val_accuracy: 0.4782 - val_val_binary_crossentropy: 0.6862 - val_auc_16: 0.5659\n",
      "Epoch 56/200\n",
      ".7700/7700 - 1s - loss: 0.6355 - accuracy: 0.6712 - val_binary_crossentropy: 0.6355 - auc_16: 0.7344 - val_loss: 0.6850 - val_accuracy: 0.4842 - val_val_binary_crossentropy: 0.6851 - val_auc_16: 0.5830\n",
      "Epoch 57/200\n",
      ".7700/7700 - 1s - loss: 0.6331 - accuracy: 0.6753 - val_binary_crossentropy: 0.6331 - auc_16: 0.7363 - val_loss: 0.6891 - val_accuracy: 0.4667 - val_val_binary_crossentropy: 0.6891 - val_auc_16: 0.5562\n",
      "Epoch 58/200\n",
      ".7700/7700 - 1s - loss: 0.6314 - accuracy: 0.6804 - val_binary_crossentropy: 0.6314 - auc_16: 0.7384 - val_loss: 0.6779 - val_accuracy: 0.5176 - val_val_binary_crossentropy: 0.6778 - val_auc_16: 0.6141\n",
      "Epoch 59/200\n",
      ".7700/7700 - 1s - loss: 0.6322 - accuracy: 0.6769 - val_binary_crossentropy: 0.6322 - auc_16: 0.7387 - val_loss: 0.6799 - val_accuracy: 0.5406 - val_val_binary_crossentropy: 0.6799 - val_auc_16: 0.6029\n",
      "Epoch 60/200\n",
      ".7700/7700 - 1s - loss: 0.6331 - accuracy: 0.6810 - val_binary_crossentropy: 0.6331 - auc_16: 0.7317 - val_loss: 0.6838 - val_accuracy: 0.4842 - val_val_binary_crossentropy: 0.6839 - val_auc_16: 0.5846\n",
      "Epoch 61/200\n",
      ".7700/7700 - 1s - loss: 0.6300 - accuracy: 0.6808 - val_binary_crossentropy: 0.6300 - auc_16: 0.7439 - val_loss: 0.6872 - val_accuracy: 0.4782 - val_val_binary_crossentropy: 0.6872 - val_auc_16: 0.5576\n",
      "Epoch 62/200\n",
      ".7700/7700 - 1s - loss: 0.6295 - accuracy: 0.6855 - val_binary_crossentropy: 0.6295 - auc_16: 0.7415 - val_loss: 0.6900 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6902 - val_auc_16: 0.5516\n",
      "Epoch 63/200\n",
      ".7700/7700 - 1s - loss: 0.6314 - accuracy: 0.6825 - val_binary_crossentropy: 0.6314 - auc_16: 0.7362 - val_loss: 0.6875 - val_accuracy: 0.4745 - val_val_binary_crossentropy: 0.6877 - val_auc_16: 0.5704\n",
      "Epoch 64/200\n",
      ".7700/7700 - 1s - loss: 0.6305 - accuracy: 0.6808 - val_binary_crossentropy: 0.6305 - auc_16: 0.7399 - val_loss: 0.6882 - val_accuracy: 0.4806 - val_val_binary_crossentropy: 0.6882 - val_auc_16: 0.5398\n",
      "Epoch 65/200\n",
      ".7700/7700 - 1s - loss: 0.6278 - accuracy: 0.6918 - val_binary_crossentropy: 0.6278 - auc_16: 0.7450 - val_loss: 0.6841 - val_accuracy: 0.5006 - val_val_binary_crossentropy: 0.6841 - val_auc_16: 0.5885\n",
      "Epoch 66/200\n",
      ".7700/7700 - 1s - loss: 0.6291 - accuracy: 0.6825 - val_binary_crossentropy: 0.6291 - auc_16: 0.7446 - val_loss: 0.6808 - val_accuracy: 0.4970 - val_val_binary_crossentropy: 0.6810 - val_auc_16: 0.6067\n",
      "Epoch 67/200\n",
      ".7700/7700 - 1s - loss: 0.6312 - accuracy: 0.6856 - val_binary_crossentropy: 0.6312 - auc_16: 0.7420 - val_loss: 0.6807 - val_accuracy: 0.4994 - val_val_binary_crossentropy: 0.6808 - val_auc_16: 0.5868\n",
      "Epoch 68/200\n",
      ".7700/7700 - 1s - loss: 0.6258 - accuracy: 0.6944 - val_binary_crossentropy: 0.6258 - auc_16: 0.7500 - val_loss: 0.6858 - val_accuracy: 0.4800 - val_val_binary_crossentropy: 0.6858 - val_auc_16: 0.5665\n",
      "Epoch 69/200\n",
      ".7700/7700 - 1s - loss: 0.6266 - accuracy: 0.6969 - val_binary_crossentropy: 0.6266 - auc_16: 0.7468 - val_loss: 0.6814 - val_accuracy: 0.4927 - val_val_binary_crossentropy: 0.6815 - val_auc_16: 0.5949\n",
      "Epoch 70/200\n",
      ".7700/7700 - 1s - loss: 0.6239 - accuracy: 0.7012 - val_binary_crossentropy: 0.6239 - auc_16: 0.7543 - val_loss: 0.6855 - val_accuracy: 0.4818 - val_val_binary_crossentropy: 0.6854 - val_auc_16: 0.5795\n",
      "Epoch 71/200\n",
      ".7700/7700 - 1s - loss: 0.6244 - accuracy: 0.6982 - val_binary_crossentropy: 0.6244 - auc_16: 0.7505 - val_loss: 0.6829 - val_accuracy: 0.4915 - val_val_binary_crossentropy: 0.6831 - val_auc_16: 0.5832\n",
      "Epoch 72/200\n",
      ".7700/7700 - 1s - loss: 0.6256 - accuracy: 0.6964 - val_binary_crossentropy: 0.6256 - auc_16: 0.7555 - val_loss: 0.6845 - val_accuracy: 0.4848 - val_val_binary_crossentropy: 0.6845 - val_auc_16: 0.5794\n",
      "Epoch 73/200\n",
      ".7700/7700 - 1s - loss: 0.6266 - accuracy: 0.6945 - val_binary_crossentropy: 0.6266 - auc_16: 0.7464 - val_loss: 0.6887 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6886 - val_auc_16: 0.5525\n",
      "Epoch 74/200\n",
      ".7700/7700 - 1s - loss: 0.6223 - accuracy: 0.7019 - val_binary_crossentropy: 0.6223 - auc_16: 0.7543 - val_loss: 0.6865 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6866 - val_auc_16: 0.5564\n",
      "Epoch 75/200\n",
      ".7700/7700 - 1s - loss: 0.6222 - accuracy: 0.7055 - val_binary_crossentropy: 0.6222 - auc_16: 0.7558 - val_loss: 0.6849 - val_accuracy: 0.4824 - val_val_binary_crossentropy: 0.6849 - val_auc_16: 0.5649\n",
      "Epoch 76/200\n",
      ".7700/7700 - 1s - loss: 0.6259 - accuracy: 0.6974 - val_binary_crossentropy: 0.6259 - auc_16: 0.7465 - val_loss: 0.6887 - val_accuracy: 0.4715 - val_val_binary_crossentropy: 0.6885 - val_auc_16: 0.5376\n",
      "Epoch 77/200\n",
      ".7700/7700 - 1s - loss: 0.6237 - accuracy: 0.6984 - val_binary_crossentropy: 0.6237 - auc_16: 0.7519 - val_loss: 0.6839 - val_accuracy: 0.5103 - val_val_binary_crossentropy: 0.6834 - val_auc_16: 0.5952\n",
      "Epoch 78/200\n",
      ".7700/7700 - 1s - loss: 0.6223 - accuracy: 0.6997 - val_binary_crossentropy: 0.6223 - auc_16: 0.7517 - val_loss: 0.6882 - val_accuracy: 0.4897 - val_val_binary_crossentropy: 0.6881 - val_auc_16: 0.5436\n",
      "Epoch 79/200\n",
      ".7700/7700 - 1s - loss: 0.6229 - accuracy: 0.7016 - val_binary_crossentropy: 0.6229 - auc_16: 0.7546 - val_loss: 0.6877 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6877 - val_auc_16: 0.5424\n",
      "Epoch 80/200\n",
      ".7700/7700 - 1s - loss: 0.6207 - accuracy: 0.7040 - val_binary_crossentropy: 0.6207 - auc_16: 0.7591 - val_loss: 0.6795 - val_accuracy: 0.5164 - val_val_binary_crossentropy: 0.6789 - val_auc_16: 0.5926\n",
      "Epoch 81/200\n",
      ".7700/7700 - 1s - loss: 0.6215 - accuracy: 0.7055 - val_binary_crossentropy: 0.6215 - auc_16: 0.7580 - val_loss: 0.6906 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6904 - val_auc_16: 0.5338\n",
      "Epoch 82/200\n",
      ".7700/7700 - 1s - loss: 0.6192 - accuracy: 0.7134 - val_binary_crossentropy: 0.6192 - auc_16: 0.7666 - val_loss: 0.6857 - val_accuracy: 0.4897 - val_val_binary_crossentropy: 0.6855 - val_auc_16: 0.5584\n",
      "Epoch 83/200\n",
      ".7700/7700 - 1s - loss: 0.6197 - accuracy: 0.7066 - val_binary_crossentropy: 0.6197 - auc_16: 0.7625 - val_loss: 0.6843 - val_accuracy: 0.4945 - val_val_binary_crossentropy: 0.6843 - val_auc_16: 0.5632\n",
      "Epoch 84/200\n",
      ".7700/7700 - 1s - loss: 0.6206 - accuracy: 0.7094 - val_binary_crossentropy: 0.6206 - auc_16: 0.7594 - val_loss: 0.6820 - val_accuracy: 0.5018 - val_val_binary_crossentropy: 0.6822 - val_auc_16: 0.5796\n",
      "Epoch 85/200\n",
      ".7700/7700 - 1s - loss: 0.6197 - accuracy: 0.7095 - val_binary_crossentropy: 0.6197 - auc_16: 0.7617 - val_loss: 0.6879 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6878 - val_auc_16: 0.5502\n",
      "Epoch 86/200\n",
      ".7700/7700 - 1s - loss: 0.6171 - accuracy: 0.7165 - val_binary_crossentropy: 0.6171 - auc_16: 0.7660 - val_loss: 0.6871 - val_accuracy: 0.4800 - val_val_binary_crossentropy: 0.6870 - val_auc_16: 0.5512\n",
      "Epoch 87/200\n",
      ".7700/7700 - 1s - loss: 0.6202 - accuracy: 0.7034 - val_binary_crossentropy: 0.6202 - auc_16: 0.7589 - val_loss: 0.6855 - val_accuracy: 0.4824 - val_val_binary_crossentropy: 0.6852 - val_auc_16: 0.5460\n",
      "Epoch 88/200\n",
      ".7700/7700 - 1s - loss: 0.6163 - accuracy: 0.7149 - val_binary_crossentropy: 0.6163 - auc_16: 0.7618 - val_loss: 0.6878 - val_accuracy: 0.4794 - val_val_binary_crossentropy: 0.6876 - val_auc_16: 0.5546\n",
      "Epoch 89/200\n",
      ".7700/7700 - 1s - loss: 0.6157 - accuracy: 0.7184 - val_binary_crossentropy: 0.6157 - auc_16: 0.7653 - val_loss: 0.6915 - val_accuracy: 0.4642 - val_val_binary_crossentropy: 0.6913 - val_auc_16: 0.5137\n",
      "Epoch 90/200\n",
      ".7700/7700 - 1s - loss: 0.6164 - accuracy: 0.7186 - val_binary_crossentropy: 0.6164 - auc_16: 0.7653 - val_loss: 0.6910 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6909 - val_auc_16: 0.5265\n",
      "Epoch 91/200\n",
      ".7700/7700 - 1s - loss: 0.6164 - accuracy: 0.7170 - val_binary_crossentropy: 0.6164 - auc_16: 0.7672 - val_loss: 0.6888 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6887 - val_auc_16: 0.5432\n",
      "Epoch 92/200\n",
      ".7700/7700 - 1s - loss: 0.6167 - accuracy: 0.7149 - val_binary_crossentropy: 0.6167 - auc_16: 0.7681 - val_loss: 0.6892 - val_accuracy: 0.4727 - val_val_binary_crossentropy: 0.6889 - val_auc_16: 0.5412\n",
      "Epoch 93/200\n",
      ".7700/7700 - 1s - loss: 0.6123 - accuracy: 0.7219 - val_binary_crossentropy: 0.6123 - auc_16: 0.7744 - val_loss: 0.6916 - val_accuracy: 0.4642 - val_val_binary_crossentropy: 0.6914 - val_auc_16: 0.5171\n",
      "Epoch 94/200\n",
      ".7700/7700 - 1s - loss: 0.6129 - accuracy: 0.7252 - val_binary_crossentropy: 0.6129 - auc_16: 0.7728 - val_loss: 0.6862 - val_accuracy: 0.4867 - val_val_binary_crossentropy: 0.6864 - val_auc_16: 0.5468\n",
      "Epoch 95/200\n",
      ".7700/7700 - 1s - loss: 0.6122 - accuracy: 0.7266 - val_binary_crossentropy: 0.6122 - auc_16: 0.7752 - val_loss: 0.6918 - val_accuracy: 0.4618 - val_val_binary_crossentropy: 0.6917 - val_auc_16: 0.5166\n",
      "Epoch 96/200\n",
      ".7700/7700 - 1s - loss: 0.6148 - accuracy: 0.7166 - val_binary_crossentropy: 0.6148 - auc_16: 0.7697 - val_loss: 0.6923 - val_accuracy: 0.4636 - val_val_binary_crossentropy: 0.6922 - val_auc_16: 0.5161\n",
      "Epoch 97/200\n",
      ".7700/7700 - 1s - loss: 0.6143 - accuracy: 0.7213 - val_binary_crossentropy: 0.6143 - auc_16: 0.7732 - val_loss: 0.6855 - val_accuracy: 0.4891 - val_val_binary_crossentropy: 0.6859 - val_auc_16: 0.5556\n",
      "Epoch 98/200\n",
      ".7700/7700 - 1s - loss: 0.6152 - accuracy: 0.7184 - val_binary_crossentropy: 0.6152 - auc_16: 0.7669 - val_loss: 0.6887 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6891 - val_auc_16: 0.5388\n",
      "Epoch 99/200\n",
      ".7700/7700 - 1s - loss: 0.6136 - accuracy: 0.7248 - val_binary_crossentropy: 0.6136 - auc_16: 0.7731 - val_loss: 0.6874 - val_accuracy: 0.4788 - val_val_binary_crossentropy: 0.6876 - val_auc_16: 0.5389\n",
      "Epoch 100/200\n",
      ".7700/7700 - 1s - loss: 0.6119 - accuracy: 0.7312 - val_binary_crossentropy: 0.6119 - auc_16: 0.7758 - val_loss: 0.6872 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6876 - val_auc_16: 0.5413\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch: 100, accuracy:0.7239,  auc_16:0.7707,  loss:0.6134,  val_accuracy:0.4776,  val_auc_16:0.5374,  val_binary_crossentropy:0.6134,  val_loss:0.6892,  val_val_binary_crossentropy:0.6893,  \n",
      ".7700/7700 - 1s - loss: 0.6134 - accuracy: 0.7239 - val_binary_crossentropy: 0.6134 - auc_16: 0.7707 - val_loss: 0.6892 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6893 - val_auc_16: 0.5374\n",
      "Epoch 102/200\n",
      ".7700/7700 - 1s - loss: 0.6098 - accuracy: 0.7305 - val_binary_crossentropy: 0.6098 - auc_16: 0.7766 - val_loss: 0.6844 - val_accuracy: 0.5012 - val_val_binary_crossentropy: 0.6841 - val_auc_16: 0.5659\n",
      "Epoch 103/200\n",
      ".7700/7700 - 1s - loss: 0.6104 - accuracy: 0.7274 - val_binary_crossentropy: 0.6104 - auc_16: 0.7766 - val_loss: 0.6892 - val_accuracy: 0.4758 - val_val_binary_crossentropy: 0.6894 - val_auc_16: 0.5284\n",
      "Epoch 104/200\n",
      ".7700/7700 - 1s - loss: 0.6099 - accuracy: 0.7326 - val_binary_crossentropy: 0.6099 - auc_16: 0.7772 - val_loss: 0.6881 - val_accuracy: 0.4830 - val_val_binary_crossentropy: 0.6881 - val_auc_16: 0.5454\n",
      "Epoch 105/200\n",
      ".7700/7700 - 1s - loss: 0.6114 - accuracy: 0.7287 - val_binary_crossentropy: 0.6114 - auc_16: 0.7730 - val_loss: 0.6905 - val_accuracy: 0.4727 - val_val_binary_crossentropy: 0.6905 - val_auc_16: 0.5225\n",
      "Epoch 106/200\n",
      ".7700/7700 - 1s - loss: 0.6102 - accuracy: 0.7331 - val_binary_crossentropy: 0.6102 - auc_16: 0.7768 - val_loss: 0.6908 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6907 - val_auc_16: 0.5259\n",
      "Epoch 107/200\n",
      ".7700/7700 - 1s - loss: 0.6086 - accuracy: 0.7358 - val_binary_crossentropy: 0.6086 - auc_16: 0.7779 - val_loss: 0.6906 - val_accuracy: 0.4691 - val_val_binary_crossentropy: 0.6907 - val_auc_16: 0.5242\n",
      "Epoch 108/200\n",
      ".7700/7700 - 1s - loss: 0.6090 - accuracy: 0.7351 - val_binary_crossentropy: 0.6090 - auc_16: 0.7796 - val_loss: 0.6892 - val_accuracy: 0.4733 - val_val_binary_crossentropy: 0.6893 - val_auc_16: 0.5336\n",
      "Epoch 109/200\n",
      ".7700/7700 - 1s - loss: 0.6094 - accuracy: 0.7339 - val_binary_crossentropy: 0.6094 - auc_16: 0.7784 - val_loss: 0.6908 - val_accuracy: 0.4721 - val_val_binary_crossentropy: 0.6905 - val_auc_16: 0.5381\n",
      "Epoch 110/200\n",
      ".7700/7700 - 1s - loss: 0.6084 - accuracy: 0.7352 - val_binary_crossentropy: 0.6084 - auc_16: 0.7813 - val_loss: 0.6900 - val_accuracy: 0.4733 - val_val_binary_crossentropy: 0.6902 - val_auc_16: 0.5295\n",
      "Epoch 111/200\n",
      ".7700/7700 - 1s - loss: 0.6062 - accuracy: 0.7435 - val_binary_crossentropy: 0.6062 - auc_16: 0.7866 - val_loss: 0.6908 - val_accuracy: 0.4727 - val_val_binary_crossentropy: 0.6909 - val_auc_16: 0.5214\n",
      "Epoch 112/200\n",
      ".7700/7700 - 1s - loss: 0.6041 - accuracy: 0.7451 - val_binary_crossentropy: 0.6041 - auc_16: 0.7877 - val_loss: 0.6904 - val_accuracy: 0.4709 - val_val_binary_crossentropy: 0.6905 - val_auc_16: 0.5205\n",
      "Epoch 113/200\n",
      ".7700/7700 - 1s - loss: 0.6067 - accuracy: 0.7396 - val_binary_crossentropy: 0.6067 - auc_16: 0.7854 - val_loss: 0.6902 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6902 - val_auc_16: 0.5239\n",
      "Epoch 114/200\n",
      ".7700/7700 - 1s - loss: 0.6055 - accuracy: 0.7403 - val_binary_crossentropy: 0.6055 - auc_16: 0.7868 - val_loss: 0.6896 - val_accuracy: 0.4697 - val_val_binary_crossentropy: 0.6895 - val_auc_16: 0.5299\n",
      "Epoch 115/200\n",
      ".7700/7700 - 1s - loss: 0.6045 - accuracy: 0.7430 - val_binary_crossentropy: 0.6045 - auc_16: 0.7863 - val_loss: 0.6896 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6898 - val_auc_16: 0.5345\n",
      "Epoch 116/200\n",
      ".7700/7700 - 1s - loss: 0.6050 - accuracy: 0.7435 - val_binary_crossentropy: 0.6050 - auc_16: 0.7836 - val_loss: 0.6901 - val_accuracy: 0.4758 - val_val_binary_crossentropy: 0.6900 - val_auc_16: 0.5349\n",
      "Epoch 117/200\n",
      ".7700/7700 - 1s - loss: 0.6039 - accuracy: 0.7452 - val_binary_crossentropy: 0.6039 - auc_16: 0.7894 - val_loss: 0.6901 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6904 - val_auc_16: 0.5345\n",
      "Epoch 118/200\n",
      ".7700/7700 - 1s - loss: 0.6027 - accuracy: 0.7500 - val_binary_crossentropy: 0.6027 - auc_16: 0.7905 - val_loss: 0.6881 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6882 - val_auc_16: 0.5357\n",
      "Epoch 119/200\n",
      ".7700/7700 - 1s - loss: 0.6020 - accuracy: 0.7497 - val_binary_crossentropy: 0.6020 - auc_16: 0.7896 - val_loss: 0.6884 - val_accuracy: 0.4830 - val_val_binary_crossentropy: 0.6885 - val_auc_16: 0.5462\n",
      "Epoch 120/200\n",
      ".7700/7700 - 1s - loss: 0.6026 - accuracy: 0.7473 - val_binary_crossentropy: 0.6026 - auc_16: 0.7920 - val_loss: 0.6894 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6895 - val_auc_16: 0.5314\n",
      "Epoch 121/200\n",
      ".7700/7700 - 1s - loss: 0.6044 - accuracy: 0.7445 - val_binary_crossentropy: 0.6044 - auc_16: 0.7918 - val_loss: 0.6891 - val_accuracy: 0.4709 - val_val_binary_crossentropy: 0.6891 - val_auc_16: 0.5286\n",
      "Epoch 122/200\n",
      ".7700/7700 - 1s - loss: 0.6040 - accuracy: 0.7436 - val_binary_crossentropy: 0.6040 - auc_16: 0.7884 - val_loss: 0.6898 - val_accuracy: 0.4709 - val_val_binary_crossentropy: 0.6897 - val_auc_16: 0.5193\n",
      "Epoch 123/200\n",
      ".7700/7700 - 1s - loss: 0.6026 - accuracy: 0.7492 - val_binary_crossentropy: 0.6026 - auc_16: 0.7914 - val_loss: 0.6896 - val_accuracy: 0.4830 - val_val_binary_crossentropy: 0.6897 - val_auc_16: 0.5261\n",
      "Epoch 124/200\n",
      ".7700/7700 - 1s - loss: 0.6028 - accuracy: 0.7473 - val_binary_crossentropy: 0.6028 - auc_16: 0.7880 - val_loss: 0.6861 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6864 - val_auc_16: 0.5562\n",
      "Epoch 125/200\n",
      ".7700/7700 - 1s - loss: 0.6011 - accuracy: 0.7512 - val_binary_crossentropy: 0.6011 - auc_16: 0.7943 - val_loss: 0.6871 - val_accuracy: 0.4873 - val_val_binary_crossentropy: 0.6873 - val_auc_16: 0.5467\n",
      "Epoch 126/200\n",
      ".7700/7700 - 1s - loss: 0.5996 - accuracy: 0.7523 - val_binary_crossentropy: 0.5996 - auc_16: 0.7974 - val_loss: 0.6900 - val_accuracy: 0.4758 - val_val_binary_crossentropy: 0.6897 - val_auc_16: 0.5334\n",
      "Epoch 127/200\n",
      ".7700/7700 - 1s - loss: 0.5989 - accuracy: 0.7573 - val_binary_crossentropy: 0.5989 - auc_16: 0.7992 - val_loss: 0.6891 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6888 - val_auc_16: 0.5320\n",
      "Epoch 128/200\n",
      ".7700/7700 - 1s - loss: 0.5985 - accuracy: 0.7581 - val_binary_crossentropy: 0.5985 - auc_16: 0.7992 - val_loss: 0.6878 - val_accuracy: 0.4842 - val_val_binary_crossentropy: 0.6879 - val_auc_16: 0.5443\n",
      "Epoch 129/200\n",
      ".7700/7700 - 1s - loss: 0.5989 - accuracy: 0.7565 - val_binary_crossentropy: 0.5989 - auc_16: 0.8033 - val_loss: 0.6883 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6883 - val_auc_16: 0.5321\n",
      "Epoch 130/200\n",
      ".7700/7700 - 1s - loss: 0.6009 - accuracy: 0.7534 - val_binary_crossentropy: 0.6009 - auc_16: 0.7981 - val_loss: 0.6861 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6858 - val_auc_16: 0.5481\n",
      "Epoch 131/200\n",
      ".7700/7700 - 1s - loss: 0.5970 - accuracy: 0.7635 - val_binary_crossentropy: 0.5970 - auc_16: 0.8012 - val_loss: 0.6911 - val_accuracy: 0.4721 - val_val_binary_crossentropy: 0.6909 - val_auc_16: 0.5239\n",
      "Epoch 132/200\n",
      ".7700/7700 - 1s - loss: 0.5991 - accuracy: 0.7529 - val_binary_crossentropy: 0.5991 - auc_16: 0.7969 - val_loss: 0.6882 - val_accuracy: 0.4873 - val_val_binary_crossentropy: 0.6881 - val_auc_16: 0.5527\n",
      "Epoch 133/200\n",
      ".7700/7700 - 1s - loss: 0.5978 - accuracy: 0.7601 - val_binary_crossentropy: 0.5978 - auc_16: 0.8016 - val_loss: 0.6932 - val_accuracy: 0.4648 - val_val_binary_crossentropy: 0.6928 - val_auc_16: 0.5108\n",
      "Epoch 134/200\n",
      ".7700/7700 - 1s - loss: 0.5987 - accuracy: 0.7557 - val_binary_crossentropy: 0.5987 - auc_16: 0.8025 - val_loss: 0.6858 - val_accuracy: 0.4915 - val_val_binary_crossentropy: 0.6855 - val_auc_16: 0.5524\n",
      "Epoch 135/200\n",
      ".7700/7700 - 1s - loss: 0.5974 - accuracy: 0.7617 - val_binary_crossentropy: 0.5974 - auc_16: 0.8033 - val_loss: 0.6896 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6893 - val_auc_16: 0.5383\n",
      "Epoch 136/200\n",
      ".7700/7700 - 1s - loss: 0.5980 - accuracy: 0.7575 - val_binary_crossentropy: 0.5980 - auc_16: 0.8017 - val_loss: 0.6884 - val_accuracy: 0.4794 - val_val_binary_crossentropy: 0.6884 - val_auc_16: 0.5310\n",
      "Epoch 137/200\n",
      ".7700/7700 - 1s - loss: 0.5913 - accuracy: 0.7734 - val_binary_crossentropy: 0.5913 - auc_16: 0.8129 - val_loss: 0.6900 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6899 - val_auc_16: 0.5267\n",
      "Epoch 138/200\n",
      ".7700/7700 - 1s - loss: 0.5949 - accuracy: 0.7644 - val_binary_crossentropy: 0.5949 - auc_16: 0.8067 - val_loss: 0.6896 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6892 - val_auc_16: 0.5255\n",
      "Epoch 139/200\n",
      ".7700/7700 - 1s - loss: 0.5953 - accuracy: 0.7634 - val_binary_crossentropy: 0.5953 - auc_16: 0.8061 - val_loss: 0.6915 - val_accuracy: 0.4642 - val_val_binary_crossentropy: 0.6914 - val_auc_16: 0.5148\n",
      "Epoch 140/200\n",
      ".7700/7700 - 1s - loss: 0.5933 - accuracy: 0.7696 - val_binary_crossentropy: 0.5933 - auc_16: 0.8111 - val_loss: 0.6910 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6910 - val_auc_16: 0.5345\n",
      "Epoch 141/200\n",
      ".7700/7700 - 1s - loss: 0.5937 - accuracy: 0.7705 - val_binary_crossentropy: 0.5937 - auc_16: 0.8054 - val_loss: 0.6871 - val_accuracy: 0.4848 - val_val_binary_crossentropy: 0.6869 - val_auc_16: 0.5426\n",
      "Epoch 142/200\n",
      ".7700/7700 - 1s - loss: 0.5950 - accuracy: 0.7674 - val_binary_crossentropy: 0.5950 - auc_16: 0.8087 - val_loss: 0.6889 - val_accuracy: 0.4733 - val_val_binary_crossentropy: 0.6888 - val_auc_16: 0.5206\n",
      "Epoch 143/200\n",
      ".7700/7700 - 1s - loss: 0.5953 - accuracy: 0.7640 - val_binary_crossentropy: 0.5953 - auc_16: 0.8046 - val_loss: 0.6921 - val_accuracy: 0.4661 - val_val_binary_crossentropy: 0.6918 - val_auc_16: 0.5110\n",
      "Epoch 144/200\n",
      ".7700/7700 - 1s - loss: 0.5953 - accuracy: 0.7639 - val_binary_crossentropy: 0.5953 - auc_16: 0.8047 - val_loss: 0.6898 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6895 - val_auc_16: 0.5240\n",
      "Epoch 145/200\n",
      ".7700/7700 - 1s - loss: 0.5941 - accuracy: 0.7662 - val_binary_crossentropy: 0.5941 - auc_16: 0.8100 - val_loss: 0.6907 - val_accuracy: 0.4848 - val_val_binary_crossentropy: 0.6900 - val_auc_16: 0.5388\n",
      "Epoch 146/200\n",
      ".7700/7700 - 1s - loss: 0.5933 - accuracy: 0.7704 - val_binary_crossentropy: 0.5933 - auc_16: 0.8123 - val_loss: 0.6897 - val_accuracy: 0.4733 - val_val_binary_crossentropy: 0.6894 - val_auc_16: 0.5270\n",
      "Epoch 147/200\n",
      ".7700/7700 - 1s - loss: 0.5941 - accuracy: 0.7670 - val_binary_crossentropy: 0.5941 - auc_16: 0.8086 - val_loss: 0.6878 - val_accuracy: 0.4830 - val_val_binary_crossentropy: 0.6872 - val_auc_16: 0.5391\n",
      "Epoch 148/200\n",
      ".7700/7700 - 1s - loss: 0.5937 - accuracy: 0.7700 - val_binary_crossentropy: 0.5937 - auc_16: 0.8100 - val_loss: 0.6881 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6877 - val_auc_16: 0.5285\n",
      "Epoch 149/200\n",
      ".7700/7700 - 1s - loss: 0.5930 - accuracy: 0.7704 - val_binary_crossentropy: 0.5930 - auc_16: 0.8117 - val_loss: 0.6908 - val_accuracy: 0.4709 - val_val_binary_crossentropy: 0.6907 - val_auc_16: 0.5254\n",
      "Epoch 150/200\n",
      ".7700/7700 - 1s - loss: 0.5889 - accuracy: 0.7786 - val_binary_crossentropy: 0.5889 - auc_16: 0.8195 - val_loss: 0.6896 - val_accuracy: 0.4855 - val_val_binary_crossentropy: 0.6894 - val_auc_16: 0.5470\n",
      "Epoch 151/200\n",
      ".7700/7700 - 1s - loss: 0.5918 - accuracy: 0.7752 - val_binary_crossentropy: 0.5918 - auc_16: 0.8106 - val_loss: 0.6924 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6921 - val_auc_16: 0.5310\n",
      "Epoch 152/200\n",
      ".7700/7700 - 1s - loss: 0.5894 - accuracy: 0.7758 - val_binary_crossentropy: 0.5894 - auc_16: 0.8181 - val_loss: 0.6898 - val_accuracy: 0.4903 - val_val_binary_crossentropy: 0.6898 - val_auc_16: 0.5388\n",
      "Epoch 153/200\n",
      ".7700/7700 - 1s - loss: 0.5903 - accuracy: 0.7765 - val_binary_crossentropy: 0.5903 - auc_16: 0.8161 - val_loss: 0.6915 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6910 - val_auc_16: 0.5201\n",
      "Epoch 154/200\n",
      ".7700/7700 - 1s - loss: 0.5863 - accuracy: 0.7875 - val_binary_crossentropy: 0.5863 - auc_16: 0.8254 - val_loss: 0.6864 - val_accuracy: 0.5030 - val_val_binary_crossentropy: 0.6862 - val_auc_16: 0.5560\n",
      "Epoch 155/200\n",
      ".7700/7700 - 1s - loss: 0.5866 - accuracy: 0.7855 - val_binary_crossentropy: 0.5866 - auc_16: 0.8242 - val_loss: 0.6869 - val_accuracy: 0.4945 - val_val_binary_crossentropy: 0.6864 - val_auc_16: 0.5487\n",
      "Epoch 156/200\n",
      ".7700/7700 - 1s - loss: 0.5864 - accuracy: 0.7883 - val_binary_crossentropy: 0.5864 - auc_16: 0.8244 - val_loss: 0.6901 - val_accuracy: 0.4727 - val_val_binary_crossentropy: 0.6899 - val_auc_16: 0.5292\n",
      "Epoch 157/200\n",
      ".7700/7700 - 1s - loss: 0.5869 - accuracy: 0.7852 - val_binary_crossentropy: 0.5869 - auc_16: 0.8221 - val_loss: 0.6915 - val_accuracy: 0.4679 - val_val_binary_crossentropy: 0.6915 - val_auc_16: 0.5078\n",
      "Epoch 158/200\n",
      ".7700/7700 - 1s - loss: 0.5893 - accuracy: 0.7787 - val_binary_crossentropy: 0.5893 - auc_16: 0.8185 - val_loss: 0.6901 - val_accuracy: 0.4788 - val_val_binary_crossentropy: 0.6899 - val_auc_16: 0.5211\n",
      "Epoch 159/200\n",
      ".7700/7700 - 1s - loss: 0.5885 - accuracy: 0.7812 - val_binary_crossentropy: 0.5885 - auc_16: 0.8179 - val_loss: 0.6888 - val_accuracy: 0.4818 - val_val_binary_crossentropy: 0.6884 - val_auc_16: 0.5322\n",
      "Epoch 160/200\n",
      ".7700/7700 - 1s - loss: 0.5879 - accuracy: 0.7821 - val_binary_crossentropy: 0.5879 - auc_16: 0.8204 - val_loss: 0.6909 - val_accuracy: 0.4806 - val_val_binary_crossentropy: 0.6905 - val_auc_16: 0.5258\n",
      "Epoch 161/200\n",
      ".7700/7700 - 1s - loss: 0.5867 - accuracy: 0.7825 - val_binary_crossentropy: 0.5867 - auc_16: 0.8215 - val_loss: 0.6881 - val_accuracy: 0.4958 - val_val_binary_crossentropy: 0.6878 - val_auc_16: 0.5523\n",
      "Epoch 162/200\n",
      ".7700/7700 - 1s - loss: 0.5878 - accuracy: 0.7806 - val_binary_crossentropy: 0.5878 - auc_16: 0.8229 - val_loss: 0.6855 - val_accuracy: 0.5073 - val_val_binary_crossentropy: 0.6857 - val_auc_16: 0.5605\n",
      "Epoch 163/200\n",
      ".7700/7700 - 1s - loss: 0.5889 - accuracy: 0.7792 - val_binary_crossentropy: 0.5889 - auc_16: 0.8208 - val_loss: 0.6888 - val_accuracy: 0.4897 - val_val_binary_crossentropy: 0.6889 - val_auc_16: 0.5367\n",
      "Epoch 164/200\n",
      ".7700/7700 - 1s - loss: 0.5865 - accuracy: 0.7860 - val_binary_crossentropy: 0.5865 - auc_16: 0.8271 - val_loss: 0.6930 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6921 - val_auc_16: 0.5257\n",
      "Epoch 165/200\n",
      ".7700/7700 - 1s - loss: 0.5880 - accuracy: 0.7827 - val_binary_crossentropy: 0.5880 - auc_16: 0.8215 - val_loss: 0.6888 - val_accuracy: 0.4952 - val_val_binary_crossentropy: 0.6882 - val_auc_16: 0.5475\n",
      "Epoch 166/200\n",
      ".7700/7700 - 1s - loss: 0.5841 - accuracy: 0.7873 - val_binary_crossentropy: 0.5841 - auc_16: 0.8261 - val_loss: 0.6911 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6908 - val_auc_16: 0.5415\n",
      "Epoch 167/200\n",
      ".7700/7700 - 1s - loss: 0.5844 - accuracy: 0.7906 - val_binary_crossentropy: 0.5844 - auc_16: 0.8266 - val_loss: 0.6932 - val_accuracy: 0.4752 - val_val_binary_crossentropy: 0.6924 - val_auc_16: 0.5357\n",
      "Epoch 168/200\n",
      ".7700/7700 - 1s - loss: 0.5848 - accuracy: 0.7887 - val_binary_crossentropy: 0.5848 - auc_16: 0.8276 - val_loss: 0.6881 - val_accuracy: 0.4903 - val_val_binary_crossentropy: 0.6876 - val_auc_16: 0.5498\n",
      "Epoch 169/200\n",
      ".7700/7700 - 1s - loss: 0.5832 - accuracy: 0.7919 - val_binary_crossentropy: 0.5832 - auc_16: 0.8297 - val_loss: 0.6939 - val_accuracy: 0.4624 - val_val_binary_crossentropy: 0.6939 - val_auc_16: 0.5063\n",
      "Epoch 170/200\n",
      ".7700/7700 - 1s - loss: 0.5864 - accuracy: 0.7827 - val_binary_crossentropy: 0.5864 - auc_16: 0.8246 - val_loss: 0.6931 - val_accuracy: 0.4770 - val_val_binary_crossentropy: 0.6928 - val_auc_16: 0.5322\n",
      "Epoch 171/200\n",
      ".7700/7700 - 1s - loss: 0.5834 - accuracy: 0.7927 - val_binary_crossentropy: 0.5834 - auc_16: 0.8290 - val_loss: 0.6887 - val_accuracy: 0.4939 - val_val_binary_crossentropy: 0.6883 - val_auc_16: 0.5508\n",
      "Epoch 172/200\n",
      ".7700/7700 - 1s - loss: 0.5844 - accuracy: 0.7884 - val_binary_crossentropy: 0.5844 - auc_16: 0.8343 - val_loss: 0.6934 - val_accuracy: 0.4642 - val_val_binary_crossentropy: 0.6929 - val_auc_16: 0.5111\n",
      "Epoch 173/200\n",
      ".7700/7700 - 1s - loss: 0.5836 - accuracy: 0.7891 - val_binary_crossentropy: 0.5836 - auc_16: 0.8322 - val_loss: 0.6926 - val_accuracy: 0.4818 - val_val_binary_crossentropy: 0.6924 - val_auc_16: 0.5284\n",
      "Epoch 174/200\n",
      ".7700/7700 - 1s - loss: 0.5835 - accuracy: 0.7918 - val_binary_crossentropy: 0.5835 - auc_16: 0.8296 - val_loss: 0.6896 - val_accuracy: 0.4952 - val_val_binary_crossentropy: 0.6893 - val_auc_16: 0.5399\n",
      "Epoch 175/200\n",
      ".7700/7700 - 1s - loss: 0.5834 - accuracy: 0.7926 - val_binary_crossentropy: 0.5834 - auc_16: 0.8327 - val_loss: 0.6889 - val_accuracy: 0.5097 - val_val_binary_crossentropy: 0.6885 - val_auc_16: 0.5541\n",
      "Epoch 176/200\n",
      ".7700/7700 - 1s - loss: 0.5839 - accuracy: 0.7932 - val_binary_crossentropy: 0.5839 - auc_16: 0.8312 - val_loss: 0.6911 - val_accuracy: 0.4848 - val_val_binary_crossentropy: 0.6911 - val_auc_16: 0.5403\n",
      "Epoch 177/200\n",
      ".7700/7700 - 1s - loss: 0.5839 - accuracy: 0.7931 - val_binary_crossentropy: 0.5839 - auc_16: 0.8330 - val_loss: 0.6938 - val_accuracy: 0.4624 - val_val_binary_crossentropy: 0.6936 - val_auc_16: 0.5055\n",
      "Epoch 178/200\n",
      ".7700/7700 - 1s - loss: 0.5807 - accuracy: 0.7982 - val_binary_crossentropy: 0.5807 - auc_16: 0.8370 - val_loss: 0.6857 - val_accuracy: 0.4970 - val_val_binary_crossentropy: 0.6854 - val_auc_16: 0.5510\n",
      "Epoch 179/200\n",
      ".7700/7700 - 1s - loss: 0.5833 - accuracy: 0.7947 - val_binary_crossentropy: 0.5833 - auc_16: 0.8358 - val_loss: 0.6892 - val_accuracy: 0.4794 - val_val_binary_crossentropy: 0.6887 - val_auc_16: 0.5289\n",
      "Epoch 180/200\n",
      ".7700/7700 - 1s - loss: 0.5800 - accuracy: 0.7951 - val_binary_crossentropy: 0.5800 - auc_16: 0.8377 - val_loss: 0.6860 - val_accuracy: 0.4867 - val_val_binary_crossentropy: 0.6857 - val_auc_16: 0.5406\n",
      "Epoch 181/200\n",
      ".7700/7700 - 1s - loss: 0.5817 - accuracy: 0.7938 - val_binary_crossentropy: 0.5817 - auc_16: 0.8332 - val_loss: 0.6853 - val_accuracy: 0.4982 - val_val_binary_crossentropy: 0.6848 - val_auc_16: 0.5476\n",
      "Epoch 182/200\n",
      ".7700/7700 - 1s - loss: 0.5807 - accuracy: 0.7977 - val_binary_crossentropy: 0.5807 - auc_16: 0.8393 - val_loss: 0.6895 - val_accuracy: 0.4812 - val_val_binary_crossentropy: 0.6897 - val_auc_16: 0.5399\n",
      "Epoch 183/200\n",
      ".7700/7700 - 1s - loss: 0.5787 - accuracy: 0.8013 - val_binary_crossentropy: 0.5787 - auc_16: 0.8421 - val_loss: 0.6924 - val_accuracy: 0.4745 - val_val_binary_crossentropy: 0.6921 - val_auc_16: 0.5239\n",
      "Epoch 184/200\n",
      ".7700/7700 - 1s - loss: 0.5801 - accuracy: 0.7997 - val_binary_crossentropy: 0.5801 - auc_16: 0.8383 - val_loss: 0.6912 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6908 - val_auc_16: 0.5235\n",
      "Epoch 185/200\n",
      ".7700/7700 - 1s - loss: 0.5763 - accuracy: 0.8077 - val_binary_crossentropy: 0.5763 - auc_16: 0.8430 - val_loss: 0.6889 - val_accuracy: 0.4921 - val_val_binary_crossentropy: 0.6886 - val_auc_16: 0.5364\n",
      "Epoch 186/200\n",
      ".7700/7700 - 1s - loss: 0.5812 - accuracy: 0.7969 - val_binary_crossentropy: 0.5812 - auc_16: 0.8374 - val_loss: 0.6894 - val_accuracy: 0.4861 - val_val_binary_crossentropy: 0.6889 - val_auc_16: 0.5313\n",
      "Epoch 187/200\n",
      ".7700/7700 - 1s - loss: 0.5791 - accuracy: 0.8036 - val_binary_crossentropy: 0.5791 - auc_16: 0.8410 - val_loss: 0.6888 - val_accuracy: 0.4867 - val_val_binary_crossentropy: 0.6883 - val_auc_16: 0.5321\n",
      "Epoch 188/200\n",
      ".7700/7700 - 1s - loss: 0.5788 - accuracy: 0.8005 - val_binary_crossentropy: 0.5788 - auc_16: 0.8391 - val_loss: 0.6879 - val_accuracy: 0.4885 - val_val_binary_crossentropy: 0.6882 - val_auc_16: 0.5382\n",
      "Epoch 189/200\n",
      ".7700/7700 - 1s - loss: 0.5773 - accuracy: 0.8057 - val_binary_crossentropy: 0.5773 - auc_16: 0.8399 - val_loss: 0.6900 - val_accuracy: 0.4788 - val_val_binary_crossentropy: 0.6900 - val_auc_16: 0.5297\n",
      "Epoch 190/200\n",
      ".7700/7700 - 1s - loss: 0.5794 - accuracy: 0.7995 - val_binary_crossentropy: 0.5794 - auc_16: 0.8399 - val_loss: 0.6940 - val_accuracy: 0.4703 - val_val_binary_crossentropy: 0.6939 - val_auc_16: 0.5232\n",
      "Epoch 191/200\n",
      ".7700/7700 - 1s - loss: 0.5768 - accuracy: 0.8066 - val_binary_crossentropy: 0.5768 - auc_16: 0.8444 - val_loss: 0.6886 - val_accuracy: 0.4939 - val_val_binary_crossentropy: 0.6884 - val_auc_16: 0.5491\n",
      "Epoch 192/200\n",
      ".7700/7700 - 1s - loss: 0.5783 - accuracy: 0.8023 - val_binary_crossentropy: 0.5783 - auc_16: 0.8447 - val_loss: 0.6908 - val_accuracy: 0.4818 - val_val_binary_crossentropy: 0.6908 - val_auc_16: 0.5337\n",
      "Epoch 193/200\n",
      ".7700/7700 - 1s - loss: 0.5763 - accuracy: 0.8075 - val_binary_crossentropy: 0.5763 - auc_16: 0.8429 - val_loss: 0.6901 - val_accuracy: 0.4842 - val_val_binary_crossentropy: 0.6899 - val_auc_16: 0.5317\n",
      "Epoch 194/200\n",
      ".7700/7700 - 1s - loss: 0.5776 - accuracy: 0.8060 - val_binary_crossentropy: 0.5776 - auc_16: 0.8451 - val_loss: 0.6919 - val_accuracy: 0.4830 - val_val_binary_crossentropy: 0.6915 - val_auc_16: 0.5286\n",
      "Epoch 195/200\n",
      ".7700/7700 - 1s - loss: 0.5745 - accuracy: 0.8155 - val_binary_crossentropy: 0.5745 - auc_16: 0.8496 - val_loss: 0.6879 - val_accuracy: 0.4891 - val_val_binary_crossentropy: 0.6880 - val_auc_16: 0.5395\n",
      "Epoch 196/200\n",
      ".7700/7700 - 1s - loss: 0.5747 - accuracy: 0.8117 - val_binary_crossentropy: 0.5747 - auc_16: 0.8496 - val_loss: 0.6923 - val_accuracy: 0.4764 - val_val_binary_crossentropy: 0.6923 - val_auc_16: 0.5213\n",
      "Epoch 197/200\n",
      ".7700/7700 - 1s - loss: 0.5737 - accuracy: 0.8136 - val_binary_crossentropy: 0.5737 - auc_16: 0.8500 - val_loss: 0.6933 - val_accuracy: 0.4685 - val_val_binary_crossentropy: 0.6937 - val_auc_16: 0.5047\n",
      "Epoch 198/200\n",
      ".7700/7700 - 1s - loss: 0.5746 - accuracy: 0.8126 - val_binary_crossentropy: 0.5746 - auc_16: 0.8538 - val_loss: 0.6923 - val_accuracy: 0.4776 - val_val_binary_crossentropy: 0.6920 - val_auc_16: 0.5247\n",
      "Epoch 199/200\n",
      ".7700/7700 - 1s - loss: 0.5766 - accuracy: 0.8091 - val_binary_crossentropy: 0.5766 - auc_16: 0.8468 - val_loss: 0.6913 - val_accuracy: 0.4842 - val_val_binary_crossentropy: 0.6909 - val_auc_16: 0.5293\n",
      "Epoch 200/200\n",
      ".7700/7700 - 1s - loss: 0.5740 - accuracy: 0.8125 - val_binary_crossentropy: 0.5740 - auc_16: 0.8532 - val_loss: 0.6909 - val_accuracy: 0.4879 - val_val_binary_crossentropy: 0.6906 - val_auc_16: 0.5416\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.layers import LeakyReLU\n",
    "#import pandas as pd\n",
    "#import io\n",
    "#import os\n",
    "#import requests\n",
    "#import numpy as np\n",
    "#from sklearn import metrics\n",
    "#from tensorflow.keras import optimizers\n",
    "\n",
    "#Parms needed for case study  \n",
    "\n",
    "#We selected a five-layer neural network with 300 hidden units in each layer,\n",
    "#a learning rate of 0.05, and a weight decay coefficient of 1 × 10−5.\n",
    "# Hidden layer have tanh activation function\n",
    "#Gradient computations were made on mini-batches of size 100\n",
    "#The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10^−6,\n",
    "\n",
    "def define_predictor(n_input):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(300, kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), input_dim = n_input, activation='tanh')) # Hidden 1\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 2\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 3\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 4\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 5\n",
    "    model.add(tf.keras.layers.Dense(1,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None),activation='sigmoid')) # Output #1.2\n",
    "    #model.add(tf.keras.layers.Dense(1,activation='softmax')) # Output\n",
    "    #sgd = tf.keras.optimizers.SGD(lr=.05, decay = 1.0000002, momentum=0.99)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=get_optimizer() , \n",
    "                  metrics=['accuracy', \n",
    "                            tf.keras.losses.BinaryCrossentropy(\n",
    "                            from_logits=True, name='val_binary_crossentropy'), \n",
    "                            tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "    \n",
    "#setup scaler\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "dfx\n",
    "X = dfx.loc[:, 1:28]\n",
    "#X = dfx[features].copy()\n",
    "Y = dfx[0].copy()\n",
    "y = Y.values\n",
    "\n",
    "# the cv=cvx parameter sets the grid search to split the training and testing data 10 times. \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, test_size = 0.5, random_state = 101)\n",
    "#N_VALIDATION = int(1e3)\n",
    "N_TRAIN = len(X_train)\n",
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
    "\n",
    "Att_model = define_predictor(X_train.shape[1])\n",
    "history = Att_model.fit(scaler.fit_transform(X_train),\n",
    "                        np.array(y_train),\n",
    "                        callbacks=get_callbacks(),\n",
    "                        verbose=2, \n",
    "                        steps_per_epoch = STEPS_PER_EPOCH, \n",
    "                        epochs=200, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data =  ( X_validate, y_validate ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650/1650 [==============================] - 0s 54us/sample - loss: 0.6578 - accuracy: 0.6491 - val_binary_crossentropy: 0.6584 - auc_16: 0.6799\n"
     ]
    }
   ],
   "source": [
    "results = Att_model.evaluate(scaler.fit_transform(X_test),np.array(y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6578117175535723, 0.6490909, 0.6583682, 0.67988455]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Att_model.predict(scaler.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2645105e-05],\n",
       "       [1.3921408e-05],\n",
       "       [1.2070360e-07],\n",
       "       ...,\n",
       "       [9.9747002e-01],\n",
       "       [9.9997723e-01],\n",
       "       [7.9552429e-08]], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.264511e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.392141e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.207036e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.626197e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.890253e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0  1.264511e-05\n",
       "1  1.392141e-05\n",
       "2  1.207036e-07\n",
       "3  6.626197e-08\n",
       "4  2.890253e-06"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame(predictions)\n",
    "preddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddf['round'] = preddf.round(preddf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.264511e-05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.392141e-05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.207036e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.626197e-08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.890253e-06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>3.229860e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>9.966354e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>9.974700e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>9.999772e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>7.955243e-08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  round\n",
       "0     1.264511e-05    0.0\n",
       "1     1.392141e-05    0.0\n",
       "2     1.207036e-07    0.0\n",
       "3     6.626197e-08    0.0\n",
       "4     2.890253e-06    0.0\n",
       "...            ...    ...\n",
       "1645  3.229860e-07    0.0\n",
       "1646  9.966354e-01    1.0\n",
       "1647  9.974700e-01    1.0\n",
       "1648  9.999772e-01    1.0\n",
       "1649  7.955243e-08    0.0\n",
       "\n",
       "[1650 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "yresultsround = preddf['round'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yresultsround[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1650,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybinary = label_binarize(y_test, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = label_binarize(yresultsround, classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produced Receiver Operating Characteristic (ROC) curves to illustrate performance, metric for comparison was area under the ROC curve (AUC), with larger values indicating higher classification accuracy across a range of threshold choices. Directly connected to classification accuracy; standard in ML, correlated to other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_classes = ybinary.shape[1]\n",
    "#X1_train, X1_test, y1_train, y1_test\n",
    "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(ybinary[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ybinary.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "#Plot of a ROC curve for a specific class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.26163522, 1.        ])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6522233256096215, 'micro': 0.6522233256096215}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic Higgs')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 370,201\n",
      "Trainable params: 370,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Att_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6988053631472897,\n",
       "  0.6729618001293827,\n",
       "  0.6700219032052276,\n",
       "  0.6680652878501199,\n",
       "  0.6686523874084671,\n",
       "  0.6673628440150967,\n",
       "  0.6652503346467947,\n",
       "  0.665189891666561,\n",
       "  0.6660237606469687,\n",
       "  0.6651811313319516,\n",
       "  0.6647168483052935,\n",
       "  0.6647767354915668,\n",
       "  0.6632420219384231,\n",
       "  0.6630454218232786,\n",
       "  0.6639981989736681,\n",
       "  0.663050314048668,\n",
       "  0.6627510530608041,\n",
       "  0.6621634627317453,\n",
       "  0.6610848067642806,\n",
       "  0.6624972549351779,\n",
       "  0.6595791183508836,\n",
       "  0.659564034505324,\n",
       "  0.6591425499358734,\n",
       "  0.6593509269999219,\n",
       "  0.6572725184552082,\n",
       "  0.6576365851736689,\n",
       "  0.6567838308099029,\n",
       "  0.6548877621625925,\n",
       "  0.6541923106490791,\n",
       "  0.6538002235548837,\n",
       "  0.6537352746183221,\n",
       "  0.651894020569789,\n",
       "  0.6514623854067418,\n",
       "  0.6494092012380625,\n",
       "  0.6508492386186278,\n",
       "  0.6499072173973183,\n",
       "  0.6487365666922037,\n",
       "  0.6480445768926051,\n",
       "  0.6456331974500186,\n",
       "  0.6472331564147751,\n",
       "  0.6460090573731955,\n",
       "  0.6438161366945737,\n",
       "  0.6434759892426528,\n",
       "  0.6411932033377808,\n",
       "  0.6408245060350988,\n",
       "  0.6418154711847182,\n",
       "  0.6409295348377971,\n",
       "  0.6416480185149552,\n",
       "  0.6392170585595168,\n",
       "  0.6377734456743512,\n",
       "  0.6362099709448876,\n",
       "  0.6368892695996669,\n",
       "  0.6368529254739935,\n",
       "  0.6368169103349958,\n",
       "  0.636268063025041,\n",
       "  0.6354832378300753,\n",
       "  0.6331270685443631,\n",
       "  0.6313687725500627,\n",
       "  0.6322187107878846,\n",
       "  0.6331165448411719,\n",
       "  0.6300100815760625,\n",
       "  0.6295464069812329,\n",
       "  0.631410044509095,\n",
       "  0.6304780740242499,\n",
       "  0.6277702350121039,\n",
       "  0.6291247993320613,\n",
       "  0.6312463492542119,\n",
       "  0.6258283721936213,\n",
       "  0.6266343725192083,\n",
       "  0.6239068717151494,\n",
       "  0.624378536428724,\n",
       "  0.6255920041691173,\n",
       "  0.626571101801736,\n",
       "  0.6223166375965267,\n",
       "  0.6222129626707598,\n",
       "  0.6258987567641519,\n",
       "  0.6237446224534666,\n",
       "  0.6222788335440995,\n",
       "  0.6229368874004909,\n",
       "  0.6207052538921307,\n",
       "  0.6214519599815468,\n",
       "  0.6192019837243217,\n",
       "  0.619691595628664,\n",
       "  0.6206354018929717,\n",
       "  0.6196616895787128,\n",
       "  0.6170531867386458,\n",
       "  0.6202168689145671,\n",
       "  0.6162999984505889,\n",
       "  0.6157372679029193,\n",
       "  0.6163814733554791,\n",
       "  0.6164240883542346,\n",
       "  0.6166802513134944,\n",
       "  0.6123386946591464,\n",
       "  0.6128572403610527,\n",
       "  0.6121710554345862,\n",
       "  0.6147576075095635,\n",
       "  0.6142779813184367,\n",
       "  0.6151745327107319,\n",
       "  0.6136430718682029,\n",
       "  0.6119292703541842,\n",
       "  0.6134466768859269,\n",
       "  0.6097852069062072,\n",
       "  0.6103705500627493,\n",
       "  0.609932513206036,\n",
       "  0.6114299769525404,\n",
       "  0.6101877588730353,\n",
       "  0.6085533267491824,\n",
       "  0.6089953780174255,\n",
       "  0.6093840266202951,\n",
       "  0.6083594266470377,\n",
       "  0.6061701658484223,\n",
       "  0.604095877765061,\n",
       "  0.6066692688248374,\n",
       "  0.6055479901177543,\n",
       "  0.6045021074158805,\n",
       "  0.6049610577620469,\n",
       "  0.6039255516869682,\n",
       "  0.6027400780033756,\n",
       "  0.602030818338518,\n",
       "  0.602638228373094,\n",
       "  0.6044143028073496,\n",
       "  0.6039539792320945,\n",
       "  0.602560734594023,\n",
       "  0.6028143809987353,\n",
       "  0.6010563567087248,\n",
       "  0.5996022967549114,\n",
       "  0.5989150404930115,\n",
       "  0.5984723947264932,\n",
       "  0.5988943398772896,\n",
       "  0.6009025720806865,\n",
       "  0.5970392707106355,\n",
       "  0.5991338235991341,\n",
       "  0.5978156901025152,\n",
       "  0.5986569415439259,\n",
       "  0.5973608904070669,\n",
       "  0.5979986020496914,\n",
       "  0.591311762859295,\n",
       "  0.5948665451693844,\n",
       "  0.5953386116337467,\n",
       "  0.593321228182161,\n",
       "  0.5936818641501588,\n",
       "  0.5950446693928211,\n",
       "  0.5953089486468922,\n",
       "  0.5953072263048841,\n",
       "  0.5941417735892457,\n",
       "  0.5932894612287546,\n",
       "  0.5940888299570455,\n",
       "  0.5937258144477745,\n",
       "  0.5929994722465416,\n",
       "  0.5889349742369219,\n",
       "  0.5918235074390065,\n",
       "  0.5893737712463776,\n",
       "  0.5903128579065398,\n",
       "  0.5862943413969758,\n",
       "  0.5865962056370525,\n",
       "  0.5864341529932889,\n",
       "  0.5868935662430602,\n",
       "  0.589277108768364,\n",
       "  0.5884869276703178,\n",
       "  0.5879049572077665,\n",
       "  0.5866507516278849,\n",
       "  0.5878173267686522,\n",
       "  0.588877721266313,\n",
       "  0.5864524918717223,\n",
       "  0.5879812883092211,\n",
       "  0.5840968830244881,\n",
       "  0.5844368237953681,\n",
       "  0.5847981688264129,\n",
       "  0.5832488591020758,\n",
       "  0.586405826079381,\n",
       "  0.5834395676464229,\n",
       "  0.5844448100436818,\n",
       "  0.5836102219371052,\n",
       "  0.5834859067743475,\n",
       "  0.5833661811692374,\n",
       "  0.5838963327469764,\n",
       "  0.5839450359344482,\n",
       "  0.5807097887063956,\n",
       "  0.5832662489507106,\n",
       "  0.579974272034385,\n",
       "  0.5817360847027271,\n",
       "  0.5806743688397593,\n",
       "  0.5786668898223283,\n",
       "  0.5801470511919492,\n",
       "  0.5763013873781476,\n",
       "  0.5811587966881789,\n",
       "  0.5790507979207224,\n",
       "  0.5788195961481565,\n",
       "  0.577305217842003,\n",
       "  0.579378853370617,\n",
       "  0.5768399555961807,\n",
       "  0.5782662877788791,\n",
       "  0.5763062091616841,\n",
       "  0.5776221322548853,\n",
       "  0.5745312757306285,\n",
       "  0.5746512451729218,\n",
       "  0.5736977179329117,\n",
       "  0.5746200742659631,\n",
       "  0.5766048593954607,\n",
       "  0.574046455420457],\n",
       " 'accuracy': [0.50701296,\n",
       "  0.55805194,\n",
       "  0.56142855,\n",
       "  0.5766234,\n",
       "  0.5725974,\n",
       "  0.5766234,\n",
       "  0.59155846,\n",
       "  0.5937662,\n",
       "  0.58272725,\n",
       "  0.59,\n",
       "  0.59168833,\n",
       "  0.5888312,\n",
       "  0.5944156,\n",
       "  0.5974026,\n",
       "  0.59324676,\n",
       "  0.6012987,\n",
       "  0.6011688,\n",
       "  0.6025974,\n",
       "  0.6033766,\n",
       "  0.59987015,\n",
       "  0.6074026,\n",
       "  0.61207795,\n",
       "  0.60844153,\n",
       "  0.61,\n",
       "  0.61233765,\n",
       "  0.6157143,\n",
       "  0.6135065,\n",
       "  0.6257143,\n",
       "  0.625974,\n",
       "  0.6242857,\n",
       "  0.6251948,\n",
       "  0.63207793,\n",
       "  0.63,\n",
       "  0.6380519,\n",
       "  0.6315584,\n",
       "  0.6363636,\n",
       "  0.64012986,\n",
       "  0.63935065,\n",
       "  0.64675325,\n",
       "  0.6431169,\n",
       "  0.64272726,\n",
       "  0.6544156,\n",
       "  0.654026,\n",
       "  0.6568831,\n",
       "  0.65831167,\n",
       "  0.6588312,\n",
       "  0.65753245,\n",
       "  0.6555844,\n",
       "  0.6653247,\n",
       "  0.6648052,\n",
       "  0.6661039,\n",
       "  0.6701299,\n",
       "  0.6687013,\n",
       "  0.6692208,\n",
       "  0.67311686,\n",
       "  0.6711688,\n",
       "  0.6753247,\n",
       "  0.6803896,\n",
       "  0.6768831,\n",
       "  0.681039,\n",
       "  0.6807792,\n",
       "  0.68545455,\n",
       "  0.6824675,\n",
       "  0.6807792,\n",
       "  0.6918182,\n",
       "  0.6824675,\n",
       "  0.6855844,\n",
       "  0.69441557,\n",
       "  0.69688314,\n",
       "  0.70116884,\n",
       "  0.6981818,\n",
       "  0.6963636,\n",
       "  0.69454545,\n",
       "  0.70194805,\n",
       "  0.7054545,\n",
       "  0.6974026,\n",
       "  0.69844157,\n",
       "  0.69974023,\n",
       "  0.7015585,\n",
       "  0.704026,\n",
       "  0.7054545,\n",
       "  0.71337664,\n",
       "  0.7066234,\n",
       "  0.70935065,\n",
       "  0.7094805,\n",
       "  0.7164935,\n",
       "  0.70337665,\n",
       "  0.71493506,\n",
       "  0.71844155,\n",
       "  0.7185714,\n",
       "  0.717013,\n",
       "  0.71493506,\n",
       "  0.721948,\n",
       "  0.7251948,\n",
       "  0.72662336,\n",
       "  0.71662337,\n",
       "  0.7212987,\n",
       "  0.71844155,\n",
       "  0.7248052,\n",
       "  0.7311688,\n",
       "  0.7238961,\n",
       "  0.7305195,\n",
       "  0.72740257,\n",
       "  0.7325974,\n",
       "  0.7287013,\n",
       "  0.73311687,\n",
       "  0.73584414,\n",
       "  0.7350649,\n",
       "  0.7338961,\n",
       "  0.7351948,\n",
       "  0.7435065,\n",
       "  0.7450649,\n",
       "  0.7396104,\n",
       "  0.74025977,\n",
       "  0.74298704,\n",
       "  0.7435065,\n",
       "  0.7451948,\n",
       "  0.75,\n",
       "  0.74974024,\n",
       "  0.74727273,\n",
       "  0.74454546,\n",
       "  0.74363637,\n",
       "  0.7492208,\n",
       "  0.74727273,\n",
       "  0.75116885,\n",
       "  0.75233763,\n",
       "  0.7572727,\n",
       "  0.75805193,\n",
       "  0.7564935,\n",
       "  0.7533766,\n",
       "  0.7635065,\n",
       "  0.75285715,\n",
       "  0.76012987,\n",
       "  0.7557143,\n",
       "  0.7616883,\n",
       "  0.7575325,\n",
       "  0.77337664,\n",
       "  0.76441556,\n",
       "  0.7633766,\n",
       "  0.7696104,\n",
       "  0.7705195,\n",
       "  0.7674026,\n",
       "  0.764026,\n",
       "  0.7638961,\n",
       "  0.76623374,\n",
       "  0.7703896,\n",
       "  0.767013,\n",
       "  0.77,\n",
       "  0.7703896,\n",
       "  0.7785714,\n",
       "  0.7751948,\n",
       "  0.77584416,\n",
       "  0.7764935,\n",
       "  0.78753245,\n",
       "  0.7854546,\n",
       "  0.78831166,\n",
       "  0.7851948,\n",
       "  0.7787013,\n",
       "  0.7811688,\n",
       "  0.7820779,\n",
       "  0.78246754,\n",
       "  0.78064936,\n",
       "  0.77922076,\n",
       "  0.785974,\n",
       "  0.7827273,\n",
       "  0.78727275,\n",
       "  0.79064935,\n",
       "  0.7887013,\n",
       "  0.7919481,\n",
       "  0.7827273,\n",
       "  0.7927273,\n",
       "  0.78844154,\n",
       "  0.78909093,\n",
       "  0.7918182,\n",
       "  0.7925974,\n",
       "  0.79324675,\n",
       "  0.79311687,\n",
       "  0.79818183,\n",
       "  0.79467535,\n",
       "  0.7950649,\n",
       "  0.79376626,\n",
       "  0.7976623,\n",
       "  0.8012987,\n",
       "  0.79974025,\n",
       "  0.8076623,\n",
       "  0.7968831,\n",
       "  0.8036364,\n",
       "  0.80051947,\n",
       "  0.8057143,\n",
       "  0.7994805,\n",
       "  0.8066234,\n",
       "  0.80233765,\n",
       "  0.8075325,\n",
       "  0.805974,\n",
       "  0.81545454,\n",
       "  0.8116883,\n",
       "  0.81363636,\n",
       "  0.8125974,\n",
       "  0.8090909,\n",
       "  0.8124675],\n",
       " 'val_binary_crossentropy': [0.69880533,\n",
       "  0.67296183,\n",
       "  0.6700219,\n",
       "  0.66806525,\n",
       "  0.66865236,\n",
       "  0.6673628,\n",
       "  0.6652505,\n",
       "  0.66519004,\n",
       "  0.66602373,\n",
       "  0.665181,\n",
       "  0.6647167,\n",
       "  0.6647766,\n",
       "  0.6632419,\n",
       "  0.66304547,\n",
       "  0.66399837,\n",
       "  0.6630503,\n",
       "  0.662751,\n",
       "  0.6621634,\n",
       "  0.66108483,\n",
       "  0.6624972,\n",
       "  0.6595791,\n",
       "  0.6595642,\n",
       "  0.65914255,\n",
       "  0.65935075,\n",
       "  0.65727246,\n",
       "  0.65763664,\n",
       "  0.65678376,\n",
       "  0.65488774,\n",
       "  0.65419227,\n",
       "  0.65380013,\n",
       "  0.6537353,\n",
       "  0.65189385,\n",
       "  0.6514623,\n",
       "  0.6494093,\n",
       "  0.6508491,\n",
       "  0.6499072,\n",
       "  0.6487366,\n",
       "  0.6480447,\n",
       "  0.6456331,\n",
       "  0.6472331,\n",
       "  0.6460091,\n",
       "  0.64381605,\n",
       "  0.64347595,\n",
       "  0.6411932,\n",
       "  0.64082474,\n",
       "  0.6418155,\n",
       "  0.6409296,\n",
       "  0.6416479,\n",
       "  0.63921714,\n",
       "  0.6377735,\n",
       "  0.63620996,\n",
       "  0.6368893,\n",
       "  0.63685304,\n",
       "  0.63681674,\n",
       "  0.6362683,\n",
       "  0.6354832,\n",
       "  0.63312703,\n",
       "  0.63136876,\n",
       "  0.6322188,\n",
       "  0.6331165,\n",
       "  0.63001,\n",
       "  0.62954646,\n",
       "  0.6314102,\n",
       "  0.6304781,\n",
       "  0.6277702,\n",
       "  0.6291247,\n",
       "  0.6312463,\n",
       "  0.6258282,\n",
       "  0.6266344,\n",
       "  0.6239068,\n",
       "  0.62437844,\n",
       "  0.625592,\n",
       "  0.62657094,\n",
       "  0.62231666,\n",
       "  0.6222128,\n",
       "  0.6258987,\n",
       "  0.62374467,\n",
       "  0.62227875,\n",
       "  0.62293696,\n",
       "  0.62070537,\n",
       "  0.6214519,\n",
       "  0.6192021,\n",
       "  0.61969155,\n",
       "  0.6206355,\n",
       "  0.6196617,\n",
       "  0.6170531,\n",
       "  0.6202169,\n",
       "  0.6163,\n",
       "  0.6157372,\n",
       "  0.6163815,\n",
       "  0.616424,\n",
       "  0.6166801,\n",
       "  0.6123387,\n",
       "  0.6128572,\n",
       "  0.6121711,\n",
       "  0.61475754,\n",
       "  0.6142781,\n",
       "  0.6151745,\n",
       "  0.61364305,\n",
       "  0.61192924,\n",
       "  0.6134468,\n",
       "  0.60978526,\n",
       "  0.6103705,\n",
       "  0.6099324,\n",
       "  0.61142987,\n",
       "  0.61018765,\n",
       "  0.60855335,\n",
       "  0.60899544,\n",
       "  0.60938394,\n",
       "  0.6083594,\n",
       "  0.6061702,\n",
       "  0.6040959,\n",
       "  0.6066693,\n",
       "  0.60554796,\n",
       "  0.604502,\n",
       "  0.60496116,\n",
       "  0.6039255,\n",
       "  0.60274,\n",
       "  0.60203075,\n",
       "  0.6026382,\n",
       "  0.60441434,\n",
       "  0.6039539,\n",
       "  0.60256064,\n",
       "  0.6028143,\n",
       "  0.6010563,\n",
       "  0.5996023,\n",
       "  0.5989151,\n",
       "  0.5984724,\n",
       "  0.59889436,\n",
       "  0.60090244,\n",
       "  0.59703916,\n",
       "  0.59913373,\n",
       "  0.59781575,\n",
       "  0.59865683,\n",
       "  0.5973609,\n",
       "  0.59799856,\n",
       "  0.5913118,\n",
       "  0.5948664,\n",
       "  0.59533876,\n",
       "  0.5933213,\n",
       "  0.5936818,\n",
       "  0.5950448,\n",
       "  0.59530884,\n",
       "  0.5953073,\n",
       "  0.5941417,\n",
       "  0.5932894,\n",
       "  0.5940888,\n",
       "  0.59372574,\n",
       "  0.59299946,\n",
       "  0.58893484,\n",
       "  0.59182334,\n",
       "  0.5893738,\n",
       "  0.59031284,\n",
       "  0.5862944,\n",
       "  0.58659625,\n",
       "  0.58643407,\n",
       "  0.5868937,\n",
       "  0.589277,\n",
       "  0.58848697,\n",
       "  0.5879049,\n",
       "  0.5866507,\n",
       "  0.58781725,\n",
       "  0.5888777,\n",
       "  0.5864525,\n",
       "  0.5879813,\n",
       "  0.58409685,\n",
       "  0.5844368,\n",
       "  0.5847983,\n",
       "  0.5832486,\n",
       "  0.5864059,\n",
       "  0.5834395,\n",
       "  0.5844449,\n",
       "  0.5836102,\n",
       "  0.5834859,\n",
       "  0.58336616,\n",
       "  0.58389634,\n",
       "  0.58394504,\n",
       "  0.58070964,\n",
       "  0.5832662,\n",
       "  0.5799743,\n",
       "  0.58173597,\n",
       "  0.58067423,\n",
       "  0.5786668,\n",
       "  0.5801471,\n",
       "  0.57630146,\n",
       "  0.5811587,\n",
       "  0.5790509,\n",
       "  0.5788195,\n",
       "  0.5773052,\n",
       "  0.5793789,\n",
       "  0.57684,\n",
       "  0.57826626,\n",
       "  0.5763064,\n",
       "  0.57762206,\n",
       "  0.5745313,\n",
       "  0.57465124,\n",
       "  0.5736977,\n",
       "  0.57462,\n",
       "  0.5766049,\n",
       "  0.5740464],\n",
       " 'auc_16': [0.56580305,\n",
       "  0.6341976,\n",
       "  0.6400992,\n",
       "  0.6442998,\n",
       "  0.6456232,\n",
       "  0.6487665,\n",
       "  0.655057,\n",
       "  0.6574763,\n",
       "  0.6518081,\n",
       "  0.65325165,\n",
       "  0.65550566,\n",
       "  0.6578061,\n",
       "  0.6575749,\n",
       "  0.66170084,\n",
       "  0.6615515,\n",
       "  0.65920997,\n",
       "  0.66258407,\n",
       "  0.66238695,\n",
       "  0.6674056,\n",
       "  0.66432416,\n",
       "  0.66736835,\n",
       "  0.672192,\n",
       "  0.67137754,\n",
       "  0.67190266,\n",
       "  0.67323375,\n",
       "  0.672303,\n",
       "  0.67463434,\n",
       "  0.67792714,\n",
       "  0.68167186,\n",
       "  0.6834066,\n",
       "  0.6837086,\n",
       "  0.6843036,\n",
       "  0.6899898,\n",
       "  0.6955014,\n",
       "  0.68994766,\n",
       "  0.69476116,\n",
       "  0.69965297,\n",
       "  0.7004802,\n",
       "  0.7041916,\n",
       "  0.70250595,\n",
       "  0.70228845,\n",
       "  0.71361196,\n",
       "  0.7131035,\n",
       "  0.71787035,\n",
       "  0.71500117,\n",
       "  0.713558,\n",
       "  0.7168595,\n",
       "  0.72021943,\n",
       "  0.72473633,\n",
       "  0.72409534,\n",
       "  0.7276473,\n",
       "  0.7265421,\n",
       "  0.7276373,\n",
       "  0.7266615,\n",
       "  0.7331502,\n",
       "  0.73444986,\n",
       "  0.7363035,\n",
       "  0.7384488,\n",
       "  0.7387282,\n",
       "  0.7316975,\n",
       "  0.7438987,\n",
       "  0.7415044,\n",
       "  0.73619527,\n",
       "  0.7399211,\n",
       "  0.74501836,\n",
       "  0.7446103,\n",
       "  0.7420362,\n",
       "  0.7499712,\n",
       "  0.7467635,\n",
       "  0.7543015,\n",
       "  0.75052637,\n",
       "  0.7555174,\n",
       "  0.7464169,\n",
       "  0.7543374,\n",
       "  0.75580996,\n",
       "  0.74650675,\n",
       "  0.75191075,\n",
       "  0.75170076,\n",
       "  0.7545776,\n",
       "  0.75908196,\n",
       "  0.75803244,\n",
       "  0.76661664,\n",
       "  0.7625127,\n",
       "  0.7593957,\n",
       "  0.76173544,\n",
       "  0.7659567,\n",
       "  0.7589348,\n",
       "  0.761773,\n",
       "  0.7653155,\n",
       "  0.76529205,\n",
       "  0.7672092,\n",
       "  0.768138,\n",
       "  0.77438354,\n",
       "  0.7728069,\n",
       "  0.77520883,\n",
       "  0.7697479,\n",
       "  0.7731782,\n",
       "  0.76694477,\n",
       "  0.77313,\n",
       "  0.7757593,\n",
       "  0.77070147,\n",
       "  0.7766173,\n",
       "  0.77661747,\n",
       "  0.7772178,\n",
       "  0.773035,\n",
       "  0.7767784,\n",
       "  0.77786237,\n",
       "  0.77963984,\n",
       "  0.77838516,\n",
       "  0.781292,\n",
       "  0.78657746,\n",
       "  0.78770256,\n",
       "  0.78538495,\n",
       "  0.786777,\n",
       "  0.7863218,\n",
       "  0.783611,\n",
       "  0.78935784,\n",
       "  0.7904932,\n",
       "  0.78962725,\n",
       "  0.79203504,\n",
       "  0.7918291,\n",
       "  0.7884443,\n",
       "  0.7913514,\n",
       "  0.7880063,\n",
       "  0.79426795,\n",
       "  0.7973676,\n",
       "  0.7991711,\n",
       "  0.79916495,\n",
       "  0.803311,\n",
       "  0.798121,\n",
       "  0.80123264,\n",
       "  0.79690176,\n",
       "  0.8015701,\n",
       "  0.8025494,\n",
       "  0.8032985,\n",
       "  0.801738,\n",
       "  0.8128511,\n",
       "  0.80669945,\n",
       "  0.80605966,\n",
       "  0.81109077,\n",
       "  0.80536526,\n",
       "  0.80866796,\n",
       "  0.8045977,\n",
       "  0.8046541,\n",
       "  0.80996215,\n",
       "  0.81234205,\n",
       "  0.8085871,\n",
       "  0.8100201,\n",
       "  0.8117208,\n",
       "  0.8194952,\n",
       "  0.81055087,\n",
       "  0.81811446,\n",
       "  0.8161242,\n",
       "  0.82536477,\n",
       "  0.8242488,\n",
       "  0.82440305,\n",
       "  0.8221356,\n",
       "  0.81846595,\n",
       "  0.81786615,\n",
       "  0.82040834,\n",
       "  0.82150954,\n",
       "  0.82292956,\n",
       "  0.8207994,\n",
       "  0.8270735,\n",
       "  0.8215437,\n",
       "  0.8260604,\n",
       "  0.8266279,\n",
       "  0.8275895,\n",
       "  0.82970846,\n",
       "  0.8245635,\n",
       "  0.82897013,\n",
       "  0.8342559,\n",
       "  0.8322458,\n",
       "  0.8295944,\n",
       "  0.8327094,\n",
       "  0.831212,\n",
       "  0.83301604,\n",
       "  0.836978,\n",
       "  0.83584315,\n",
       "  0.8376908,\n",
       "  0.83318627,\n",
       "  0.8392958,\n",
       "  0.8421164,\n",
       "  0.8382658,\n",
       "  0.84301335,\n",
       "  0.8374217,\n",
       "  0.84103185,\n",
       "  0.8390636,\n",
       "  0.83989495,\n",
       "  0.8398713,\n",
       "  0.8443827,\n",
       "  0.84470713,\n",
       "  0.84289724,\n",
       "  0.845053,\n",
       "  0.8495724,\n",
       "  0.8495687,\n",
       "  0.85002697,\n",
       "  0.8538221,\n",
       "  0.84683573,\n",
       "  0.85317135],\n",
       " 'val_loss': [0.6822686484365752,\n",
       "  0.6777559193697843,\n",
       "  0.6771165024150502,\n",
       "  0.6773928891528737,\n",
       "  0.6749456615159006,\n",
       "  0.6759425961610043,\n",
       "  0.6743539824630275,\n",
       "  0.6719244176691229,\n",
       "  0.6731202566262448,\n",
       "  0.6711205045382181,\n",
       "  0.6734534267223242,\n",
       "  0.6747076150142786,\n",
       "  0.6743636944077231,\n",
       "  0.6763859300902395,\n",
       "  0.6726283069812891,\n",
       "  0.6722446698130984,\n",
       "  0.6694751670866301,\n",
       "  0.6698136094844702,\n",
       "  0.6716587001627142,\n",
       "  0.670642760666934,\n",
       "  0.670511928471652,\n",
       "  0.6726802695881237,\n",
       "  0.6752462947007382,\n",
       "  0.670735984137564,\n",
       "  0.6741228663560116,\n",
       "  0.6758159055854335,\n",
       "  0.6754537260893619,\n",
       "  0.6792328466068615,\n",
       "  0.6747852112307693,\n",
       "  0.6763849619663123,\n",
       "  0.6783850193023682,\n",
       "  0.6855434919848586,\n",
       "  0.6806192036831018,\n",
       "  0.6803264184431597,\n",
       "  0.6811815355763291,\n",
       "  0.6835198041164514,\n",
       "  0.6872046842719569,\n",
       "  0.6865886883302168,\n",
       "  0.6841913497809208,\n",
       "  0.6783998626651186,\n",
       "  0.6873678167661031,\n",
       "  0.6888403404842723,\n",
       "  0.6848082054745067,\n",
       "  0.6880718523805792,\n",
       "  0.6827385985490048,\n",
       "  0.6870479222499963,\n",
       "  0.6857619990002025,\n",
       "  0.6857609676592278,\n",
       "  0.6858433755961332,\n",
       "  0.6832528818737377,\n",
       "  0.6792410919160554,\n",
       "  0.682320874748808,\n",
       "  0.68548440210747,\n",
       "  0.6821576465259899,\n",
       "  0.6860866962057172,\n",
       "  0.6850286270632888,\n",
       "  0.689082884427273,\n",
       "  0.6779064788962855,\n",
       "  0.6799406878875963,\n",
       "  0.6838049599618623,\n",
       "  0.6871849099795023,\n",
       "  0.6900026400883993,\n",
       "  0.6875235944083242,\n",
       "  0.6882187669927423,\n",
       "  0.6840663082671888,\n",
       "  0.6808267849864382,\n",
       "  0.6807414058483008,\n",
       "  0.6858383904803883,\n",
       "  0.6814001094211232,\n",
       "  0.6855146993290294,\n",
       "  0.6829372644424438,\n",
       "  0.6845209020556826,\n",
       "  0.6886923836939263,\n",
       "  0.686493127635031,\n",
       "  0.684930353453665,\n",
       "  0.6887049385995576,\n",
       "  0.6838749791636611,\n",
       "  0.6882093548774719,\n",
       "  0.6877337477423928,\n",
       "  0.6794907017187639,\n",
       "  0.690587908932657,\n",
       "  0.6856534914536909,\n",
       "  0.6843189777749957,\n",
       "  0.682035354050723,\n",
       "  0.6879077373128949,\n",
       "  0.6871445937590166,\n",
       "  0.6854836362780947,\n",
       "  0.6877518512985923,\n",
       "  0.6914585070176558,\n",
       "  0.6909727454185486,\n",
       "  0.688803159829342,\n",
       "  0.689208903095939,\n",
       "  0.6915524565812313,\n",
       "  0.6861980756123861,\n",
       "  0.6917939944700762,\n",
       "  0.6922688195199678,\n",
       "  0.6855452692869938,\n",
       "  0.6887366825884039,\n",
       "  0.6873773083542333,\n",
       "  0.6872211712779421,\n",
       "  0.6892280506365227,\n",
       "  0.6843783837376218,\n",
       "  0.6892449476502158,\n",
       "  0.688124084111416,\n",
       "  0.6905214642033433,\n",
       "  0.6907665332158407,\n",
       "  0.6905571728041677,\n",
       "  0.6892259933731772,\n",
       "  0.6907896399497986,\n",
       "  0.6900351733872385,\n",
       "  0.6907518560236151,\n",
       "  0.6903844818924413,\n",
       "  0.6901735103491581,\n",
       "  0.6895508675864248,\n",
       "  0.6896321087172537,\n",
       "  0.6900876471490571,\n",
       "  0.6901422576470808,\n",
       "  0.6880795991781986,\n",
       "  0.6884231693816908,\n",
       "  0.6893953771302195,\n",
       "  0.689104726820281,\n",
       "  0.6897908521421028,\n",
       "  0.6896103891459379,\n",
       "  0.686104877428575,\n",
       "  0.6870754054098418,\n",
       "  0.6899886438340852,\n",
       "  0.6891461123119701,\n",
       "  0.6878229542212053,\n",
       "  0.6882747249169783,\n",
       "  0.6861157182491187,\n",
       "  0.6910547614097595,\n",
       "  0.6881973291888381,\n",
       "  0.6931621594862505,\n",
       "  0.6857831478118896,\n",
       "  0.6896250753691702,\n",
       "  0.6884001276709817,\n",
       "  0.690004081437082,\n",
       "  0.6895928563493671,\n",
       "  0.6914768200932127,\n",
       "  0.6909842707894065,\n",
       "  0.6870511409008142,\n",
       "  0.6889070453065814,\n",
       "  0.6921481670755328,\n",
       "  0.6898156982479673,\n",
       "  0.6906696594122684,\n",
       "  0.6897359129154321,\n",
       "  0.6877876899459145,\n",
       "  0.6881021333463264,\n",
       "  0.6908071745525707,\n",
       "  0.6895583217794244,\n",
       "  0.6924436616175103,\n",
       "  0.6897691470203977,\n",
       "  0.6915400804895343,\n",
       "  0.6863861950961027,\n",
       "  0.6869340882156835,\n",
       "  0.6900717688329292,\n",
       "  0.6914917212544065,\n",
       "  0.6900866302576932,\n",
       "  0.6888243808890834,\n",
       "  0.6908667250113054,\n",
       "  0.6881199461041074,\n",
       "  0.685506081942356,\n",
       "  0.6888240846720609,\n",
       "  0.6930174394087358,\n",
       "  0.6887641982598738,\n",
       "  0.6910624016414989,\n",
       "  0.6931601925329729,\n",
       "  0.6880654548153733,\n",
       "  0.6938924699118643,\n",
       "  0.6930837089365179,\n",
       "  0.688720601977724,\n",
       "  0.6933570594498606,\n",
       "  0.6926054719722632,\n",
       "  0.6896100477738814,\n",
       "  0.6889064438415297,\n",
       "  0.6910923466537938,\n",
       "  0.6938301360968387,\n",
       "  0.6856541507171862,\n",
       "  0.6891601808143385,\n",
       "  0.6859776684732148,\n",
       "  0.6853125510793744,\n",
       "  0.6895431063391946,\n",
       "  0.6923521016583298,\n",
       "  0.6912492062106277,\n",
       "  0.6888655929854421,\n",
       "  0.6893515370108865,\n",
       "  0.6888457374139265,\n",
       "  0.6879030538327766,\n",
       "  0.6900025515845327,\n",
       "  0.693961116400632,\n",
       "  0.6886294303518353,\n",
       "  0.6907907489574316,\n",
       "  0.690092039830757,\n",
       "  0.6919093402949247,\n",
       "  0.68793774915464,\n",
       "  0.6922883517814405,\n",
       "  0.6933497082103383,\n",
       "  0.6923411813649264,\n",
       "  0.6912909908728166,\n",
       "  0.6908951997756958],\n",
       " 'val_accuracy': [0.47030303,\n",
       "  0.48909092,\n",
       "  0.49151516,\n",
       "  0.49454546,\n",
       "  0.49878788,\n",
       "  0.5030303,\n",
       "  0.50848484,\n",
       "  0.5181818,\n",
       "  0.5109091,\n",
       "  0.5193939,\n",
       "  0.5042424,\n",
       "  0.5078788,\n",
       "  0.510303,\n",
       "  0.49757576,\n",
       "  0.5151515,\n",
       "  0.510303,\n",
       "  0.53818184,\n",
       "  0.5254545,\n",
       "  0.51878786,\n",
       "  0.5254545,\n",
       "  0.52,\n",
       "  0.5048485,\n",
       "  0.49939394,\n",
       "  0.5212121,\n",
       "  0.49878788,\n",
       "  0.49272728,\n",
       "  0.48727274,\n",
       "  0.4812121,\n",
       "  0.49575758,\n",
       "  0.48969698,\n",
       "  0.48666668,\n",
       "  0.4648485,\n",
       "  0.4769697,\n",
       "  0.48242423,\n",
       "  0.48181817,\n",
       "  0.47636363,\n",
       "  0.46969697,\n",
       "  0.47030303,\n",
       "  0.47454545,\n",
       "  0.49939394,\n",
       "  0.4709091,\n",
       "  0.4660606,\n",
       "  0.48,\n",
       "  0.4690909,\n",
       "  0.4969697,\n",
       "  0.4769697,\n",
       "  0.48545456,\n",
       "  0.48727274,\n",
       "  0.4878788,\n",
       "  0.49151516,\n",
       "  0.5169697,\n",
       "  0.4878788,\n",
       "  0.48606062,\n",
       "  0.5,\n",
       "  0.4781818,\n",
       "  0.4842424,\n",
       "  0.46666667,\n",
       "  0.51757574,\n",
       "  0.5406061,\n",
       "  0.4842424,\n",
       "  0.4781818,\n",
       "  0.47030303,\n",
       "  0.47454545,\n",
       "  0.48060605,\n",
       "  0.50060606,\n",
       "  0.4969697,\n",
       "  0.49939394,\n",
       "  0.48,\n",
       "  0.49272728,\n",
       "  0.48181817,\n",
       "  0.49151516,\n",
       "  0.4848485,\n",
       "  0.47030303,\n",
       "  0.47757575,\n",
       "  0.48242423,\n",
       "  0.47151515,\n",
       "  0.510303,\n",
       "  0.48969698,\n",
       "  0.47757575,\n",
       "  0.5163636,\n",
       "  0.47030303,\n",
       "  0.48969698,\n",
       "  0.49454546,\n",
       "  0.5018182,\n",
       "  0.4769697,\n",
       "  0.48,\n",
       "  0.48242423,\n",
       "  0.47939393,\n",
       "  0.46424243,\n",
       "  0.47030303,\n",
       "  0.4769697,\n",
       "  0.47272727,\n",
       "  0.46424243,\n",
       "  0.48666668,\n",
       "  0.4618182,\n",
       "  0.46363637,\n",
       "  0.48909092,\n",
       "  0.47636363,\n",
       "  0.47878787,\n",
       "  0.47636363,\n",
       "  0.47757575,\n",
       "  0.5012121,\n",
       "  0.47575757,\n",
       "  0.4830303,\n",
       "  0.47272727,\n",
       "  0.47030303,\n",
       "  0.4690909,\n",
       "  0.47333333,\n",
       "  0.4721212,\n",
       "  0.47333333,\n",
       "  0.47272727,\n",
       "  0.4709091,\n",
       "  0.47030303,\n",
       "  0.46969697,\n",
       "  0.4751515,\n",
       "  0.47575757,\n",
       "  0.4769697,\n",
       "  0.4769697,\n",
       "  0.4830303,\n",
       "  0.4751515,\n",
       "  0.4709091,\n",
       "  0.4709091,\n",
       "  0.4830303,\n",
       "  0.4878788,\n",
       "  0.48727274,\n",
       "  0.47575757,\n",
       "  0.47757575,\n",
       "  0.4842424,\n",
       "  0.47636363,\n",
       "  0.4878788,\n",
       "  0.4721212,\n",
       "  0.48727274,\n",
       "  0.4648485,\n",
       "  0.49151516,\n",
       "  0.4751515,\n",
       "  0.47939393,\n",
       "  0.47636363,\n",
       "  0.47636363,\n",
       "  0.46424243,\n",
       "  0.47757575,\n",
       "  0.4848485,\n",
       "  0.47333333,\n",
       "  0.4660606,\n",
       "  0.4751515,\n",
       "  0.4848485,\n",
       "  0.47333333,\n",
       "  0.4830303,\n",
       "  0.4878788,\n",
       "  0.4709091,\n",
       "  0.48545456,\n",
       "  0.4751515,\n",
       "  0.49030304,\n",
       "  0.47757575,\n",
       "  0.5030303,\n",
       "  0.49454546,\n",
       "  0.47272727,\n",
       "  0.4678788,\n",
       "  0.47878787,\n",
       "  0.48181817,\n",
       "  0.48060605,\n",
       "  0.49575758,\n",
       "  0.5072727,\n",
       "  0.48969698,\n",
       "  0.4751515,\n",
       "  0.49515152,\n",
       "  0.4878788,\n",
       "  0.4751515,\n",
       "  0.49030304,\n",
       "  0.46242425,\n",
       "  0.4769697,\n",
       "  0.4939394,\n",
       "  0.46424243,\n",
       "  0.48181817,\n",
       "  0.49515152,\n",
       "  0.50969696,\n",
       "  0.4848485,\n",
       "  0.46242425,\n",
       "  0.4969697,\n",
       "  0.47939393,\n",
       "  0.48666668,\n",
       "  0.49818182,\n",
       "  0.4812121,\n",
       "  0.47454545,\n",
       "  0.47757575,\n",
       "  0.49212122,\n",
       "  0.48606062,\n",
       "  0.48666668,\n",
       "  0.48848486,\n",
       "  0.47878787,\n",
       "  0.47030303,\n",
       "  0.4939394,\n",
       "  0.48181817,\n",
       "  0.4842424,\n",
       "  0.4830303,\n",
       "  0.48909092,\n",
       "  0.47636363,\n",
       "  0.46848485,\n",
       "  0.47757575,\n",
       "  0.4842424,\n",
       "  0.4878788],\n",
       " 'val_val_binary_crossentropy': [0.68179995,\n",
       "  0.67788005,\n",
       "  0.6771299,\n",
       "  0.6771249,\n",
       "  0.6750782,\n",
       "  0.6759355,\n",
       "  0.67440826,\n",
       "  0.67205346,\n",
       "  0.6732382,\n",
       "  0.6712633,\n",
       "  0.6735506,\n",
       "  0.67466676,\n",
       "  0.67439127,\n",
       "  0.6764991,\n",
       "  0.67258734,\n",
       "  0.67221403,\n",
       "  0.66941094,\n",
       "  0.6698505,\n",
       "  0.6714466,\n",
       "  0.67022204,\n",
       "  0.6705149,\n",
       "  0.67244816,\n",
       "  0.6750266,\n",
       "  0.67054474,\n",
       "  0.67396224,\n",
       "  0.6755673,\n",
       "  0.6752533,\n",
       "  0.67917395,\n",
       "  0.6744348,\n",
       "  0.6759151,\n",
       "  0.67814684,\n",
       "  0.6855172,\n",
       "  0.6805233,\n",
       "  0.6801197,\n",
       "  0.6810724,\n",
       "  0.68341744,\n",
       "  0.6871686,\n",
       "  0.68658084,\n",
       "  0.6839783,\n",
       "  0.67810357,\n",
       "  0.68738425,\n",
       "  0.6888865,\n",
       "  0.68484384,\n",
       "  0.6881054,\n",
       "  0.6828078,\n",
       "  0.6869231,\n",
       "  0.6858617,\n",
       "  0.68595994,\n",
       "  0.6860045,\n",
       "  0.68328875,\n",
       "  0.6789007,\n",
       "  0.68236303,\n",
       "  0.68549234,\n",
       "  0.68209964,\n",
       "  0.68615377,\n",
       "  0.68512565,\n",
       "  0.6890949,\n",
       "  0.677823,\n",
       "  0.679856,\n",
       "  0.6838973,\n",
       "  0.6872422,\n",
       "  0.6902308,\n",
       "  0.6876688,\n",
       "  0.6882235,\n",
       "  0.6840506,\n",
       "  0.68096274,\n",
       "  0.6807618,\n",
       "  0.68581945,\n",
       "  0.6815322,\n",
       "  0.68543816,\n",
       "  0.68313557,\n",
       "  0.68448687,\n",
       "  0.6886037,\n",
       "  0.6866481,\n",
       "  0.6848792,\n",
       "  0.6885007,\n",
       "  0.68340445,\n",
       "  0.68805933,\n",
       "  0.68767256,\n",
       "  0.67892104,\n",
       "  0.69044954,\n",
       "  0.68553746,\n",
       "  0.68430424,\n",
       "  0.6821974,\n",
       "  0.6878004,\n",
       "  0.6870293,\n",
       "  0.6852165,\n",
       "  0.6876002,\n",
       "  0.69131535,\n",
       "  0.6908999,\n",
       "  0.68868816,\n",
       "  0.68893003,\n",
       "  0.6913978,\n",
       "  0.68640393,\n",
       "  0.6916814,\n",
       "  0.6921577,\n",
       "  0.68591505,\n",
       "  0.68908983,\n",
       "  0.6876146,\n",
       "  0.68761015,\n",
       "  0.68928957,\n",
       "  0.6841255,\n",
       "  0.689447,\n",
       "  0.6880672,\n",
       "  0.6905419,\n",
       "  0.69068515,\n",
       "  0.6907118,\n",
       "  0.6893185,\n",
       "  0.69052875,\n",
       "  0.6901626,\n",
       "  0.690874,\n",
       "  0.69049436,\n",
       "  0.6901691,\n",
       "  0.68949425,\n",
       "  0.6897708,\n",
       "  0.6900059,\n",
       "  0.69043726,\n",
       "  0.68815964,\n",
       "  0.6884731,\n",
       "  0.6894664,\n",
       "  0.689053,\n",
       "  0.6896705,\n",
       "  0.689665,\n",
       "  0.68641955,\n",
       "  0.68730956,\n",
       "  0.6897061,\n",
       "  0.68883616,\n",
       "  0.6879009,\n",
       "  0.68827975,\n",
       "  0.68584764,\n",
       "  0.6909169,\n",
       "  0.68809336,\n",
       "  0.69277906,\n",
       "  0.6855314,\n",
       "  0.6893407,\n",
       "  0.68843794,\n",
       "  0.6898632,\n",
       "  0.68918324,\n",
       "  0.69137526,\n",
       "  0.6909862,\n",
       "  0.686942,\n",
       "  0.6888384,\n",
       "  0.6917586,\n",
       "  0.6894887,\n",
       "  0.69001865,\n",
       "  0.68942696,\n",
       "  0.6872119,\n",
       "  0.6876572,\n",
       "  0.6906663,\n",
       "  0.68938196,\n",
       "  0.69211113,\n",
       "  0.6897861,\n",
       "  0.6910219,\n",
       "  0.68620574,\n",
       "  0.6864099,\n",
       "  0.689892,\n",
       "  0.6914504,\n",
       "  0.6898837,\n",
       "  0.6883533,\n",
       "  0.690546,\n",
       "  0.6878146,\n",
       "  0.68571186,\n",
       "  0.6888513,\n",
       "  0.6920806,\n",
       "  0.68818504,\n",
       "  0.69077504,\n",
       "  0.6924137,\n",
       "  0.6875521,\n",
       "  0.6938925,\n",
       "  0.69284624,\n",
       "  0.6883224,\n",
       "  0.69294864,\n",
       "  0.6923725,\n",
       "  0.68934673,\n",
       "  0.6885289,\n",
       "  0.69106585,\n",
       "  0.69362664,\n",
       "  0.6854137,\n",
       "  0.6887317,\n",
       "  0.6857218,\n",
       "  0.6848133,\n",
       "  0.68971574,\n",
       "  0.69210595,\n",
       "  0.69080865,\n",
       "  0.6885572,\n",
       "  0.6888601,\n",
       "  0.6882659,\n",
       "  0.6881882,\n",
       "  0.6900092,\n",
       "  0.6939395,\n",
       "  0.6884041,\n",
       "  0.69076943,\n",
       "  0.6898572,\n",
       "  0.6915302,\n",
       "  0.6880132,\n",
       "  0.69228387,\n",
       "  0.69368726,\n",
       "  0.6919909,\n",
       "  0.69089705,\n",
       "  0.69057],\n",
       " 'val_auc_16': [0.6109636,\n",
       "  0.62038887,\n",
       "  0.6232535,\n",
       "  0.6292639,\n",
       "  0.63522536,\n",
       "  0.62452275,\n",
       "  0.63377416,\n",
       "  0.63050205,\n",
       "  0.63658786,\n",
       "  0.6338489,\n",
       "  0.6357135,\n",
       "  0.62497985,\n",
       "  0.63510996,\n",
       "  0.6325671,\n",
       "  0.6368201,\n",
       "  0.64282,\n",
       "  0.63812697,\n",
       "  0.6396928,\n",
       "  0.6348955,\n",
       "  0.63991106,\n",
       "  0.64092726,\n",
       "  0.63668764,\n",
       "  0.6377054,\n",
       "  0.6368016,\n",
       "  0.635903,\n",
       "  0.630445,\n",
       "  0.63712996,\n",
       "  0.63170695,\n",
       "  0.62539405,\n",
       "  0.64348793,\n",
       "  0.6268726,\n",
       "  0.6181004,\n",
       "  0.619022,\n",
       "  0.6122136,\n",
       "  0.5961929,\n",
       "  0.5853298,\n",
       "  0.5792699,\n",
       "  0.5924303,\n",
       "  0.59690374,\n",
       "  0.6071973,\n",
       "  0.5767853,\n",
       "  0.54418933,\n",
       "  0.58279496,\n",
       "  0.5611339,\n",
       "  0.58195114,\n",
       "  0.54689556,\n",
       "  0.56644005,\n",
       "  0.5792041,\n",
       "  0.587079,\n",
       "  0.58970106,\n",
       "  0.6065768,\n",
       "  0.60405225,\n",
       "  0.59270024,\n",
       "  0.6081951,\n",
       "  0.56588167,\n",
       "  0.58297247,\n",
       "  0.5562152,\n",
       "  0.6141181,\n",
       "  0.60291773,\n",
       "  0.5846389,\n",
       "  0.5575687,\n",
       "  0.55163664,\n",
       "  0.5704104,\n",
       "  0.53984165,\n",
       "  0.5884769,\n",
       "  0.60665447,\n",
       "  0.586812,\n",
       "  0.5665347,\n",
       "  0.5949423,\n",
       "  0.5794823,\n",
       "  0.58320254,\n",
       "  0.5793563,\n",
       "  0.55248964,\n",
       "  0.55635494,\n",
       "  0.56492156,\n",
       "  0.5376124,\n",
       "  0.5951802,\n",
       "  0.54360193,\n",
       "  0.54237413,\n",
       "  0.5926087,\n",
       "  0.5337943,\n",
       "  0.55838156,\n",
       "  0.56321526,\n",
       "  0.57962126,\n",
       "  0.5502322,\n",
       "  0.551199,\n",
       "  0.54599774,\n",
       "  0.55457085,\n",
       "  0.51369077,\n",
       "  0.5265421,\n",
       "  0.54322255,\n",
       "  0.5411886,\n",
       "  0.5170821,\n",
       "  0.5468461,\n",
       "  0.51656884,\n",
       "  0.5161421,\n",
       "  0.555622,\n",
       "  0.5387684,\n",
       "  0.5389112,\n",
       "  0.5412758,\n",
       "  0.53742826,\n",
       "  0.56593704,\n",
       "  0.5283565,\n",
       "  0.54540974,\n",
       "  0.522491,\n",
       "  0.5258638,\n",
       "  0.524159,\n",
       "  0.53359234,\n",
       "  0.5381383,\n",
       "  0.5295251,\n",
       "  0.5214098,\n",
       "  0.5205281,\n",
       "  0.5238978,\n",
       "  0.5299282,\n",
       "  0.5345245,\n",
       "  0.5348675,\n",
       "  0.5344954,\n",
       "  0.53574187,\n",
       "  0.5462419,\n",
       "  0.5314024,\n",
       "  0.5285606,\n",
       "  0.5193189,\n",
       "  0.5261257,\n",
       "  0.556218,\n",
       "  0.5467197,\n",
       "  0.53339416,\n",
       "  0.53203773,\n",
       "  0.54434085,\n",
       "  0.5320651,\n",
       "  0.5480997,\n",
       "  0.5239482,\n",
       "  0.5526886,\n",
       "  0.51082987,\n",
       "  0.5523682,\n",
       "  0.53826034,\n",
       "  0.5310066,\n",
       "  0.52669156,\n",
       "  0.5255266,\n",
       "  0.5147966,\n",
       "  0.53451025,\n",
       "  0.5425701,\n",
       "  0.5205643,\n",
       "  0.51102066,\n",
       "  0.5239793,\n",
       "  0.53878915,\n",
       "  0.527,\n",
       "  0.53913164,\n",
       "  0.5285015,\n",
       "  0.52539116,\n",
       "  0.5469889,\n",
       "  0.53095704,\n",
       "  0.5388158,\n",
       "  0.52005553,\n",
       "  0.5559548,\n",
       "  0.54867375,\n",
       "  0.5292315,\n",
       "  0.50783867,\n",
       "  0.521051,\n",
       "  0.5321589,\n",
       "  0.525838,\n",
       "  0.5522618,\n",
       "  0.56052876,\n",
       "  0.53666496,\n",
       "  0.5256738,\n",
       "  0.5475289,\n",
       "  0.5415036,\n",
       "  0.5357248,\n",
       "  0.54975295,\n",
       "  0.50625145,\n",
       "  0.5321701,\n",
       "  0.550761,\n",
       "  0.5111176,\n",
       "  0.5284407,\n",
       "  0.5399076,\n",
       "  0.5540732,\n",
       "  0.5402728,\n",
       "  0.50550514,\n",
       "  0.5510384,\n",
       "  0.5288772,\n",
       "  0.54060274,\n",
       "  0.5475946,\n",
       "  0.539929,\n",
       "  0.5239489,\n",
       "  0.5235452,\n",
       "  0.5363751,\n",
       "  0.531304,\n",
       "  0.53212935,\n",
       "  0.5382151,\n",
       "  0.52971655,\n",
       "  0.52321744,\n",
       "  0.5490694,\n",
       "  0.5336546,\n",
       "  0.53171885,\n",
       "  0.52863526,\n",
       "  0.539534,\n",
       "  0.521258,\n",
       "  0.5047086,\n",
       "  0.52471524,\n",
       "  0.52929944,\n",
       "  0.5416109],\n",
       " 'lr': [0.05,\n",
       "  0.04999935,\n",
       "  0.0499987,\n",
       "  0.049998052,\n",
       "  0.049997404,\n",
       "  0.049996752,\n",
       "  0.049996104,\n",
       "  0.049995456,\n",
       "  0.049994804,\n",
       "  0.049994156,\n",
       "  0.049993508,\n",
       "  0.04999286,\n",
       "  0.049992207,\n",
       "  0.04999156,\n",
       "  0.04999091,\n",
       "  0.049990263,\n",
       "  0.04998961,\n",
       "  0.049988963,\n",
       "  0.049988315,\n",
       "  0.049987666,\n",
       "  0.049987018,\n",
       "  0.049986366,\n",
       "  0.049985718,\n",
       "  0.04998507,\n",
       "  0.04998442,\n",
       "  0.04998377,\n",
       "  0.04998312,\n",
       "  0.049982473,\n",
       "  0.049981825,\n",
       "  0.049981177,\n",
       "  0.04998053,\n",
       "  0.049979877,\n",
       "  0.04997923,\n",
       "  0.04997858,\n",
       "  0.049977932,\n",
       "  0.049977284,\n",
       "  0.049976636,\n",
       "  0.049975984,\n",
       "  0.049975336,\n",
       "  0.049974687,\n",
       "  0.04997404,\n",
       "  0.04997339,\n",
       "  0.049972743,\n",
       "  0.049972095,\n",
       "  0.049971446,\n",
       "  0.049970794,\n",
       "  0.049970146,\n",
       "  0.049969498,\n",
       "  0.04996885,\n",
       "  0.0499682,\n",
       "  0.049967553,\n",
       "  0.049966905,\n",
       "  0.049966257,\n",
       "  0.04996561,\n",
       "  0.04996496,\n",
       "  0.049964312,\n",
       "  0.049963664,\n",
       "  0.049963016,\n",
       "  0.049962368,\n",
       "  0.049961716,\n",
       "  0.049961068,\n",
       "  0.04996042,\n",
       "  0.04995977,\n",
       "  0.049959123,\n",
       "  0.049958475,\n",
       "  0.049957827,\n",
       "  0.04995718,\n",
       "  0.04995653,\n",
       "  0.049955882,\n",
       "  0.049955234,\n",
       "  0.049954586,\n",
       "  0.049953938,\n",
       "  0.04995329,\n",
       "  0.04995264,\n",
       "  0.049951993,\n",
       "  0.049951345,\n",
       "  0.049950697,\n",
       "  0.04995005,\n",
       "  0.0499494,\n",
       "  0.049948756,\n",
       "  0.049948107,\n",
       "  0.04994746,\n",
       "  0.04994681,\n",
       "  0.049946163,\n",
       "  0.049945515,\n",
       "  0.049944866,\n",
       "  0.04994422,\n",
       "  0.04994357,\n",
       "  0.04994292,\n",
       "  0.049942274,\n",
       "  0.049941625,\n",
       "  0.049940977,\n",
       "  0.049940333,\n",
       "  0.049939685,\n",
       "  0.049939036,\n",
       "  0.04993839,\n",
       "  0.04993774,\n",
       "  0.04993709,\n",
       "  0.049936444,\n",
       "  0.049935795,\n",
       "  0.04993515,\n",
       "  0.049934503,\n",
       "  0.049933854,\n",
       "  0.049933206,\n",
       "  0.049932558,\n",
       "  0.04993191,\n",
       "  0.04993126,\n",
       "  0.049930617,\n",
       "  0.04992997,\n",
       "  0.04992932,\n",
       "  0.049928673,\n",
       "  0.049928024,\n",
       "  0.04992738,\n",
       "  0.04992673,\n",
       "  0.049926084,\n",
       "  0.049925435,\n",
       "  0.049924787,\n",
       "  0.049924143,\n",
       "  0.049923494,\n",
       "  0.049922846,\n",
       "  0.049922198,\n",
       "  0.049921554,\n",
       "  0.049920905,\n",
       "  0.049920257,\n",
       "  0.04991961,\n",
       "  0.049918965,\n",
       "  0.049918316,\n",
       "  0.049917668,\n",
       "  0.04991702,\n",
       "  0.049916375,\n",
       "  0.049915727,\n",
       "  0.04991508,\n",
       "  0.04991443,\n",
       "  0.049913786,\n",
       "  0.04991314,\n",
       "  0.04991249,\n",
       "  0.049911845,\n",
       "  0.049911197,\n",
       "  0.04991055,\n",
       "  0.049909905,\n",
       "  0.049909256,\n",
       "  0.04990861,\n",
       "  0.049907964,\n",
       "  0.049907316,\n",
       "  0.049906667,\n",
       "  0.049906023,\n",
       "  0.049905375,\n",
       "  0.049904726,\n",
       "  0.049904082,\n",
       "  0.049903434,\n",
       "  0.049902786,\n",
       "  0.04990214,\n",
       "  0.049901493,\n",
       "  0.049900845,\n",
       "  0.0499002,\n",
       "  0.049899552,\n",
       "  0.049898908,\n",
       "  0.04989826,\n",
       "  0.04989761,\n",
       "  0.049896967,\n",
       "  0.04989632,\n",
       "  0.049895674,\n",
       "  0.049895026,\n",
       "  0.049894378,\n",
       "  0.049893733,\n",
       "  0.049893085,\n",
       "  0.04989244,\n",
       "  0.049891792,\n",
       "  0.049891148,\n",
       "  0.0498905,\n",
       "  0.049889855,\n",
       "  0.049889207,\n",
       "  0.049888562,\n",
       "  0.049887914,\n",
       "  0.049887266,\n",
       "  0.04988662,\n",
       "  0.049885973,\n",
       "  0.04988533,\n",
       "  0.04988468,\n",
       "  0.049884036,\n",
       "  0.049883388,\n",
       "  0.049882744,\n",
       "  0.049882095,\n",
       "  0.04988145,\n",
       "  0.049880803,\n",
       "  0.04988016,\n",
       "  0.04987951,\n",
       "  0.049878865,\n",
       "  0.04987822,\n",
       "  0.049877573,\n",
       "  0.04987693,\n",
       "  0.04987628,\n",
       "  0.049875636,\n",
       "  0.049874987,\n",
       "  0.049874343,\n",
       "  0.049873695,\n",
       "  0.04987305,\n",
       "  0.049872406,\n",
       "  0.049871758,\n",
       "  0.049871113]}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#size_histories['Tiny3'], Tinypred3, Tinyvalres3 = compile_and_fit(tiny_model3, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UekcaQdmZxnW"
   },
   "source": [
    "Note: All the above training runs used the `callbacks.EarlyStopping` to end the training once it was clear the model was not making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try different activation functions, consider ReLU, others?\n",
    "- Try different training/test split, 99% train vs 1% test – we would suggest more traditional 80% train vs 20% test\n",
    "- Try Adam -- \n",
    "\n",
    "**Sigmoid activation** – S shape, logistic function, take any input and produce result between 0 and 1, cannot be used with many layers due to vanishing gradients, nonlinear activation function, large negatives become 0/large positives become 1, drawbacks – sigmoids saturate, kill gradients; sigmoid outputs are not 0-centered, \n",
    "\n",
    "**Softmax activation** – \n",
    "\n",
    "**ReLU activation** – rectified linear activation function, piecewise linear function that outputs the input directly if positive (otherwise, output 0). Default activation, easy train, better performance, activation is threshold at 0, can accelerate SGD, implemented by simply thresholding matrix of activations at 0, can be fragile where weights could update in a way for neuron to not activate again (Leaky ReLU attempts to fix dying problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXJxtwBWIhjG"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjfnkEeQyAFG"
   },
   "source": [
    "To recap: here are the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Get more training data.\n",
    "* Reduce the capacity of the network.\n",
    "* Add weight regularization.\n",
    "* Add dropout.\n",
    "\n",
    "Two important approaches not covered in this guide are:\n",
    "\n",
    "* data-augmentation\n",
    "* batch normalization\n",
    "\n",
    "Remember that each method can help on its own, but often combining them can be even more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "NW will add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkAbvht_rIGj"
   },
   "source": [
    "## NW Q: What do we need from below? This seemed like extraneous code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lglk41MwvU5o"
   },
   "source": [
    "## Demonstrate overfitting\n",
    "\n",
    "The simplest way to prevent overfitting is to start with a small model: A model with a small number of learnable parameters (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's \"capacity\".\n",
    "\n",
    "Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\n",
    "\n",
    "Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\n",
    "\n",
    "On the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between \"too much capacity\" and \"not enough capacity\".\n",
    "\n",
    "Unfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or the right size for each layer). You will have to experiment using a series of different architectures.\n",
    "\n",
    "To find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.\n",
    "\n",
    "Start with a simple model using only `layers.Dense` as a baseline, then create larger versions, and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ReKHdC2EgVu"
   },
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNzkSkkXSP5l"
   },
   "source": [
    "Many models train better if you gradually reduce the learning rate during training. Use `optimizers.schedules` to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEQNKadHA0M3"
   },
   "source": [
    "### View in TensorBoard\n",
    "\n",
    "These models all wrote TensorBoard logs during training.\n",
    "\n",
    "To open an embedded  TensorBoard viewer inside a notebook, copy the following into a code-cell:\n",
    "\n",
    "```\n",
    "%tensorboard --logdir {logdir}/sizes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjqx3bywDPjf"
   },
   "source": [
    "You can view the [results of a previous run](https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
    "\n",
    "It's also included in an `<iframe>` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dX5fcgrADwym"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda9033b0b8>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
    "    width=\"100%\", height=\"800px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDQDBKYZBXF_"
   },
   "source": [
    "If you want to share TensorBoard results you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
    "\n",
    "Note: This step requires a Google account.\n",
    "\n",
    "```\n",
    "!tensorboard dev upload --logdir  {logdir}/sizes\n",
    "```\n",
    "\n",
    "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "overfit_and_underfit.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
