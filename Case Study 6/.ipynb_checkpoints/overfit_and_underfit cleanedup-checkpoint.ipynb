{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTFj8ft5dlbS"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "lzyBOpYMdp3F"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "m_x4KfSJ7Vt7"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9HmC2T4ld5B"
   },
   "source": [
    "# Overfit and underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRTxFhXAlnl1"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19rPukKZsPG6"
   },
   "source": [
    "As always, the code in this example will use the `tf.keras` API, which you can learn more about in the TensorFlow [Keras guide](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "In both of the previous examples—[classifying text](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub) and [predicting fuel efficiency](https://www.tensorflow.org/tutorials/keras/regression) — we saw that the accuracy of our model on the validation data would peak after training for a number of epochs, and would then stagnate or start decreasing.\n",
    "\n",
    "In other words, our model would *overfit* to the training data. Learning how to deal with overfitting is important. Although it's often possible to achieve high accuracy on the *training set*, what we really want is to develop models that generalize well to a *testing set* (or data they haven't seen before).\n",
    "\n",
    "The opposite of overfitting is *underfitting*. Underfitting occurs when there is still room for improvement on the test data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data.\n",
    "\n",
    "If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. We need to strike a balance. Understanding how to train for an appropriate number of epochs as we'll explore below is a useful skill.\n",
    "\n",
    "To prevent overfitting, the best solution is to use more complete training data. The dataset should cover the full range of inputs that the model is expected to handle. Additional data may only be useful if it covers new and interesting cases.\n",
    "\n",
    "A model trained on more complete data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization. These place constraints on the quantity and type of information your model can store.  If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
    "\n",
    "In this notebook, we'll explore several common regularization techniques, and use them to improve on a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WL8UoOTmGGsL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "Before getting started, import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pZ8A2liqvgk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnAtAjqRYVXe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs\n",
    "#!pip install git+https://github.com/tensorflow/docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3S2qD9OrICX"
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pnOU-ctX27Q"
   },
   "outputs": [],
   "source": [
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jj6I4dvTtbUe"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cweoTiruj8O"
   },
   "source": [
    "\n",
    "## The Higgs Dataset\n",
    "\n",
    "The goal of this tutorial is not to do particle physics, so don't dwell on the details of the dataset. It contains 11&#x202F;000&#x202F;000 examples, each with 28 features, and a binary class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvrEErCirICm"
   },
   "source": [
    "https://www.tensorflow.org/datasets/catalog/higgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPjAvwb-6dFd"
   },
   "outputs": [],
   "source": [
    "#gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
    "#gz = 'C:/Users/kevinm/Documents/SMU/MSDS7337_NaturalLanguageProcessing/Week11/HIGGS.csv.gz'\n",
    "gz = 'C:/Users/shayden/Downloads/HIGGS.csv.gz'\n",
    "#gz = tf.keras.utils.get_file('HIGGS.csv.gz', '/home/jjschued/HIGGS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "proxy = 'http://proxy.rockwellcollins.com:9090'\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/17vHho4WOidi1xsU5nuYIxvgaxKTK2L75/view?usp=sharing\n",
    "file_id = '17vHho4WOidi1xsU5nuYIxvgaxKTK2L75'\n",
    "#url_dataset = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "#url_dataset = '/user/jjschued/HIGGS.csv'\n",
    "#url_dataset = 'https://drive.google.com/u/1/uc?export=download&confirm=cg02&id=17vHho4WOidi1xsU5nuYIxvgaxKTK2L75'\n",
    "#url_dataset = \"https://doc-0g-4c-docs.googleusercontent.com/docs/securesc/aplji487l37jpmjrtc3fc7mkdck4ah8r/qd2imheo9cvohf9r82e90395lm9o7bva/1595540025000/05402358165231425872/05402358165231425872/17vHho4WOidi1xsU5nuYIxvgaxKTK2L75?e=download&authuser=1\"\n",
    "#url = requests.get(url_dataset).text\n",
    "#csv_raw = StringIO(url)\n",
    "#url_dataset = '/home/jjschued/HIGGS.csv'\n",
    "df = pd.read_csv('C:/Users/shayden/Downloads/HIGGS.csv.gz', compression = 'gzip', header = None)\n",
    "#url_dataset = 'C:/Users/shayden/Downloads/HIGGS.csv'\n",
    "#df = pd.read_csv(url_dataset, nrows=11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.159912</td>\n",
       "      <td>1.013847</td>\n",
       "      <td>0.108615</td>\n",
       "      <td>1.495524</td>\n",
       "      <td>-0.537545</td>\n",
       "      <td>2.342396</td>\n",
       "      <td>-0.839740</td>\n",
       "      <td>1.320683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097068</td>\n",
       "      <td>1.190680</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.822136</td>\n",
       "      <td>0.766772</td>\n",
       "      <td>1.002191</td>\n",
       "      <td>1.061233</td>\n",
       "      <td>0.837004</td>\n",
       "      <td>0.860472</td>\n",
       "      <td>0.772484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618388</td>\n",
       "      <td>-1.012982</td>\n",
       "      <td>1.110139</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>-0.379199</td>\n",
       "      <td>1.004656</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>-1.678593</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216995</td>\n",
       "      <td>1.049177</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.826829</td>\n",
       "      <td>0.989809</td>\n",
       "      <td>1.029104</td>\n",
       "      <td>1.199679</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>0.865269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700559</td>\n",
       "      <td>0.774251</td>\n",
       "      <td>1.520182</td>\n",
       "      <td>0.847112</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>1.585235</td>\n",
       "      <td>1.713962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337374</td>\n",
       "      <td>0.845208</td>\n",
       "      <td>0.987610</td>\n",
       "      <td>0.883422</td>\n",
       "      <td>1.888438</td>\n",
       "      <td>1.153766</td>\n",
       "      <td>0.931279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.178030</td>\n",
       "      <td>0.117796</td>\n",
       "      <td>-1.276980</td>\n",
       "      <td>1.864457</td>\n",
       "      <td>-0.584370</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>-1.264549</td>\n",
       "      <td>1.276333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399515</td>\n",
       "      <td>-1.313189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838842</td>\n",
       "      <td>0.882890</td>\n",
       "      <td>1.201380</td>\n",
       "      <td>0.939216</td>\n",
       "      <td>0.339705</td>\n",
       "      <td>0.759070</td>\n",
       "      <td>0.719119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464477</td>\n",
       "      <td>-0.337047</td>\n",
       "      <td>0.229019</td>\n",
       "      <td>0.954596</td>\n",
       "      <td>-0.868466</td>\n",
       "      <td>0.430004</td>\n",
       "      <td>-0.271348</td>\n",
       "      <td>-1.252278</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.652782</td>\n",
       "      <td>-0.586254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752535</td>\n",
       "      <td>0.740727</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>0.663952</td>\n",
       "      <td>0.576084</td>\n",
       "      <td>0.541427</td>\n",
       "      <td>0.517420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0         1.0  0.869293 -0.635082  0.225690  0.327470 -0.689993  0.754202   \n",
       "1         1.0  0.907542  0.329147  0.359412  1.497970 -0.313010  1.095531   \n",
       "2         1.0  0.798835  1.470639 -1.635975  0.453773  0.425629  1.104875   \n",
       "3         0.0  1.344385 -0.876626  0.935913  1.992050  0.882454  1.786066   \n",
       "4         1.0  1.105009  0.321356  1.522401  0.882808 -1.205349  0.681466   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "10999995  1.0  1.159912  1.013847  0.108615  1.495524 -0.537545  2.342396   \n",
       "10999996  1.0  0.618388 -1.012982  1.110139  0.941023 -0.379199  1.004656   \n",
       "10999997  1.0  0.700559  0.774251  1.520182  0.847112  0.211230  1.095531   \n",
       "10999998  0.0  1.178030  0.117796 -1.276980  1.864457 -0.584370  0.998519   \n",
       "10999999  0.0  0.464477 -0.337047  0.229019  0.954596 -0.868466  0.430004   \n",
       "\n",
       "                7         8         9   ...        19        20        21  \\\n",
       "0        -0.248573 -1.092064  0.000000  ... -0.010455 -0.045767  3.101961   \n",
       "1        -0.557525 -1.588230  2.173076  ... -1.138930 -0.000819  0.000000   \n",
       "2         1.282322  1.381664  0.000000  ...  1.128848  0.900461  0.000000   \n",
       "3        -1.646778 -0.942383  0.000000  ... -0.678379 -1.360356  0.000000   \n",
       "4        -1.070464 -0.921871  0.000000  ... -0.373566  0.113041  0.000000   \n",
       "...            ...       ...       ...  ...       ...       ...       ...   \n",
       "10999995 -0.839740  1.320683  0.000000  ... -0.097068  1.190680  3.101961   \n",
       "10999996  0.348535 -1.678593  2.173076  ... -0.216995  1.049177  3.101961   \n",
       "10999997  0.052457  0.024553  2.173076  ...  1.585235  1.713962  0.000000   \n",
       "10999998 -1.264549  1.276333  0.000000  ...  1.399515 -1.313189  0.000000   \n",
       "10999999 -0.271348 -1.252278  2.173076  ... -1.652782 -0.586254  0.000000   \n",
       "\n",
       "                22        23        24        25        26        27        28  \n",
       "0         1.353760  0.979563  0.978076  0.920005  0.721657  0.988751  0.876678  \n",
       "1         0.302220  0.833048  0.985700  0.978098  0.779732  0.992356  0.798343  \n",
       "2         0.909753  1.108330  0.985692  0.951331  0.803252  0.865924  0.780118  \n",
       "3         0.946652  1.028704  0.998656  0.728281  0.869200  1.026736  0.957904  \n",
       "4         0.755856  1.361057  0.986610  0.838085  1.133295  0.872245  0.808487  \n",
       "...            ...       ...       ...       ...       ...       ...       ...  \n",
       "10999995  0.822136  0.766772  1.002191  1.061233  0.837004  0.860472  0.772484  \n",
       "10999996  0.826829  0.989809  1.029104  1.199679  0.891481  0.938490  0.865269  \n",
       "10999997  0.337374  0.845208  0.987610  0.883422  1.888438  1.153766  0.931279  \n",
       "10999998  0.838842  0.882890  1.201380  0.939216  0.339705  0.759070  0.719119  \n",
       "10999999  0.752535  0.740727  0.986917  0.663952  0.576084  0.541427  0.517420  \n",
       "\n",
       "[11000000 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000 entries, 0 to 10999\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   1.000000000000000000e+00    11000 non-null  float64\n",
      " 1   8.692932128906250000e-01    11000 non-null  float64\n",
      " 2   -6.350818276405334473e-01   11000 non-null  float64\n",
      " 3   2.256902605295181274e-01    11000 non-null  float64\n",
      " 4   3.274700641632080078e-01    11000 non-null  float64\n",
      " 5   -6.899932026863098145e-01   11000 non-null  float64\n",
      " 6   7.542022466659545898e-01    11000 non-null  float64\n",
      " 7   -2.485731393098831177e-01   11000 non-null  float64\n",
      " 8   -1.092063903808593750e+00   11000 non-null  float64\n",
      " 9   0.000000000000000000e+00    11000 non-null  float64\n",
      " 10  1.374992132186889648e+00    11000 non-null  float64\n",
      " 11  -6.536741852760314941e-01   11000 non-null  float64\n",
      " 12  9.303491115570068359e-01    11000 non-null  float64\n",
      " 13  1.107436060905456543e+00    11000 non-null  float64\n",
      " 14  1.138904333114624023e+00    11000 non-null  float64\n",
      " 15  -1.578198313713073730e+00   11000 non-null  float64\n",
      " 16  -1.046985387802124023e+00   11000 non-null  float64\n",
      " 17  0.000000000000000000e+00.1  11000 non-null  float64\n",
      " 18  6.579295396804809570e-01    11000 non-null  float64\n",
      " 19  -1.045456994324922562e-02   11000 non-null  float64\n",
      " 20  -4.576716944575309753e-02   11000 non-null  float64\n",
      " 21  3.101961374282836914e+00    11000 non-null  float64\n",
      " 22  1.353760004043579102e+00    11000 non-null  float64\n",
      " 23  9.795631170272827148e-01    11000 non-null  float64\n",
      " 24  9.780761599540710449e-01    11000 non-null  float64\n",
      " 25  9.200048446655273438e-01    11000 non-null  float64\n",
      " 26  7.216574549674987793e-01    11000 non-null  float64\n",
      " 27  9.887509346008300781e-01    11000 non-null  float64\n",
      " 28  8.766783475875854492e-01    11000 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkiyUdaWIrww"
   },
   "outputs": [],
   "source": [
    "FEATURES = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFggl9gYKKRJ"
   },
   "source": [
    "The `tf.data.experimental.CsvDataset` class can be used to read csv records directly from a gzip file with no intermediate decompression step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHz4sLVQEVIU"
   },
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzahEELTKlSV"
   },
   "source": [
    "That csv reader class returns a list of scalars for each record. The following function repacks that list of scalars into a (feature_vector, label) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPD6ICDlF6Wf"
   },
   "outputs": [],
   "source": [
    "def pack_row(*row):\n",
    "  label = row[0]\n",
    "  features = tf.stack(row[1:],1)\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oa8tLuwLsbO"
   },
   "source": [
    "TensorFlow is most efficient when operating on large batches of data.\n",
    "\n",
    "So instead of repacking each row individually make a new `Dataset` that takes batches of 10000-examples, applies the `pack_row` function to each batch, and then splits the batches back up into individual records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-w-VHTwwGVoZ"
   },
   "outputs": [],
   "source": [
    "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUbxc5bxNSXV"
   },
   "source": [
    "Have a look at some of the records from this new `packed_ds`.\n",
    "\n",
    "The features are not perfectly normalized, but this is sufficient for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfcXuv33Fvka"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.8692932  -0.6350818   0.22569026  0.32747006 -0.6899932   0.75420225\n",
      " -0.24857314 -1.0920639   0.          1.3749921  -0.6536742   0.9303491\n",
      "  1.1074361   1.1389043  -1.5781983  -1.0469854   0.          0.65792954\n",
      " -0.01045457 -0.04576717  3.1019614   1.35376     0.9795631   0.97807616\n",
      "  0.92000484  0.72165745  0.98875093  0.87667835], shape=(28,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for features,label in packed_ds.batch(1000).take(1):\n",
    "  print(features[0])\n",
    "  plt.hist(features.numpy().flatten(), bins = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICKZRY7gN-QM"
   },
   "source": [
    "To keep this tutorial relatively short use just the first 1000 samples for validation, and the next 10 000 for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmk49OqZIFZP"
   },
   "outputs": [],
   "source": [
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FP3M9DmvON32"
   },
   "source": [
    "The `Dataset.skip` and `Dataset.take` methods make this easy.\n",
    "\n",
    "At the same time, use the `Dataset.cache` method to ensure that the loader doesn't need to re-read the data form the file on each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8H_ZzpBOOk-"
   },
   "outputs": [],
   "source": [
    "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zAOqk2_Px7K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: ((28,), ()), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxwwxMRQrIDm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: ((28,), ()), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PMliHoVO3OL"
   },
   "source": [
    "These datasets return individual examples. Use the `.batch` method to create batches of an appropriate size for training. Before batching also remember to `.shuffle` and `.repeat` the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7I4J355O223"
   },
   "outputs": [],
   "source": [
    "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTDdrrEbrIDw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28), (None,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9vkcK9crID3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28), (None,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lglk41MwvU5o"
   },
   "source": [
    "## Demonstrate overfitting\n",
    "\n",
    "The simplest way to prevent overfitting is to start with a small model: A model with a small number of learnable parameters (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's \"capacity\".\n",
    "\n",
    "Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\n",
    "\n",
    "Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\n",
    "\n",
    "On the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between \"too much capacity\" and \"not enough capacity\".\n",
    "\n",
    "Unfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or the right size for each layer). You will have to experiment using a series of different architectures.\n",
    "\n",
    "To find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.\n",
    "\n",
    "Start with a simple model using only `layers.Dense` as a baseline, then create larger versions, and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ReKHdC2EgVu"
   },
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNzkSkkXSP5l"
   },
   "source": [
    "Many models train better if you gradually reduce the learning rate during training. Use `optimizers.schedules` to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwQp-ERhAD6F"
   },
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kANLx6OYTQ8B"
   },
   "source": [
    "The code above sets a `schedules.InverseTimeDecay` to hyperbolically decrease the learning rate to 1/2 of the base rate at 1000 epochs, 1/3 at 2000 epochs and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIo_yPjEAFgn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ya7x7gr9UjU0"
   },
   "source": [
    "Each model in this tutorial will use the same training configuration. So set these up in a reusable way, starting with the list of callbacks.\n",
    "\n",
    "The training for this tutorial runs for many short epochs. To reduce the logging noise use the `tfdocs.EpochDots` which simply a `.` for each epoch and, and a full set of metrics every 100 epochs.\n",
    "\n",
    "Next include `callbacks.EarlyStopping` to avoid long and unnecessary training times. Note that this callback is set to monitor the `val_binary_crossentropy`, not the `val_loss`. This difference will be important later.\n",
    "\n",
    "Use `callbacks.TensorBoard` to generate TensorBoard logs for the training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSv8rfw_T85n"
   },
   "outputs": [],
   "source": [
    "def schedule(lr):\n",
    "    if lr > 0.000001:\n",
    "        return float(lr)\n",
    "    else:\n",
    "        return float(0.000001)\n",
    "def\n",
    "\n",
    "def get_callbacks(name):\n",
    "    \n",
    "  return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "    tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhctzKhBWVDD"
   },
   "source": [
    "Similarly each model will use the same `Model.compile` and `Model.fit` settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRCGwU3YH5sT"
   },
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=500):\n",
    "  sgd = tf.keras.optimizers.SGD(lr=.05, decay = 1.0000002 )\n",
    "    \n",
    "  if optimizer is None:\n",
    "    optimizer = sgd # Steven's optimizer\n",
    "    #optimizer = get_optimizer()#example optimizer tuned to match case study parms\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(\n",
    "                      from_logits=True, name='binary_crossentropy'),\n",
    "                  'accuracy',\n",
    "#                  keras.metrics.AUC(name='auc')\n",
    "                ])\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "#tf.keras.metrics.AUC(\n",
    "#    num_thresholds=200, curve='ROC', summation_method='interpolation', name=None,\n",
    "#    dtype=None, thresholds=None, multi_label=False, label_weights=None\n",
    "#)\n",
    "  model.summary()\n",
    "\n",
    "  history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=validate_ds,\n",
    "    callbacks=get_callbacks(name),\n",
    "    verbose=0)\n",
    "\n",
    "  #historypredict = model.predict(train_ds, steps = 10)\n",
    "  #eval_result = model.evaluate(train_ds, steps=10)  \n",
    "  return history\n",
    "#, historypredict, eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vw0AoNIUrIES"
   },
   "outputs": [],
   "source": [
    "#understand models how to determine what the actual values and what was predicted\n",
    "#simple output in dataframe like format of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxBeiLUiWHJV"
   },
   "source": [
    "### Tiny model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6JDv12scLTI"
   },
   "source": [
    "Start by training a model:\n",
    "#### Case parms \n",
    "* (Done) We selected a five-layer neural network with 300 hidden units in each layer, \n",
    "* (Done) a learning rate of 0.05, and a weight decay coefficient of 1 × 10−5.\n",
    "* (Done)Hidden layers have tanh activation function\n",
    "* (Done)Gradient computations were made on mini-batches of size 100\n",
    "* An additional boost in performance is obtained by using the dropout training algorithm, in which we stochastically drop neurons in the top hidden layer with 50% probability during training.\n",
    "* Weights were initialized from a normal distribution with zero mean and standard deviation 0.1 in the first layer, 0.001 in the output layer, and 0.05 all other hidden layers. \n",
    "* A momentum term increased linearly over the first 200 epochs from 0.9 to 0.99, at which point it remained constant. \n",
    "* The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10^−6,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzZ0_MzPrIEb"
   },
   "outputs": [],
   "source": [
    "tiny_model2 = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='tanh', input_shape=(FEATURES,)),#input + hidden 1 # maybe set weights at each layer\n",
    "    layers.Dense(300, activation='tanh'), #hidden 2\n",
    "    layers.Dense(300, activation='tanh'), #hidden 3\n",
    "    layers.Dense(300, activation='tanh'), #hidden 4\n",
    "    layers.Dense(300, activation='tanh'), #hidden 5\n",
    "    layers.Dense(1,activation='sigmoid') #output\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_model3 = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='tanh', kernel_regularizer=regularizers.l2(0.05), input_shape=(FEATURES,)),#input + hidden 1 # maybe set weights at each layer\n",
    "    layers.Dense(300, activation='tanh', kernel_regularizer=regularizers.l2(0.05)), #hidden 2\n",
    "    layers.Dense(300, activation='tanh', kernel_regularizer=regularizers.l2(0.05)), #hidden 3\n",
    "    layers.Dense(300, activation='tanh', kernel_regularizer=regularizers.l2(0.05)), #hidden 4\n",
    "    layers.Dense(300, activation='tanh', kernel_regularizer=regularizers.l2(0.05)), #hidden 5\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "#    kernel_regularizer=regularizers.l2(0.001)) #output\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X72IUdWYipIS"
   },
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdOcJtPGHhJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 370,201\n",
      "Trainable params: 370,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, accuracy:0.7745,  binary_crossentropy:0.5952,  loss:0.5952,  val_accuracy:0.6410,  val_binary_crossentropy:0.6494,  val_loss:0.6494,  \n",
      "....................................................................................................\n",
      "Epoch: 100, accuracy:0.7873,  binary_crossentropy:0.5888,  loss:0.5888,  val_accuracy:0.6020,  val_binary_crossentropy:0.6543,  val_loss:0.6543,  \n",
      "....................................................................................................\n",
      "Epoch: 200, accuracy:0.7969,  binary_crossentropy:0.5837,  loss:0.5837,  val_accuracy:0.6560,  val_binary_crossentropy:0.6469,  val_loss:0.6469,  \n",
      "....................................................................................................\n",
      "Epoch: 300, accuracy:0.8191,  binary_crossentropy:0.5744,  loss:0.5744,  val_accuracy:0.6400,  val_binary_crossentropy:0.6503,  val_loss:0.6503,  \n",
      "................CPU times: user 51min 1s, sys: 37min 40s, total: 1h 28min 42s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "size_histories['Tiny'] = compile_and_fit(tiny_model2, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAW8oihArIEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval result: [0.5938187420368195, 0.5938187, 0.792]\n"
     ]
    }
   ],
   "source": [
    "print('Eval result: {}'.format(Tinyvalres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQ-aquzsrIEx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69c0569978>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_histories['Tiny']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rS_QGT6icwdI"
   },
   "source": [
    "Now check how the model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkEvb2x5XsjE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.7)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XmKDtOWzOpk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs [Log Scale]')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([5, max(plt.xlim())])\n",
    "plt.ylim([0.5, 0.7])\n",
    "plt.xlabel(\"Epochs [Log Scale]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>1.084947</td>\n",
       "      <td>0.299487</td>\n",
       "      <td>0.511042</td>\n",
       "      <td>-0.374340</td>\n",
       "      <td>0.774997</td>\n",
       "      <td>-0.218866</td>\n",
       "      <td>-0.547110</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377029</td>\n",
       "      <td>-1.557350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.077976</td>\n",
       "      <td>1.028331</td>\n",
       "      <td>0.984388</td>\n",
       "      <td>1.004476</td>\n",
       "      <td>0.925660</td>\n",
       "      <td>1.012436</td>\n",
       "      <td>1.057740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306174</td>\n",
       "      <td>-1.808714</td>\n",
       "      <td>-1.240359</td>\n",
       "      <td>1.365489</td>\n",
       "      <td>-1.740858</td>\n",
       "      <td>1.695741</td>\n",
       "      <td>0.731754</td>\n",
       "      <td>0.156495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.232118</td>\n",
       "      <td>-0.143431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886896</td>\n",
       "      <td>0.935988</td>\n",
       "      <td>0.973462</td>\n",
       "      <td>0.879262</td>\n",
       "      <td>0.605018</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>1.018208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794809</td>\n",
       "      <td>-0.059466</td>\n",
       "      <td>0.526425</td>\n",
       "      <td>0.736569</td>\n",
       "      <td>1.662992</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>-1.320002</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.238036</td>\n",
       "      <td>-1.050715</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.443730</td>\n",
       "      <td>1.199071</td>\n",
       "      <td>0.990352</td>\n",
       "      <td>0.697507</td>\n",
       "      <td>0.799412</td>\n",
       "      <td>1.002388</td>\n",
       "      <td>0.846745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.839097</td>\n",
       "      <td>0.202531</td>\n",
       "      <td>0.619642</td>\n",
       "      <td>0.497209</td>\n",
       "      <td>-1.177904</td>\n",
       "      <td>0.544788</td>\n",
       "      <td>1.262518</td>\n",
       "      <td>0.130439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.053149</td>\n",
       "      <td>-0.947502</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.806380</td>\n",
       "      <td>1.075156</td>\n",
       "      <td>0.987699</td>\n",
       "      <td>0.624265</td>\n",
       "      <td>0.887372</td>\n",
       "      <td>0.718604</td>\n",
       "      <td>0.641227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.265836</td>\n",
       "      <td>-0.984737</td>\n",
       "      <td>0.296158</td>\n",
       "      <td>1.097784</td>\n",
       "      <td>1.345723</td>\n",
       "      <td>0.668183</td>\n",
       "      <td>-0.723884</td>\n",
       "      <td>-0.941828</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112803</td>\n",
       "      <td>1.656251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704588</td>\n",
       "      <td>0.594713</td>\n",
       "      <td>1.772387</td>\n",
       "      <td>1.243505</td>\n",
       "      <td>0.677384</td>\n",
       "      <td>0.990824</td>\n",
       "      <td>0.782038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6   \\\n",
       "0      1.0  0.907542  0.329147  0.359412  1.497970 -0.313010  1.095531   \n",
       "1      1.0  0.798835  1.470639 -1.635975  0.453773  0.425629  1.104875   \n",
       "2      0.0  1.344385 -0.876626  0.935913  1.992050  0.882454  1.786066   \n",
       "3      1.0  1.105009  0.321356  1.522401  0.882808 -1.205349  0.681466   \n",
       "4      0.0  1.595839 -0.607811  0.007075  1.818450 -0.111906  0.847550   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "10995  1.0  0.767357  1.084947  0.299487  0.511042 -0.374340  0.774997   \n",
       "10996  0.0  0.306174 -1.808714 -1.240359  1.365489 -1.740858  1.695741   \n",
       "10997  1.0  0.794809 -0.059466  0.526425  0.736569  1.662992  0.614593   \n",
       "10998  0.0  0.839097  0.202531  0.619642  0.497209 -1.177904  0.544788   \n",
       "10999  1.0  2.265836 -0.984737  0.296158  1.097784  1.345723  0.668183   \n",
       "\n",
       "             7         8         9   ...        19        20        21  \\\n",
       "0     -0.557525 -1.588230  2.173076  ... -1.138930 -0.000819  0.000000   \n",
       "1      1.282322  1.381664  0.000000  ...  1.128848  0.900461  0.000000   \n",
       "2     -1.646778 -0.942383  0.000000  ... -0.678379 -1.360356  0.000000   \n",
       "3     -1.070464 -0.921871  0.000000  ... -0.373566  0.113041  0.000000   \n",
       "4     -0.566437  1.581239  2.173076  ... -0.654227 -1.274345  3.101961   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10995 -0.218866 -0.547110  1.086538  ...  1.377029 -1.557350  0.000000   \n",
       "10996  0.731754  0.156495  0.000000  ...  1.232118 -0.143431  0.000000   \n",
       "10997 -1.320002 -0.025997  0.000000  ... -1.238036 -1.050715  3.101961   \n",
       "10998  1.262518  0.130439  0.000000  ... -1.053149 -0.947502  3.101961   \n",
       "10999 -0.723884 -0.941828  2.173076  ...  0.112803  1.656251  0.000000   \n",
       "\n",
       "             22        23        24        25        26        27        28  \n",
       "0      0.302220  0.833048  0.985700  0.978098  0.779732  0.992356  0.798343  \n",
       "1      0.909753  1.108330  0.985692  0.951331  0.803252  0.865924  0.780118  \n",
       "2      0.946652  1.028704  0.998656  0.728281  0.869200  1.026736  0.957904  \n",
       "3      0.755856  1.361057  0.986610  0.838085  1.133295  0.872245  0.808487  \n",
       "4      0.823761  0.938191  0.971758  0.789176  0.430553  0.961357  0.957818  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10995  1.077976  1.028331  0.984388  1.004476  0.925660  1.012436  1.057740  \n",
       "10996  0.886896  0.935988  0.973462  0.879262  0.605018  0.853274  1.018208  \n",
       "10997  1.443730  1.199071  0.990352  0.697507  0.799412  1.002388  0.846745  \n",
       "10998  0.806380  1.075156  0.987699  0.624265  0.887372  0.718604  0.641227  \n",
       "10999  0.704588  0.594713  1.772387  1.243505  0.677384  0.990824  0.782038  \n",
       "\n",
       "[11000 rows x 29 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case parms\n",
    "* (Done) We selected a five-layer neural network with 300 hidden units in each layer,\n",
    "* (Done) a learning rate of 0.05, and a weight decay coefficient of 1 × 10−5.\n",
    "* (Done)Hidden layers have tanh activation function\n",
    "* (Done)Gradient computations were made on mini-batches of size 100\n",
    "* (Done)An additional boost in performance is obtained by using the dropout training algorithm, in which we stochastically drop neurons in the top hidden layer with 50% probability during training.\n",
    "* (Done)Weights were initialized from a normal distribution with zero mean and standard deviation 0.1 in the first layer, 0.001 in the output layer, and 0.05 all other hidden layers.\n",
    "* A momentum term increased linearly over the first 200 epochs from 0.9 to 0.99, at which point it remained constant.\n",
    "* (Done)The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10^−6,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout -\n",
    "Dropout technique works by randomly reducing the number of interconnecting neurons within a neural network. At every training step, each neuron has a chance of being left out, or rather, dropped out of the collated contribution from connected neurons.\n",
    "This technique minimizes overfitting because each neuron becomes independently sufficient, in the sense that the neurons within the layers learn weight values that are not based on the cooperation of its neighbouring neurons.\n",
    "\n",
    "#### Momentum -\n",
    "momentum helps SGD to navigate along the relevant directions and softens the oscillations in the irrelevant. It simply adds a fraction of the direction of the previous step to a current step. This achieves amplification of speed in the correct direction and softens oscillation in wrong directions. This fraction is usually in the (0, 1) range. It also makes sense to use adaptive momentum. In the beginning of learning a big momentum will only hinder your progress, so it makes sense to use something like 0.01 and once all the high gradients disappeared you can use a bigger momentum. There is one problem with momentum: when we are very close to the goal, our momentum in most of the cases is very high and it does not know that it should slow down. This can cause it to miss or oscillate around the minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8800 samples\n",
      "Epoch 1/50\n",
      "8800/8800 - 2s - loss: 0.6572 - accuracy: 0.6036 - auc: 0.6433\n",
      "Epoch 2/50\n",
      "8800/8800 - 1s - loss: 0.6452 - accuracy: 0.6245 - auc: 0.6679\n",
      "Epoch 3/50\n",
      "8800/8800 - 1s - loss: 0.6393 - accuracy: 0.6338 - auc: 0.6787\n",
      "Epoch 4/50\n",
      "8800/8800 - 1s - loss: 0.6366 - accuracy: 0.6359 - auc: 0.6828\n",
      "Epoch 5/50\n",
      "8800/8800 - 1s - loss: 0.6351 - accuracy: 0.6401 - auc: 0.6869\n",
      "Epoch 6/50\n",
      "8800/8800 - 1s - loss: 0.6320 - accuracy: 0.6413 - auc: 0.6898\n",
      "Epoch 7/50\n",
      "8800/8800 - 1s - loss: 0.6299 - accuracy: 0.6495 - auc: 0.6952\n",
      "Epoch 8/50\n",
      "8800/8800 - 1s - loss: 0.6279 - accuracy: 0.6482 - auc: 0.6982\n",
      "Epoch 9/50\n",
      "8800/8800 - 1s - loss: 0.6265 - accuracy: 0.6500 - auc: 0.7002\n",
      "Epoch 10/50\n",
      "8800/8800 - 1s - loss: 0.6234 - accuracy: 0.6540 - auc: 0.7057\n",
      "Epoch 11/50\n",
      "8800/8800 - 1s - loss: 0.6203 - accuracy: 0.6592 - auc: 0.7097\n",
      "Epoch 12/50\n",
      "8800/8800 - 1s - loss: 0.6181 - accuracy: 0.6587 - auc: 0.7144\n",
      "Epoch 13/50\n",
      "8800/8800 - 1s - loss: 0.6140 - accuracy: 0.6615 - auc: 0.7197\n",
      "Epoch 14/50\n",
      "8800/8800 - 1s - loss: 0.6118 - accuracy: 0.6682 - auc: 0.7226\n",
      "Epoch 15/50\n",
      "8800/8800 - 1s - loss: 0.6084 - accuracy: 0.6724 - auc: 0.7280\n",
      "Epoch 16/50\n",
      "8800/8800 - 1s - loss: 0.6029 - accuracy: 0.6725 - auc: 0.7345\n",
      "Epoch 17/50\n",
      "8800/8800 - 1s - loss: 0.5996 - accuracy: 0.6770 - auc: 0.7388\n",
      "Epoch 18/50\n",
      "8800/8800 - 1s - loss: 0.5941 - accuracy: 0.6831 - auc: 0.7468\n",
      "Epoch 19/50\n",
      "8800/8800 - 1s - loss: 0.5887 - accuracy: 0.6841 - auc: 0.7519\n",
      "Epoch 20/50\n",
      "8800/8800 - 1s - loss: 0.5828 - accuracy: 0.6927 - auc: 0.7582\n",
      "Epoch 21/50\n",
      "8800/8800 - 1s - loss: 0.5759 - accuracy: 0.7017 - auc: 0.7663\n",
      "Epoch 22/50\n",
      "8800/8800 - 1s - loss: 0.5725 - accuracy: 0.7000 - auc: 0.7696\n",
      "Epoch 23/50\n",
      "8800/8800 - 1s - loss: 0.5665 - accuracy: 0.7055 - auc: 0.7757\n",
      "Epoch 24/50\n",
      "8800/8800 - 1s - loss: 0.5608 - accuracy: 0.7136 - auc: 0.7818\n",
      "Epoch 25/50\n",
      "8800/8800 - 1s - loss: 0.5541 - accuracy: 0.7180 - auc: 0.7876\n",
      "Epoch 26/50\n",
      "8800/8800 - 1s - loss: 0.5508 - accuracy: 0.7206 - auc: 0.7918\n",
      "Epoch 27/50\n",
      "8800/8800 - 1s - loss: 0.5453 - accuracy: 0.7202 - auc: 0.7957\n",
      "Epoch 28/50\n",
      "8800/8800 - 1s - loss: 0.5415 - accuracy: 0.7255 - auc: 0.8004\n",
      "Epoch 29/50\n",
      "8800/8800 - 1s - loss: 0.5348 - accuracy: 0.7335 - auc: 0.8055\n",
      "Epoch 30/50\n",
      "8800/8800 - 1s - loss: 0.5329 - accuracy: 0.7309 - auc: 0.8074\n",
      "Epoch 31/50\n",
      "8800/8800 - 1s - loss: 0.5285 - accuracy: 0.7367 - auc: 0.8114\n",
      "Epoch 32/50\n",
      "8800/8800 - 1s - loss: 0.5225 - accuracy: 0.7383 - auc: 0.8162\n",
      "Epoch 33/50\n",
      "8800/8800 - 1s - loss: 0.5132 - accuracy: 0.7418 - auc: 0.8235\n",
      "Epoch 34/50\n",
      "8800/8800 - 1s - loss: 0.5085 - accuracy: 0.7470 - auc: 0.8277\n",
      "Epoch 35/50\n",
      "8800/8800 - 1s - loss: 0.5016 - accuracy: 0.7550 - auc: 0.8336\n",
      "Epoch 36/50\n",
      "8800/8800 - 1s - loss: 0.4938 - accuracy: 0.7555 - auc: 0.8392\n",
      "Epoch 37/50\n",
      "8800/8800 - 1s - loss: 0.4891 - accuracy: 0.7584 - auc: 0.8424\n",
      "Epoch 38/50\n",
      "8800/8800 - 1s - loss: 0.4811 - accuracy: 0.7632 - auc: 0.8487\n",
      "Epoch 39/50\n",
      "8800/8800 - 1s - loss: 0.4770 - accuracy: 0.7673 - auc: 0.8513\n",
      "Epoch 40/50\n",
      "8800/8800 - 1s - loss: 0.4663 - accuracy: 0.7752 - auc: 0.8593\n",
      "Epoch 41/50\n",
      "8800/8800 - 1s - loss: 0.4564 - accuracy: 0.7803 - auc: 0.8654\n",
      "Epoch 42/50\n",
      "8800/8800 - 1s - loss: 0.4513 - accuracy: 0.7811 - auc: 0.8686\n",
      "Epoch 43/50\n",
      "8800/8800 - 1s - loss: 0.4446 - accuracy: 0.7890 - auc: 0.8736\n",
      "Epoch 44/50\n",
      "8800/8800 - 1s - loss: 0.4339 - accuracy: 0.7939 - auc: 0.8807\n",
      "Epoch 45/50\n",
      "8800/8800 - 1s - loss: 0.4240 - accuracy: 0.7993 - auc: 0.8860\n",
      "Epoch 46/50\n",
      "8800/8800 - 1s - loss: 0.4169 - accuracy: 0.8055 - auc: 0.8903\n",
      "Epoch 47/50\n",
      "8800/8800 - 1s - loss: 0.4058 - accuracy: 0.8134 - auc: 0.8963\n",
      "Epoch 48/50\n",
      "8800/8800 - 1s - loss: 0.3915 - accuracy: 0.8233 - auc: 0.9048\n",
      "Epoch 49/50\n",
      "8800/8800 - 1s - loss: 0.3815 - accuracy: 0.8238 - auc: 0.9095\n",
      "Epoch 50/50\n",
      "8800/8800 - 1s - loss: 0.3697 - accuracy: 0.8351 - auc: 0.9161\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.layers import LeakyReLU\n",
    "#import pandas as pd\n",
    "#import io\n",
    "#import os\n",
    "#import requests\n",
    "#import numpy as np\n",
    "#from sklearn import metrics\n",
    "#from tensorflow.keras import optimizers\n",
    "\n",
    "#Parms needed for case study  \n",
    "\n",
    "#We selected a five-layer neural network with 300 hidden units in each layer,\n",
    "#a learning rate of 0.05, and a weight decay coefficient of 1 × 10−5.\n",
    "# Hidden layer have tanh activation function\n",
    "#Gradient computations were made on mini-batches of size 100\n",
    "#The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10^−6,\n",
    "\n",
    "def define_predictor(n_input):\n",
    " \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(300, kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), input_dim = n_input, activation='tanh')) # Hidden 1\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 2\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 3\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 4\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 5\n",
    "    model.add(tf.keras.layers.Dense(1,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None),activation='sigmoid')) # Output #1.2\n",
    "    #model.add(tf.keras.layers.Dense(1,activation='softmax')) # Output\n",
    "    sgd = tf.keras.optimizers.SGD(lr=.05, decay = 1.0000002, momentum=0.99)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd , metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "    \n",
    "#setup scaler\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "dfx\n",
    "X = dfx.loc[:, 1:28]\n",
    "#X = dfx[features].copy()\n",
    "Y = dfx[0].copy()\n",
    "y = Y.values\n",
    "\n",
    "# the cv=cvx parameter sets the grid search to split the training and testing data 10 times. \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\n",
    "\n",
    "Att_model = define_predictor(X_train.shape[1])\n",
    "history = Att_model.fit(scaler.fit_transform(X_train),np.array(y_train), verbose=2, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 370,201\n",
      "Trainable params: 370,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Att_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6571563876758922,\n",
       "  0.6451599910042503,\n",
       "  0.6393206540021029,\n",
       "  0.6365889770334417,\n",
       "  0.6351459620215676,\n",
       "  0.632022726319053,\n",
       "  0.6299112811955538,\n",
       "  0.6278824225339023,\n",
       "  0.6265260841629722,\n",
       "  0.6233547865260731,\n",
       "  0.6203326708620245,\n",
       "  0.6181080332669345,\n",
       "  0.6139661247080023,\n",
       "  0.6117591753872959,\n",
       "  0.6084144574945624,\n",
       "  0.6028794667937539,\n",
       "  0.5996392462470315,\n",
       "  0.5941123262318698,\n",
       "  0.5886754343726418,\n",
       "  0.5827725547010248,\n",
       "  0.5758814484422857,\n",
       "  0.5724782414869829,\n",
       "  0.5664510674910112,\n",
       "  0.5607989998297258,\n",
       "  0.5540679315003482,\n",
       "  0.5508389516310258,\n",
       "  0.5453073200312528,\n",
       "  0.5414554762840271,\n",
       "  0.5347706621343439,\n",
       "  0.5329407605257901,\n",
       "  0.5284980921311812,\n",
       "  0.5224655214222995,\n",
       "  0.5132276472178372,\n",
       "  0.5085079901868647,\n",
       "  0.5015570425987244,\n",
       "  0.4937599340352145,\n",
       "  0.48912333466789937,\n",
       "  0.4811018681526184,\n",
       "  0.47699236848137594,\n",
       "  0.46630282878875734,\n",
       "  0.45635651902718977,\n",
       "  0.45127150665629995,\n",
       "  0.444645285172896,\n",
       "  0.43388305100527674,\n",
       "  0.42397350159558383,\n",
       "  0.41693418090993706,\n",
       "  0.40582965222272005,\n",
       "  0.3914793762293729,\n",
       "  0.38147627494551917,\n",
       "  0.36970856254751033],\n",
       " 'accuracy': [0.6036364,\n",
       "  0.62454545,\n",
       "  0.63375,\n",
       "  0.6359091,\n",
       "  0.64011365,\n",
       "  0.64125,\n",
       "  0.64954543,\n",
       "  0.6481818,\n",
       "  0.65,\n",
       "  0.6539773,\n",
       "  0.65920454,\n",
       "  0.65875,\n",
       "  0.66147727,\n",
       "  0.66818184,\n",
       "  0.67238635,\n",
       "  0.6725,\n",
       "  0.67704546,\n",
       "  0.68306816,\n",
       "  0.6840909,\n",
       "  0.69272727,\n",
       "  0.70170456,\n",
       "  0.7,\n",
       "  0.7054545,\n",
       "  0.71363634,\n",
       "  0.7179545,\n",
       "  0.7205682,\n",
       "  0.7202273,\n",
       "  0.72545457,\n",
       "  0.7335227,\n",
       "  0.7309091,\n",
       "  0.7367045,\n",
       "  0.73829544,\n",
       "  0.7418182,\n",
       "  0.74704546,\n",
       "  0.755,\n",
       "  0.75545454,\n",
       "  0.7584091,\n",
       "  0.7631818,\n",
       "  0.7672727,\n",
       "  0.77522725,\n",
       "  0.7803409,\n",
       "  0.78113633,\n",
       "  0.78897727,\n",
       "  0.79386365,\n",
       "  0.7993182,\n",
       "  0.80545455,\n",
       "  0.8134091,\n",
       "  0.8232955,\n",
       "  0.82375,\n",
       "  0.83511364],\n",
       " 'auc': [0.64332473,\n",
       "  0.6679386,\n",
       "  0.6787412,\n",
       "  0.68282443,\n",
       "  0.6869157,\n",
       "  0.6897572,\n",
       "  0.6951942,\n",
       "  0.69815385,\n",
       "  0.70015836,\n",
       "  0.70569396,\n",
       "  0.7097394,\n",
       "  0.71437216,\n",
       "  0.71967506,\n",
       "  0.7226175,\n",
       "  0.7279764,\n",
       "  0.7345458,\n",
       "  0.73876673,\n",
       "  0.746764,\n",
       "  0.75186753,\n",
       "  0.758176,\n",
       "  0.76633686,\n",
       "  0.76959157,\n",
       "  0.77569115,\n",
       "  0.7818465,\n",
       "  0.78764486,\n",
       "  0.79181284,\n",
       "  0.7957259,\n",
       "  0.80039746,\n",
       "  0.8055142,\n",
       "  0.8074161,\n",
       "  0.81143415,\n",
       "  0.8162462,\n",
       "  0.8234735,\n",
       "  0.82765484,\n",
       "  0.83364177,\n",
       "  0.8392443,\n",
       "  0.8424327,\n",
       "  0.84866774,\n",
       "  0.851338,\n",
       "  0.85934025,\n",
       "  0.865439,\n",
       "  0.8686479,\n",
       "  0.87360513,\n",
       "  0.88071525,\n",
       "  0.88602895,\n",
       "  0.8903444,\n",
       "  0.8963058,\n",
       "  0.9048125,\n",
       "  0.9094707,\n",
       "  0.9160601]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#size_histories['Tiny3'], Tinypred3, Tinyvalres3 = compile_and_fit(tiny_model3, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UekcaQdmZxnW"
   },
   "source": [
    "Note: All the above training runs used the `callbacks.EarlyStopping` to end the training once it was clear the model was not making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEQNKadHA0M3"
   },
   "source": [
    "### View in TensorBoard\n",
    "\n",
    "These models all wrote TensorBoard logs during training.\n",
    "\n",
    "To open an embedded  TensorBoard viewer inside a notebook, copy the following into a code-cell:\n",
    "\n",
    "```\n",
    "%tensorboard --logdir {logdir}/sizes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjqx3bywDPjf"
   },
   "source": [
    "You can view the [results of a previous run](https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
    "\n",
    "It's also included in an `<iframe>` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dX5fcgrADwym"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f69c0590550>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
    "    width=\"100%\", height=\"800px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDQDBKYZBXF_"
   },
   "source": [
    "If you want to share TensorBoard results you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
    "\n",
    "Note: This step requires a Google account.\n",
    "\n",
    "```\n",
    "!tensorboard dev upload --logdir  {logdir}/sizes\n",
    "```\n",
    "\n",
    "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40k1eBtnQzNo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/tmpuu20wvxa/tensorboard_logs/regularizers/Tiny')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.rmtree(logdir/'regularizers/Tiny', ignore_errors=True)\n",
    "shutil.copytree(logdir/'sizes/Tiny', logdir/'regularizers/Tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFWMeFo7jLpN"
   },
   "outputs": [],
   "source": [
    "regularizer_histories = {}\n",
    "regularizer_histories['Tiny'] = size_histories['Tiny']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXJxtwBWIhjG"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjfnkEeQyAFG"
   },
   "source": [
    "To recap: here are the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Get more training data.\n",
    "* Reduce the capacity of the network.\n",
    "* Add weight regularization.\n",
    "* Add dropout.\n",
    "\n",
    "Two important approaches not covered in this guide are:\n",
    "\n",
    "* data-augmentation\n",
    "* batch normalization\n",
    "\n",
    "Remember that each method can help on its own, but often combining them can be even more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkAbvht_rIGj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "overfit_and_underfit.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
