{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: Deep Learning and the Higgs Dataset\n",
    "Steven Hayden, Kevin Mendonsa, Joe Schueder, Nicole Wittlin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of *high energy physics*, observing exotic particles and measuring their properties may yield critical insights about the nature of matter. However, the data for analyzing these exotic particles is extremely sparse. For example, the Large Hadron Collider produces approximately 10^11 collisions per hour, yet only roughly 300 of these collisions result in the desired exotic particles. Therefore, good data analysis depends on distinguishing collisions which produce particles of interest (signal) from those producing other particles (background).\n",
    "\n",
    "Machine learning, and specifically signal-versus-background classification, can aid in the analysis process. In 2014, Baldi et al. published the results of their work in \"Searching for Exotic Particles in High-Energy Phyics with Deep Learning\" to highlight how neural networks and deep learning approaches can improve the collider searches for exotic particles. \n",
    "\n",
    "Our objective is to recreate the neural network specifications used by Baldi and team leveraging updated techiques, as well as suggest improvements on their existing work given the advances in deep learning in the past six years.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WL8UoOTmGGsL"
   },
   "source": [
    "## Data Preparation and Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baldi and team published a dataset of 11 million simulated collision events for benchmarking machine learning classification algorithms in distinguishing exotic particles (known as Higgs bosons). This data was produced using a Monte Carlo simulation and has 28 features: the first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator and the last seven features are high-level features derived by physicists from the first 21 features to help discriminate between the two classes. This dataset is known as the Higgs data.\n",
    "\n",
    "The data set used in the Baldi analysis was nearly balanced with 53% positive examples in Higgs data. \n",
    "\n",
    "We did not conduct true data exploration on the data set; instead, we focused on confirming we had the correct information in a usable format. We opted to import the data as a dataframe, rather than a tensor. This choice made the data more readable and allowed for more straightforward analysis. We acknowledge that this may have a negative impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "### Package Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pZ8A2liqvgk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnAtAjqRYVXe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs\n",
    "#!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3S2qD9OrICX"
   },
   "outputs": [],
   "source": [
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.modeling\n",
    "#import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pnOU-ctX27Q"
   },
   "outputs": [],
   "source": [
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jj6I4dvTtbUe"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We should clean up the cell below and retain just the https line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPjAvwb-6dFd"
   },
   "outputs": [],
   "source": [
    "#gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
    "#gz = 'C:/Users/shayden/Downloads/HIGGS.csv.gz'\n",
    "#gz = tf.keras.utils.get_file('HIGGS.csv.gz', '/home/jjschued/HIGGS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "proxy = 'http://proxy.rockwellcollins.com:9090'\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We should clean up the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/17vHho4WOidi1xsU5nuYIxvgaxKTK2L75/view?usp=sharing\n",
    "file_id = '17vHho4WOidi1xsU5nuYIxvgaxKTK2L75'\n",
    "#url_dataset = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "#url_dataset = '/user/jjschued/HIGGS.csv'\n",
    "#url_dataset = 'https://drive.google.com/u/1/uc?export=download&confirm=cg02&id=17vHho4WOidi1xsU5nuYIxvgaxKTK2L75'\n",
    "#url_dataset = \"https://doc-0g-4c-docs.googleusercontent.com/docs/securesc/aplji487l37jpmjrtc3fc7mkdck4ah8r/qd2imheo9cvohf9r82e90395lm9o7bva/1595540025000/05402358165231425872/05402358165231425872/17vHho4WOidi1xsU5nuYIxvgaxKTK2L75?e=download&authuser=1\"\n",
    "#url = requests.get(url_dataset).text\n",
    "#csv_raw = StringIO(url)\n",
    "url_dataset = '/home/jjschued/HIGGS.csv'\n",
    "#df = pd.read_csv('C:/Users/shayden/Downloads/HIGGS.csv.gz', compression = 'gzip',nrows=11000, header = None)\n",
    "#url_dataset = 'C:/Users/shayden/Downloads/HIGGS.csv'\n",
    "#df = pd.read_csv(url_dataset, nrows=1000000)\n",
    "df = pd.read_csv(url_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jjschued/HIGGS.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.000000000000000000e+00</th>\n",
       "      <th>8.692932128906250000e-01</th>\n",
       "      <th>-6.350818276405334473e-01</th>\n",
       "      <th>2.256902605295181274e-01</th>\n",
       "      <th>3.274700641632080078e-01</th>\n",
       "      <th>-6.899932026863098145e-01</th>\n",
       "      <th>7.542022466659545898e-01</th>\n",
       "      <th>-2.485731393098831177e-01</th>\n",
       "      <th>-1.092063903808593750e+00</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>...</th>\n",
       "      <th>-1.045456994324922562e-02</th>\n",
       "      <th>-4.576716944575309753e-02</th>\n",
       "      <th>3.101961374282836914e+00</th>\n",
       "      <th>1.353760004043579102e+00</th>\n",
       "      <th>9.795631170272827148e-01</th>\n",
       "      <th>9.780761599540710449e-01</th>\n",
       "      <th>9.200048446655273438e-01</th>\n",
       "      <th>7.216574549674987793e-01</th>\n",
       "      <th>9.887509346008300781e-01</th>\n",
       "      <th>8.766783475875854492e-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999994</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.159912</td>\n",
       "      <td>1.013847</td>\n",
       "      <td>0.108615</td>\n",
       "      <td>1.495524</td>\n",
       "      <td>-0.537545</td>\n",
       "      <td>2.342396</td>\n",
       "      <td>-0.839740</td>\n",
       "      <td>1.320683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097068</td>\n",
       "      <td>1.190680</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.822136</td>\n",
       "      <td>0.766772</td>\n",
       "      <td>1.002191</td>\n",
       "      <td>1.061233</td>\n",
       "      <td>0.837004</td>\n",
       "      <td>0.860472</td>\n",
       "      <td>0.772484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618388</td>\n",
       "      <td>-1.012982</td>\n",
       "      <td>1.110139</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>-0.379199</td>\n",
       "      <td>1.004656</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>-1.678593</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216995</td>\n",
       "      <td>1.049177</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.826829</td>\n",
       "      <td>0.989809</td>\n",
       "      <td>1.029104</td>\n",
       "      <td>1.199679</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>0.865269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700559</td>\n",
       "      <td>0.774251</td>\n",
       "      <td>1.520182</td>\n",
       "      <td>0.847112</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>1.585235</td>\n",
       "      <td>1.713962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337374</td>\n",
       "      <td>0.845208</td>\n",
       "      <td>0.987610</td>\n",
       "      <td>0.883422</td>\n",
       "      <td>1.888438</td>\n",
       "      <td>1.153766</td>\n",
       "      <td>0.931279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.178030</td>\n",
       "      <td>0.117796</td>\n",
       "      <td>-1.276980</td>\n",
       "      <td>1.864457</td>\n",
       "      <td>-0.584370</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>-1.264549</td>\n",
       "      <td>1.276333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399515</td>\n",
       "      <td>-1.313189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838842</td>\n",
       "      <td>0.882890</td>\n",
       "      <td>1.201380</td>\n",
       "      <td>0.939216</td>\n",
       "      <td>0.339705</td>\n",
       "      <td>0.759070</td>\n",
       "      <td>0.719119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464477</td>\n",
       "      <td>-0.337047</td>\n",
       "      <td>0.229019</td>\n",
       "      <td>0.954596</td>\n",
       "      <td>-0.868466</td>\n",
       "      <td>0.430004</td>\n",
       "      <td>-0.271348</td>\n",
       "      <td>-1.252278</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.652782</td>\n",
       "      <td>-0.586254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752535</td>\n",
       "      <td>0.740727</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>0.663952</td>\n",
       "      <td>0.576084</td>\n",
       "      <td>0.541427</td>\n",
       "      <td>0.517420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10999999 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1.000000000000000000e+00  8.692932128906250000e-01  \\\n",
       "0                              1.0                  0.907542   \n",
       "1                              1.0                  0.798835   \n",
       "2                              0.0                  1.344385   \n",
       "3                              1.0                  1.105009   \n",
       "4                              0.0                  1.595839   \n",
       "...                            ...                       ...   \n",
       "10999994                       1.0                  1.159912   \n",
       "10999995                       1.0                  0.618388   \n",
       "10999996                       1.0                  0.700559   \n",
       "10999997                       0.0                  1.178030   \n",
       "10999998                       0.0                  0.464477   \n",
       "\n",
       "          -6.350818276405334473e-01  2.256902605295181274e-01  \\\n",
       "0                          0.329147                  0.359412   \n",
       "1                          1.470639                 -1.635975   \n",
       "2                         -0.876626                  0.935913   \n",
       "3                          0.321356                  1.522401   \n",
       "4                         -0.607811                  0.007075   \n",
       "...                             ...                       ...   \n",
       "10999994                   1.013847                  0.108615   \n",
       "10999995                  -1.012982                  1.110139   \n",
       "10999996                   0.774251                  1.520182   \n",
       "10999997                   0.117796                 -1.276980   \n",
       "10999998                  -0.337047                  0.229019   \n",
       "\n",
       "          3.274700641632080078e-01  -6.899932026863098145e-01  \\\n",
       "0                         1.497970                  -0.313010   \n",
       "1                         0.453773                   0.425629   \n",
       "2                         1.992050                   0.882454   \n",
       "3                         0.882808                  -1.205349   \n",
       "4                         1.818450                  -0.111906   \n",
       "...                            ...                        ...   \n",
       "10999994                  1.495524                  -0.537545   \n",
       "10999995                  0.941023                  -0.379199   \n",
       "10999996                  0.847112                   0.211230   \n",
       "10999997                  1.864457                  -0.584370   \n",
       "10999998                  0.954596                  -0.868466   \n",
       "\n",
       "          7.542022466659545898e-01  -2.485731393098831177e-01  \\\n",
       "0                         1.095531                  -0.557525   \n",
       "1                         1.104875                   1.282322   \n",
       "2                         1.786066                  -1.646778   \n",
       "3                         0.681466                  -1.070464   \n",
       "4                         0.847550                  -0.566437   \n",
       "...                            ...                        ...   \n",
       "10999994                  2.342396                  -0.839740   \n",
       "10999995                  1.004656                   0.348535   \n",
       "10999996                  1.095531                   0.052457   \n",
       "10999997                  0.998519                  -1.264549   \n",
       "10999998                  0.430004                  -0.271348   \n",
       "\n",
       "          -1.092063903808593750e+00  0.000000000000000000e+00  ...  \\\n",
       "0                         -1.588230                  2.173076  ...   \n",
       "1                          1.381664                  0.000000  ...   \n",
       "2                         -0.942383                  0.000000  ...   \n",
       "3                         -0.921871                  0.000000  ...   \n",
       "4                          1.581239                  2.173076  ...   \n",
       "...                             ...                       ...  ...   \n",
       "10999994                   1.320683                  0.000000  ...   \n",
       "10999995                  -1.678593                  2.173076  ...   \n",
       "10999996                   0.024553                  2.173076  ...   \n",
       "10999997                   1.276333                  0.000000  ...   \n",
       "10999998                  -1.252278                  2.173076  ...   \n",
       "\n",
       "          -1.045456994324922562e-02  -4.576716944575309753e-02  \\\n",
       "0                         -1.138930                  -0.000819   \n",
       "1                          1.128848                   0.900461   \n",
       "2                         -0.678379                  -1.360356   \n",
       "3                         -0.373566                   0.113041   \n",
       "4                         -0.654227                  -1.274345   \n",
       "...                             ...                        ...   \n",
       "10999994                  -0.097068                   1.190680   \n",
       "10999995                  -0.216995                   1.049177   \n",
       "10999996                   1.585235                   1.713962   \n",
       "10999997                   1.399515                  -1.313189   \n",
       "10999998                  -1.652782                  -0.586254   \n",
       "\n",
       "          3.101961374282836914e+00  1.353760004043579102e+00  \\\n",
       "0                         0.000000                  0.302220   \n",
       "1                         0.000000                  0.909753   \n",
       "2                         0.000000                  0.946652   \n",
       "3                         0.000000                  0.755856   \n",
       "4                         3.101961                  0.823761   \n",
       "...                            ...                       ...   \n",
       "10999994                  3.101961                  0.822136   \n",
       "10999995                  3.101961                  0.826829   \n",
       "10999996                  0.000000                  0.337374   \n",
       "10999997                  0.000000                  0.838842   \n",
       "10999998                  0.000000                  0.752535   \n",
       "\n",
       "          9.795631170272827148e-01  9.780761599540710449e-01  \\\n",
       "0                         0.833048                  0.985700   \n",
       "1                         1.108330                  0.985692   \n",
       "2                         1.028704                  0.998656   \n",
       "3                         1.361057                  0.986610   \n",
       "4                         0.938191                  0.971758   \n",
       "...                            ...                       ...   \n",
       "10999994                  0.766772                  1.002191   \n",
       "10999995                  0.989809                  1.029104   \n",
       "10999996                  0.845208                  0.987610   \n",
       "10999997                  0.882890                  1.201380   \n",
       "10999998                  0.740727                  0.986917   \n",
       "\n",
       "          9.200048446655273438e-01  7.216574549674987793e-01  \\\n",
       "0                         0.978098                  0.779732   \n",
       "1                         0.951331                  0.803252   \n",
       "2                         0.728281                  0.869200   \n",
       "3                         0.838085                  1.133295   \n",
       "4                         0.789176                  0.430553   \n",
       "...                            ...                       ...   \n",
       "10999994                  1.061233                  0.837004   \n",
       "10999995                  1.199679                  0.891481   \n",
       "10999996                  0.883422                  1.888438   \n",
       "10999997                  0.939216                  0.339705   \n",
       "10999998                  0.663952                  0.576084   \n",
       "\n",
       "          9.887509346008300781e-01  8.766783475875854492e-01  \n",
       "0                         0.992356                  0.798343  \n",
       "1                         0.865924                  0.780118  \n",
       "2                         1.026736                  0.957904  \n",
       "3                         0.872245                  0.808487  \n",
       "4                         0.961357                  0.957818  \n",
       "...                            ...                       ...  \n",
       "10999994                  0.860472                  0.772484  \n",
       "10999995                  0.938490                  0.865269  \n",
       "10999996                  1.153766                  0.931279  \n",
       "10999997                  0.759070                  0.719119  \n",
       "10999998                  0.541427                  0.517420  \n",
       "\n",
       "[10999999 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10999999 entries, 0 to 10999998\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   1.000000000000000000e+00    float64\n",
      " 1   8.692932128906250000e-01    float64\n",
      " 2   -6.350818276405334473e-01   float64\n",
      " 3   2.256902605295181274e-01    float64\n",
      " 4   3.274700641632080078e-01    float64\n",
      " 5   -6.899932026863098145e-01   float64\n",
      " 6   7.542022466659545898e-01    float64\n",
      " 7   -2.485731393098831177e-01   float64\n",
      " 8   -1.092063903808593750e+00   float64\n",
      " 9   0.000000000000000000e+00    float64\n",
      " 10  1.374992132186889648e+00    float64\n",
      " 11  -6.536741852760314941e-01   float64\n",
      " 12  9.303491115570068359e-01    float64\n",
      " 13  1.107436060905456543e+00    float64\n",
      " 14  1.138904333114624023e+00    float64\n",
      " 15  -1.578198313713073730e+00   float64\n",
      " 16  -1.046985387802124023e+00   float64\n",
      " 17  0.000000000000000000e+00.1  float64\n",
      " 18  6.579295396804809570e-01    float64\n",
      " 19  -1.045456994324922562e-02   float64\n",
      " 20  -4.576716944575309753e-02   float64\n",
      " 21  3.101961374282836914e+00    float64\n",
      " 22  1.353760004043579102e+00    float64\n",
      " 23  9.795631170272827148e-01    float64\n",
      " 24  9.780761599540710449e-01    float64\n",
      " 25  9.200048446655273438e-01    float64\n",
      " 26  7.216574549674987793e-01    float64\n",
      " 27  9.887509346008300781e-01    float64\n",
      " 28  8.766783475875854492e-01    float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkiyUdaWIrww"
   },
   "outputs": [],
   "source": [
    "FEATURES = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmk49OqZIFZP"
   },
   "outputs": [],
   "source": [
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating the Original Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baldi and team chose optimal hyper-parameters for the deep learning neural network using a subset of the Higgs data consisting of 2.6M training examples and 100,000 validation examples. They note that the computational costs of the process did not allow for thorough optimization; however they combined pre-training methods, network architectures, initial learning rates, and regularization methods to determine the best criteria. \n",
    "\n",
    "The hyper-parameter optimization was performed using the full set of Higgs features, and the various classifiers algorithms.  These classifiers were tested on 500,000 simulated examples generated using Monte Carlo procedures as training sets. The best performing model, as determined by the Receiver Operating Characteristic (ROC) curves is outlined below. \n",
    "\n",
    "**Optimal Model from Baldi and team**\n",
    "- Five-layer Neural Network with 300 hidden units in each layer\n",
    "- Learning Rate of 0.05\n",
    "- Weight Decay Coefficient 1 x 10-5\n",
    "- Predetermined without Optimization  \n",
    "    - Hidden units all used the **tahn** activation function\n",
    "    - Weights initialized from normal distribution with 0 mean and standard deviation 0.1 in first layer, 0.001 in output layer, and 0.05in all other hidden layers\n",
    "    - Mini-batches of size 100 to compute gradients\n",
    "    - Momentum increased linearly over the first 200 epochs from 0.9 to 0.99, then remained constant\n",
    "    - Learning Rate decayed by 1.0000002 every batch update until it reached a minimum of 10-6\n",
    "- Training ended when momentum reached maximum value and minimum error on validation set of 500,000 examples had not decreased by more than a factor of 0.00001 over 10 epochs. Early stopping prevented overfitting and each NN trained over 200-1000 epochs.\n",
    "- When training with dropout, increased learning rate decay to 1.0000003 and ended training when the momentum reached maximum value and error on validation set had not decreased for 40 epochs\n",
    "- Inputs standardized over entire train/test set with mean 0 and standard deviation of 1, except for features with values strictly greater than 0 (scaled for mean value of 1)\n",
    "- An additional boost in performance is obtained by using the dropout training algorithm, in which we stochastically drop neurons in the top hidden layer with 50% probability during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY?  It's not the assignment :)\n",
    "\n",
    "**Original model built with PyLearn2; NW will write up few sentences about PyLearn2**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999994</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.159912</td>\n",
       "      <td>1.013847</td>\n",
       "      <td>0.108615</td>\n",
       "      <td>1.495524</td>\n",
       "      <td>-0.537545</td>\n",
       "      <td>2.342396</td>\n",
       "      <td>-0.839740</td>\n",
       "      <td>1.320683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097068</td>\n",
       "      <td>1.190680</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.822136</td>\n",
       "      <td>0.766772</td>\n",
       "      <td>1.002191</td>\n",
       "      <td>1.061233</td>\n",
       "      <td>0.837004</td>\n",
       "      <td>0.860472</td>\n",
       "      <td>0.772484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618388</td>\n",
       "      <td>-1.012982</td>\n",
       "      <td>1.110139</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>-0.379199</td>\n",
       "      <td>1.004656</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>-1.678593</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216995</td>\n",
       "      <td>1.049177</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.826829</td>\n",
       "      <td>0.989809</td>\n",
       "      <td>1.029104</td>\n",
       "      <td>1.199679</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>0.865269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700559</td>\n",
       "      <td>0.774251</td>\n",
       "      <td>1.520182</td>\n",
       "      <td>0.847112</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>1.585235</td>\n",
       "      <td>1.713962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337374</td>\n",
       "      <td>0.845208</td>\n",
       "      <td>0.987610</td>\n",
       "      <td>0.883422</td>\n",
       "      <td>1.888438</td>\n",
       "      <td>1.153766</td>\n",
       "      <td>0.931279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.178030</td>\n",
       "      <td>0.117796</td>\n",
       "      <td>-1.276980</td>\n",
       "      <td>1.864457</td>\n",
       "      <td>-0.584370</td>\n",
       "      <td>0.998519</td>\n",
       "      <td>-1.264549</td>\n",
       "      <td>1.276333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399515</td>\n",
       "      <td>-1.313189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838842</td>\n",
       "      <td>0.882890</td>\n",
       "      <td>1.201380</td>\n",
       "      <td>0.939216</td>\n",
       "      <td>0.339705</td>\n",
       "      <td>0.759070</td>\n",
       "      <td>0.719119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464477</td>\n",
       "      <td>-0.337047</td>\n",
       "      <td>0.229019</td>\n",
       "      <td>0.954596</td>\n",
       "      <td>-0.868466</td>\n",
       "      <td>0.430004</td>\n",
       "      <td>-0.271348</td>\n",
       "      <td>-1.252278</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.652782</td>\n",
       "      <td>-0.586254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752535</td>\n",
       "      <td>0.740727</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>0.663952</td>\n",
       "      <td>0.576084</td>\n",
       "      <td>0.541427</td>\n",
       "      <td>0.517420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10999999 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0         1.0  0.907542  0.329147  0.359412  1.497970 -0.313010  1.095531   \n",
       "1         1.0  0.798835  1.470639 -1.635975  0.453773  0.425629  1.104875   \n",
       "2         0.0  1.344385 -0.876626  0.935913  1.992050  0.882454  1.786066   \n",
       "3         1.0  1.105009  0.321356  1.522401  0.882808 -1.205349  0.681466   \n",
       "4         0.0  1.595839 -0.607811  0.007075  1.818450 -0.111906  0.847550   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "10999994  1.0  1.159912  1.013847  0.108615  1.495524 -0.537545  2.342396   \n",
       "10999995  1.0  0.618388 -1.012982  1.110139  0.941023 -0.379199  1.004656   \n",
       "10999996  1.0  0.700559  0.774251  1.520182  0.847112  0.211230  1.095531   \n",
       "10999997  0.0  1.178030  0.117796 -1.276980  1.864457 -0.584370  0.998519   \n",
       "10999998  0.0  0.464477 -0.337047  0.229019  0.954596 -0.868466  0.430004   \n",
       "\n",
       "                7         8         9   ...        19        20        21  \\\n",
       "0        -0.557525 -1.588230  2.173076  ... -1.138930 -0.000819  0.000000   \n",
       "1         1.282322  1.381664  0.000000  ...  1.128848  0.900461  0.000000   \n",
       "2        -1.646778 -0.942383  0.000000  ... -0.678379 -1.360356  0.000000   \n",
       "3        -1.070464 -0.921871  0.000000  ... -0.373566  0.113041  0.000000   \n",
       "4        -0.566437  1.581239  2.173076  ... -0.654227 -1.274345  3.101961   \n",
       "...            ...       ...       ...  ...       ...       ...       ...   \n",
       "10999994 -0.839740  1.320683  0.000000  ... -0.097068  1.190680  3.101961   \n",
       "10999995  0.348535 -1.678593  2.173076  ... -0.216995  1.049177  3.101961   \n",
       "10999996  0.052457  0.024553  2.173076  ...  1.585235  1.713962  0.000000   \n",
       "10999997 -1.264549  1.276333  0.000000  ...  1.399515 -1.313189  0.000000   \n",
       "10999998 -0.271348 -1.252278  2.173076  ... -1.652782 -0.586254  0.000000   \n",
       "\n",
       "                22        23        24        25        26        27        28  \n",
       "0         0.302220  0.833048  0.985700  0.978098  0.779732  0.992356  0.798343  \n",
       "1         0.909753  1.108330  0.985692  0.951331  0.803252  0.865924  0.780118  \n",
       "2         0.946652  1.028704  0.998656  0.728281  0.869200  1.026736  0.957904  \n",
       "3         0.755856  1.361057  0.986610  0.838085  1.133295  0.872245  0.808487  \n",
       "4         0.823761  0.938191  0.971758  0.789176  0.430553  0.961357  0.957818  \n",
       "...            ...       ...       ...       ...       ...       ...       ...  \n",
       "10999994  0.822136  0.766772  1.002191  1.061233  0.837004  0.860472  0.772484  \n",
       "10999995  0.826829  0.989809  1.029104  1.199679  0.891481  0.938490  0.865269  \n",
       "10999996  0.337374  0.845208  0.987610  0.883422  1.888438  1.153766  0.931279  \n",
       "10999997  0.838842  0.882890  1.201380  0.939216  0.339705  0.759070  0.719119  \n",
       "10999998  0.752535  0.740727  0.986917  0.663952  0.576084  0.541427  0.517420  \n",
       "\n",
       "[10999999 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate** – a configurable hyperparameter that controls how quickly/slowly a neural network learns a problem, more specifically it controls how much to change the weights to correct for error during each iteration; a large learning rate allows model to train faster but a cost, where smaller learning rates may yield a better model, requiring more training epochs and smaller batch sizes. \n",
    "- Math: gradient descent algorithm multiples learning rate by gradient, for example value of 0.1 will update weight 10% of the amount it could be updated\n",
    "- Range: 0.0 to 1.0, traditional default 0.1 or 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate Decay** – how learning rate changes over training epochs; learning rate decay can be designed where large weight changes happen at the beginning of the process and smaller, fine-tune changes toward the end; another strategy is to decay over a fixed number of training epochs at a small, constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Out** – form of regularization to minimize overfitting; technique that randomly removes/inactivates neurons at each training step, which forces remaining neurons to be more independent because they learn rated not in conjunction/cooperation with neighboring neuron; roughly doubles the number of iterations required for convergence but training time for each epoch is less. **NW N: have picture of Drop Out if we want to include**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Functions** – aka transfer functions; functions that take a weighted sum of all inputs from previous layer and generates an output value for the next layer; for each node, it defines the output of the node given an input or set of inputs. \n",
    "- Tanh activation – a non-linear activation function that outputs values between -1.0 and 1.0 and the center falls around 0; limitations are that it can have limited sensitivity and is prone to saturation in larger, more layered networks due to vanishing gradients. **NW N: have picture of Tanh if we want to include**\n",
    "- Others include: Sigmoid, Softmax, ELU, ReLU, Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Momentum** – improves the speed of optimization in concert with step size by helping SGD algorithm navigate in relevant/optimal directions; in other words, it adds inertia to the algorithm update process to continue moving in the optimal direction; best to begin with smaller momentum and then increase after passing through larger gradients – momentum can cause learning process to miss or oscillate around the minima. \n",
    "- Math: adds a fraction of the direction of the previous step to a current step\n",
    "- Range: 0.0 to 1.0, traditional default 0.9, 0.99 or 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Gradient Descent** \n",
    "- **Batch Gradient Descent** – batch size is set to total number of examples in the training dataset\n",
    "- **Stochastic Gradient Descent** – batch size is set to one\n",
    "- **Minibatch Gradient Descent** – batch size is set to more than one and less than the total number of examples in the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch/batch size** – the number of data points/observations used in one iteration (one gradient update) of model training and dictates the number of training observations to be “learned” before updating internal parameters; generally, a larger batch involves more training examples, thus yielding a more stable learning process and accurate estimate. For example, a batch size of 32 means that 32 samples from the training dataset will be used to estimate error gradient before the model weights are updated. *NOTE: batch size and number of batches are different.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch** – represents the number of completes passes through the training dataset during the learning process, where the learning algorithm loops through a fixed number of epochs and within each, updates the network for each row in the training data; one epoch means that each sample in the training dataset has updated internal parameters; calculated as N / batch size training iterations, where N is the total number of examples. \n",
    "\n",
    "*“You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.”* - Jason Brownlee, PhD, https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration** – number of batches needed to complete on epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Iterations, Batches, and Epochs**\n",
    "\n",
    "Dataset with 200 samples<br>\n",
    "Batch size = 5<br>\n",
    "Epochs = 1000\n",
    "- Dataset will be divided into 40 batches, each with 5 samples, model weights will update after each batch of 5 samples\n",
    "- One epoch will involve 40 batches/40 updates to model\n",
    "- 1000 epochs, model will be exposed/passed through whole data 1000 times, total of 40,000 batches during entire training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def schedule( lr):\n",
    "#   if lr > 0.000001:\n",
    "#       return float(lr)\n",
    "#   else:\n",
    "#       return float(0.000001)\n",
    "\n",
    "    \n",
    "# learning rate schedule\n",
    "def schedule(epoch,lr):\n",
    "    initial_learning_rate=0.05\n",
    "    decay = 0.0000002\n",
    "    decay_steps= float(STEPS_PER_EPOCH*1000)\n",
    "    if lr > 0.000001:\n",
    "        LearningRate = initial_learning_rate / (1 + decay * epoch/ decay_steps)  \n",
    "                      \n",
    "        return float(LearningRate)\n",
    "    else:\n",
    "        LearningRate=float(0.000001)\n",
    "        return LearningRate\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_callbacks():\n",
    "    \n",
    "  return [\n",
    "#    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', mode = 'min', patience=60),\n",
    "    tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \n",
    "  return tf.keras.optimizers.SGD(learning_rate=0.05,momentum=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 - 15s - loss: 0.6971 - accuracy: 0.4948 - val_binary_crossentropy: 0.6971 - auc: 0.5678 - val_loss: 0.6843 - val_accuracy: 0.4736 - val_val_binary_crossentropy: 0.6842 - val_auc: 0.6220\n",
      "Epoch 2/200\n",
      "77/77 - 14s - loss: 0.6731 - accuracy: 0.5517 - val_binary_crossentropy: 0.6731 - auc: 0.6372 - val_loss: 0.6805 - val_accuracy: 0.4889 - val_val_binary_crossentropy: 0.6805 - val_auc: 0.6452\n",
      "Epoch 3/200\n",
      "77/77 - 14s - loss: 0.6685 - accuracy: 0.5698 - val_binary_crossentropy: 0.6685 - auc: 0.6514 - val_loss: 0.6790 - val_accuracy: 0.4945 - val_val_binary_crossentropy: 0.6790 - val_auc: 0.6537\n",
      "Epoch 4/200\n",
      "77/77 - 14s - loss: 0.6671 - accuracy: 0.5759 - val_binary_crossentropy: 0.6671 - auc: 0.6556 - val_loss: 0.6779 - val_accuracy: 0.4976 - val_val_binary_crossentropy: 0.6779 - val_auc: 0.6579\n",
      "Epoch 5/200\n",
      "77/77 - 14s - loss: 0.6663 - accuracy: 0.5795 - val_binary_crossentropy: 0.6663 - auc: 0.6581 - val_loss: 0.6768 - val_accuracy: 0.5014 - val_val_binary_crossentropy: 0.6768 - val_auc: 0.6599\n",
      "Epoch 6/200\n",
      "77/77 - 14s - loss: 0.6657 - accuracy: 0.5822 - val_binary_crossentropy: 0.6657 - auc: 0.6603 - val_loss: 0.6766 - val_accuracy: 0.5018 - val_val_binary_crossentropy: 0.6766 - val_auc: 0.6620\n",
      "Epoch 7/200\n",
      "77/77 - 14s - loss: 0.6652 - accuracy: 0.5838 - val_binary_crossentropy: 0.6652 - auc: 0.6625 - val_loss: 0.6761 - val_accuracy: 0.5032 - val_val_binary_crossentropy: 0.6761 - val_auc: 0.6640\n",
      "Epoch 8/200\n",
      "77/77 - 14s - loss: 0.6647 - accuracy: 0.5860 - val_binary_crossentropy: 0.6647 - auc: 0.6643 - val_loss: 0.6755 - val_accuracy: 0.5047 - val_val_binary_crossentropy: 0.6755 - val_auc: 0.6659\n",
      "Epoch 9/200\n",
      "77/77 - 14s - loss: 0.6643 - accuracy: 0.5877 - val_binary_crossentropy: 0.6643 - auc: 0.6662 - val_loss: 0.6754 - val_accuracy: 0.5048 - val_val_binary_crossentropy: 0.6754 - val_auc: 0.6673\n",
      "Epoch 10/200\n",
      "77/77 - 14s - loss: 0.6639 - accuracy: 0.5898 - val_binary_crossentropy: 0.6639 - auc: 0.6680 - val_loss: 0.6750 - val_accuracy: 0.5056 - val_val_binary_crossentropy: 0.6750 - val_auc: 0.6689\n",
      "Epoch 11/200\n",
      "77/77 - 14s - loss: 0.6635 - accuracy: 0.5913 - val_binary_crossentropy: 0.6635 - auc: 0.6695 - val_loss: 0.6749 - val_accuracy: 0.5055 - val_val_binary_crossentropy: 0.6749 - val_auc: 0.6703\n",
      "Epoch 12/200\n",
      "77/77 - 14s - loss: 0.6631 - accuracy: 0.5928 - val_binary_crossentropy: 0.6631 - auc: 0.6712 - val_loss: 0.6744 - val_accuracy: 0.5067 - val_val_binary_crossentropy: 0.6744 - val_auc: 0.6717\n",
      "Epoch 13/200\n",
      "77/77 - 14s - loss: 0.6626 - accuracy: 0.5945 - val_binary_crossentropy: 0.6626 - auc: 0.6728 - val_loss: 0.6746 - val_accuracy: 0.5053 - val_val_binary_crossentropy: 0.6746 - val_auc: 0.6732\n",
      "Epoch 14/200\n",
      "77/77 - 14s - loss: 0.6622 - accuracy: 0.5963 - val_binary_crossentropy: 0.6622 - auc: 0.6744 - val_loss: 0.6746 - val_accuracy: 0.5046 - val_val_binary_crossentropy: 0.6746 - val_auc: 0.6739\n",
      "Epoch 15/200\n",
      "77/77 - 14s - loss: 0.6616 - accuracy: 0.5984 - val_binary_crossentropy: 0.6616 - auc: 0.6763 - val_loss: 0.6745 - val_accuracy: 0.5041 - val_val_binary_crossentropy: 0.6745 - val_auc: 0.6750\n",
      "Epoch 16/200\n",
      "77/77 - 14s - loss: 0.6610 - accuracy: 0.6006 - val_binary_crossentropy: 0.6610 - auc: 0.6784 - val_loss: 0.6742 - val_accuracy: 0.5046 - val_val_binary_crossentropy: 0.6742 - val_auc: 0.6753\n",
      "Epoch 17/200\n",
      "77/77 - 14s - loss: 0.6604 - accuracy: 0.6032 - val_binary_crossentropy: 0.6604 - auc: 0.6806 - val_loss: 0.6754 - val_accuracy: 0.5002 - val_val_binary_crossentropy: 0.6754 - val_auc: 0.6747\n",
      "Epoch 18/200\n",
      "77/77 - 14s - loss: 0.6593 - accuracy: 0.6066 - val_binary_crossentropy: 0.6593 - auc: 0.6837 - val_loss: 0.6763 - val_accuracy: 0.4972 - val_val_binary_crossentropy: 0.6763 - val_auc: 0.6721\n",
      "Epoch 19/200\n",
      "77/77 - 14s - loss: 0.6580 - accuracy: 0.6112 - val_binary_crossentropy: 0.6580 - auc: 0.6877 - val_loss: 0.6782 - val_accuracy: 0.4919 - val_val_binary_crossentropy: 0.6782 - val_auc: 0.6663\n",
      "Epoch 20/200\n",
      "77/77 - 14s - loss: 0.6564 - accuracy: 0.6169 - val_binary_crossentropy: 0.6564 - auc: 0.6924 - val_loss: 0.6795 - val_accuracy: 0.4901 - val_val_binary_crossentropy: 0.6795 - val_auc: 0.6553\n",
      "Epoch 21/200\n",
      "77/77 - 14s - loss: 0.6547 - accuracy: 0.6225 - val_binary_crossentropy: 0.6547 - auc: 0.6965 - val_loss: 0.6815 - val_accuracy: 0.4870 - val_val_binary_crossentropy: 0.6815 - val_auc: 0.6398\n",
      "Epoch 22/200\n",
      "77/77 - 14s - loss: 0.6532 - accuracy: 0.6270 - val_binary_crossentropy: 0.6532 - auc: 0.6997 - val_loss: 0.6831 - val_accuracy: 0.4850 - val_val_binary_crossentropy: 0.6831 - val_auc: 0.6260\n",
      "Epoch 23/200\n",
      "77/77 - 14s - loss: 0.6519 - accuracy: 0.6307 - val_binary_crossentropy: 0.6519 - auc: 0.7019 - val_loss: 0.6844 - val_accuracy: 0.4829 - val_val_binary_crossentropy: 0.6844 - val_auc: 0.6162\n",
      "Epoch 24/200\n",
      "77/77 - 14s - loss: 0.6508 - accuracy: 0.6338 - val_binary_crossentropy: 0.6508 - auc: 0.7035 - val_loss: 0.6859 - val_accuracy: 0.4805 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.6051\n",
      "Epoch 25/200\n",
      "77/77 - 14s - loss: 0.6497 - accuracy: 0.6368 - val_binary_crossentropy: 0.6497 - auc: 0.7056 - val_loss: 0.6869 - val_accuracy: 0.4790 - val_val_binary_crossentropy: 0.6869 - val_auc: 0.5939\n",
      "Epoch 26/200\n",
      "77/77 - 14s - loss: 0.6484 - accuracy: 0.6403 - val_binary_crossentropy: 0.6484 - auc: 0.7081 - val_loss: 0.6872 - val_accuracy: 0.4791 - val_val_binary_crossentropy: 0.6872 - val_auc: 0.5810\n",
      "Epoch 27/200\n",
      "77/77 - 14s - loss: 0.6468 - accuracy: 0.6438 - val_binary_crossentropy: 0.6468 - auc: 0.7116 - val_loss: 0.6870 - val_accuracy: 0.4814 - val_val_binary_crossentropy: 0.6870 - val_auc: 0.5726\n",
      "Epoch 28/200\n",
      "77/77 - 14s - loss: 0.6449 - accuracy: 0.6481 - val_binary_crossentropy: 0.6449 - auc: 0.7161 - val_loss: 0.6856 - val_accuracy: 0.4870 - val_val_binary_crossentropy: 0.6856 - val_auc: 0.5692\n",
      "Epoch 29/200\n",
      "77/77 - 14s - loss: 0.6428 - accuracy: 0.6528 - val_binary_crossentropy: 0.6428 - auc: 0.7211 - val_loss: 0.6841 - val_accuracy: 0.4940 - val_val_binary_crossentropy: 0.6841 - val_auc: 0.5697\n",
      "Epoch 30/200\n",
      "77/77 - 14s - loss: 0.6409 - accuracy: 0.6569 - val_binary_crossentropy: 0.6409 - auc: 0.7255 - val_loss: 0.6844 - val_accuracy: 0.4951 - val_val_binary_crossentropy: 0.6844 - val_auc: 0.5645\n",
      "Epoch 31/200\n",
      "77/77 - 14s - loss: 0.6394 - accuracy: 0.6602 - val_binary_crossentropy: 0.6394 - auc: 0.7285 - val_loss: 0.6849 - val_accuracy: 0.4958 - val_val_binary_crossentropy: 0.6849 - val_auc: 0.5570\n",
      "Epoch 32/200\n",
      "77/77 - 14s - loss: 0.6383 - accuracy: 0.6624 - val_binary_crossentropy: 0.6383 - auc: 0.7302 - val_loss: 0.6859 - val_accuracy: 0.4934 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.5479\n",
      "Epoch 33/200\n",
      "77/77 - 14s - loss: 0.6375 - accuracy: 0.6641 - val_binary_crossentropy: 0.6375 - auc: 0.7311 - val_loss: 0.6868 - val_accuracy: 0.4917 - val_val_binary_crossentropy: 0.6868 - val_auc: 0.5414\n",
      "Epoch 34/200\n",
      "77/77 - 14s - loss: 0.6367 - accuracy: 0.6655 - val_binary_crossentropy: 0.6367 - auc: 0.7322 - val_loss: 0.6875 - val_accuracy: 0.4911 - val_val_binary_crossentropy: 0.6875 - val_auc: 0.5387\n",
      "Epoch 35/200\n",
      "77/77 - 14s - loss: 0.6360 - accuracy: 0.6671 - val_binary_crossentropy: 0.6360 - auc: 0.7332 - val_loss: 0.6881 - val_accuracy: 0.4907 - val_val_binary_crossentropy: 0.6881 - val_auc: 0.5378\n",
      "Epoch 36/200\n",
      "77/77 - 14s - loss: 0.6354 - accuracy: 0.6684 - val_binary_crossentropy: 0.6354 - auc: 0.7341 - val_loss: 0.6892 - val_accuracy: 0.4884 - val_val_binary_crossentropy: 0.6892 - val_auc: 0.5328\n",
      "Epoch 37/200\n",
      "77/77 - 14s - loss: 0.6348 - accuracy: 0.6697 - val_binary_crossentropy: 0.6348 - auc: 0.7354 - val_loss: 0.6897 - val_accuracy: 0.4876 - val_val_binary_crossentropy: 0.6897 - val_auc: 0.5324\n",
      "Epoch 38/200\n",
      "77/77 - 14s - loss: 0.6342 - accuracy: 0.6709 - val_binary_crossentropy: 0.6342 - auc: 0.7365 - val_loss: 0.6903 - val_accuracy: 0.4863 - val_val_binary_crossentropy: 0.6903 - val_auc: 0.5305\n",
      "Epoch 39/200\n",
      "77/77 - 14s - loss: 0.6337 - accuracy: 0.6723 - val_binary_crossentropy: 0.6337 - auc: 0.7377 - val_loss: 0.6909 - val_accuracy: 0.4846 - val_val_binary_crossentropy: 0.6909 - val_auc: 0.5265\n",
      "Epoch 40/200\n",
      "77/77 - 14s - loss: 0.6332 - accuracy: 0.6732 - val_binary_crossentropy: 0.6332 - auc: 0.7389 - val_loss: 0.6916 - val_accuracy: 0.4836 - val_val_binary_crossentropy: 0.6916 - val_auc: 0.5252\n",
      "Epoch 41/200\n",
      "77/77 - 14s - loss: 0.6326 - accuracy: 0.6746 - val_binary_crossentropy: 0.6326 - auc: 0.7403 - val_loss: 0.6922 - val_accuracy: 0.4838 - val_val_binary_crossentropy: 0.6922 - val_auc: 0.5278\n",
      "Epoch 42/200\n",
      "77/77 - 14s - loss: 0.6322 - accuracy: 0.6757 - val_binary_crossentropy: 0.6322 - auc: 0.7417 - val_loss: 0.6924 - val_accuracy: 0.4828 - val_val_binary_crossentropy: 0.6924 - val_auc: 0.5243\n",
      "Epoch 43/200\n",
      "77/77 - 14s - loss: 0.6317 - accuracy: 0.6770 - val_binary_crossentropy: 0.6317 - auc: 0.7431 - val_loss: 0.6930 - val_accuracy: 0.4822 - val_val_binary_crossentropy: 0.6930 - val_auc: 0.5244\n",
      "Epoch 44/200\n",
      "77/77 - 14s - loss: 0.6311 - accuracy: 0.6782 - val_binary_crossentropy: 0.6311 - auc: 0.7446 - val_loss: 0.6931 - val_accuracy: 0.4829 - val_val_binary_crossentropy: 0.6931 - val_auc: 0.5270\n",
      "Epoch 45/200\n",
      "77/77 - 14s - loss: 0.6307 - accuracy: 0.6794 - val_binary_crossentropy: 0.6307 - auc: 0.7460 - val_loss: 0.6936 - val_accuracy: 0.4824 - val_val_binary_crossentropy: 0.6936 - val_auc: 0.5299\n",
      "Epoch 46/200\n",
      "77/77 - 14s - loss: 0.6303 - accuracy: 0.6804 - val_binary_crossentropy: 0.6303 - auc: 0.7472 - val_loss: 0.6934 - val_accuracy: 0.4817 - val_val_binary_crossentropy: 0.6934 - val_auc: 0.5278\n",
      "Epoch 47/200\n",
      "77/77 - 14s - loss: 0.6299 - accuracy: 0.6811 - val_binary_crossentropy: 0.6299 - auc: 0.7481 - val_loss: 0.6932 - val_accuracy: 0.4826 - val_val_binary_crossentropy: 0.6932 - val_auc: 0.5299\n",
      "Epoch 48/200\n",
      "77/77 - 14s - loss: 0.6296 - accuracy: 0.6820 - val_binary_crossentropy: 0.6296 - auc: 0.7492 - val_loss: 0.6934 - val_accuracy: 0.4819 - val_val_binary_crossentropy: 0.6934 - val_auc: 0.5303\n",
      "Epoch 49/200\n",
      "77/77 - 14s - loss: 0.6292 - accuracy: 0.6830 - val_binary_crossentropy: 0.6292 - auc: 0.7503 - val_loss: 0.6937 - val_accuracy: 0.4810 - val_val_binary_crossentropy: 0.6937 - val_auc: 0.5290\n",
      "Epoch 50/200\n",
      "77/77 - 14s - loss: 0.6288 - accuracy: 0.6840 - val_binary_crossentropy: 0.6288 - auc: 0.7513 - val_loss: 0.6937 - val_accuracy: 0.4821 - val_val_binary_crossentropy: 0.6937 - val_auc: 0.5349\n",
      "Epoch 51/200\n",
      "77/77 - 14s - loss: 0.6285 - accuracy: 0.6846 - val_binary_crossentropy: 0.6285 - auc: 0.7521 - val_loss: 0.6937 - val_accuracy: 0.4822 - val_val_binary_crossentropy: 0.6937 - val_auc: 0.5368\n",
      "Epoch 52/200\n",
      "77/77 - 14s - loss: 0.6282 - accuracy: 0.6853 - val_binary_crossentropy: 0.6282 - auc: 0.7530 - val_loss: 0.6937 - val_accuracy: 0.4828 - val_val_binary_crossentropy: 0.6937 - val_auc: 0.5394\n",
      "Epoch 53/200\n",
      "77/77 - 14s - loss: 0.6280 - accuracy: 0.6859 - val_binary_crossentropy: 0.6280 - auc: 0.7537 - val_loss: 0.6937 - val_accuracy: 0.4829 - val_val_binary_crossentropy: 0.6937 - val_auc: 0.5399\n",
      "Epoch 54/200\n",
      "77/77 - 14s - loss: 0.6276 - accuracy: 0.6871 - val_binary_crossentropy: 0.6276 - auc: 0.7546 - val_loss: 0.6934 - val_accuracy: 0.4832 - val_val_binary_crossentropy: 0.6934 - val_auc: 0.5398\n",
      "Epoch 55/200\n",
      "77/77 - 14s - loss: 0.6273 - accuracy: 0.6875 - val_binary_crossentropy: 0.6273 - auc: 0.7552 - val_loss: 0.6942 - val_accuracy: 0.4833 - val_val_binary_crossentropy: 0.6942 - val_auc: 0.5441\n",
      "Epoch 56/200\n",
      "77/77 - 14s - loss: 0.6271 - accuracy: 0.6881 - val_binary_crossentropy: 0.6271 - auc: 0.7559 - val_loss: 0.6939 - val_accuracy: 0.4833 - val_val_binary_crossentropy: 0.6939 - val_auc: 0.5426\n",
      "Epoch 57/200\n",
      "77/77 - 14s - loss: 0.6268 - accuracy: 0.6888 - val_binary_crossentropy: 0.6268 - auc: 0.7566 - val_loss: 0.6941 - val_accuracy: 0.4845 - val_val_binary_crossentropy: 0.6941 - val_auc: 0.5468\n",
      "Epoch 58/200\n",
      "77/77 - 14s - loss: 0.6265 - accuracy: 0.6895 - val_binary_crossentropy: 0.6265 - auc: 0.7574 - val_loss: 0.6945 - val_accuracy: 0.4844 - val_val_binary_crossentropy: 0.6945 - val_auc: 0.5473\n",
      "Epoch 59/200\n",
      "77/77 - 14s - loss: 0.6263 - accuracy: 0.6901 - val_binary_crossentropy: 0.6263 - auc: 0.7580 - val_loss: 0.6941 - val_accuracy: 0.4838 - val_val_binary_crossentropy: 0.6941 - val_auc: 0.5454\n",
      "Epoch 60/200\n",
      "77/77 - 14s - loss: 0.6261 - accuracy: 0.6907 - val_binary_crossentropy: 0.6261 - auc: 0.7587 - val_loss: 0.6943 - val_accuracy: 0.4852 - val_val_binary_crossentropy: 0.6943 - val_auc: 0.5490\n",
      "Epoch 61/200\n",
      "77/77 - 14s - loss: 0.6258 - accuracy: 0.6915 - val_binary_crossentropy: 0.6258 - auc: 0.7594 - val_loss: 0.6940 - val_accuracy: 0.4855 - val_val_binary_crossentropy: 0.6940 - val_auc: 0.5508\n",
      "Epoch 62/200\n",
      "77/77 - 14s - loss: 0.6256 - accuracy: 0.6917 - val_binary_crossentropy: 0.6256 - auc: 0.7597 - val_loss: 0.6943 - val_accuracy: 0.4861 - val_val_binary_crossentropy: 0.6943 - val_auc: 0.5531\n",
      "Epoch 63/200\n",
      "77/77 - 14s - loss: 0.6253 - accuracy: 0.6925 - val_binary_crossentropy: 0.6253 - auc: 0.7605 - val_loss: 0.6941 - val_accuracy: 0.4867 - val_val_binary_crossentropy: 0.6941 - val_auc: 0.5544\n",
      "Epoch 64/200\n",
      "77/77 - 14s - loss: 0.6252 - accuracy: 0.6931 - val_binary_crossentropy: 0.6252 - auc: 0.7608 - val_loss: 0.6940 - val_accuracy: 0.4857 - val_val_binary_crossentropy: 0.6940 - val_auc: 0.5523\n",
      "Epoch 65/200\n",
      "77/77 - 14s - loss: 0.6250 - accuracy: 0.6934 - val_binary_crossentropy: 0.6250 - auc: 0.7614 - val_loss: 0.6939 - val_accuracy: 0.4881 - val_val_binary_crossentropy: 0.6940 - val_auc: 0.5597\n",
      "Epoch 66/200\n",
      "77/77 - 14s - loss: 0.6248 - accuracy: 0.6938 - val_binary_crossentropy: 0.6248 - auc: 0.7620 - val_loss: 0.6940 - val_accuracy: 0.4862 - val_val_binary_crossentropy: 0.6940 - val_auc: 0.5554\n",
      "Epoch 67/200\n",
      "77/77 - 14s - loss: 0.6246 - accuracy: 0.6944 - val_binary_crossentropy: 0.6246 - auc: 0.7625 - val_loss: 0.6939 - val_accuracy: 0.4864 - val_val_binary_crossentropy: 0.6939 - val_auc: 0.5550\n",
      "Epoch 68/200\n",
      "77/77 - 14s - loss: 0.6245 - accuracy: 0.6950 - val_binary_crossentropy: 0.6245 - auc: 0.7628 - val_loss: 0.6937 - val_accuracy: 0.4874 - val_val_binary_crossentropy: 0.6937 - val_auc: 0.5591\n",
      "Epoch 69/200\n",
      "77/77 - 14s - loss: 0.6243 - accuracy: 0.6955 - val_binary_crossentropy: 0.6243 - auc: 0.7632 - val_loss: 0.6934 - val_accuracy: 0.4875 - val_val_binary_crossentropy: 0.6934 - val_auc: 0.5576\n",
      "Epoch 70/200\n",
      "77/77 - 14s - loss: 0.6241 - accuracy: 0.6958 - val_binary_crossentropy: 0.6241 - auc: 0.7637 - val_loss: 0.6931 - val_accuracy: 0.4876 - val_val_binary_crossentropy: 0.6931 - val_auc: 0.5577\n",
      "Epoch 71/200\n",
      "77/77 - 14s - loss: 0.6239 - accuracy: 0.6963 - val_binary_crossentropy: 0.6239 - auc: 0.7643 - val_loss: 0.6933 - val_accuracy: 0.4884 - val_val_binary_crossentropy: 0.6933 - val_auc: 0.5624\n",
      "Epoch 72/200\n",
      "77/77 - 14s - loss: 0.6237 - accuracy: 0.6967 - val_binary_crossentropy: 0.6237 - auc: 0.7649 - val_loss: 0.6930 - val_accuracy: 0.4870 - val_val_binary_crossentropy: 0.6930 - val_auc: 0.5573\n",
      "Epoch 73/200\n",
      "77/77 - 14s - loss: 0.6236 - accuracy: 0.6969 - val_binary_crossentropy: 0.6236 - auc: 0.7650 - val_loss: 0.6932 - val_accuracy: 0.4891 - val_val_binary_crossentropy: 0.6932 - val_auc: 0.5643\n",
      "Epoch 74/200\n",
      "77/77 - 14s - loss: 0.6235 - accuracy: 0.6974 - val_binary_crossentropy: 0.6235 - auc: 0.7655 - val_loss: 0.6926 - val_accuracy: 0.4902 - val_val_binary_crossentropy: 0.6926 - val_auc: 0.5667\n",
      "Epoch 75/200\n",
      "77/77 - 14s - loss: 0.6233 - accuracy: 0.6979 - val_binary_crossentropy: 0.6233 - auc: 0.7660 - val_loss: 0.6927 - val_accuracy: 0.4892 - val_val_binary_crossentropy: 0.6927 - val_auc: 0.5666\n",
      "Epoch 76/200\n",
      "77/77 - 14s - loss: 0.6231 - accuracy: 0.6983 - val_binary_crossentropy: 0.6231 - auc: 0.7665 - val_loss: 0.6925 - val_accuracy: 0.4902 - val_val_binary_crossentropy: 0.6925 - val_auc: 0.5675\n",
      "Epoch 77/200\n",
      "77/77 - 14s - loss: 0.6230 - accuracy: 0.6987 - val_binary_crossentropy: 0.6230 - auc: 0.7669 - val_loss: 0.6926 - val_accuracy: 0.4901 - val_val_binary_crossentropy: 0.6926 - val_auc: 0.5692\n",
      "Epoch 78/200\n",
      "77/77 - 14s - loss: 0.6228 - accuracy: 0.6990 - val_binary_crossentropy: 0.6228 - auc: 0.7673 - val_loss: 0.6926 - val_accuracy: 0.4910 - val_val_binary_crossentropy: 0.6926 - val_auc: 0.5709\n",
      "Epoch 79/200\n",
      "77/77 - 14s - loss: 0.6227 - accuracy: 0.6993 - val_binary_crossentropy: 0.6227 - auc: 0.7676 - val_loss: 0.6919 - val_accuracy: 0.4901 - val_val_binary_crossentropy: 0.6919 - val_auc: 0.5680\n",
      "Epoch 80/200\n",
      "77/77 - 14s - loss: 0.6226 - accuracy: 0.6997 - val_binary_crossentropy: 0.6226 - auc: 0.7680 - val_loss: 0.6919 - val_accuracy: 0.4912 - val_val_binary_crossentropy: 0.6919 - val_auc: 0.5727\n",
      "Epoch 81/200\n",
      "77/77 - 14s - loss: 0.6225 - accuracy: 0.7000 - val_binary_crossentropy: 0.6225 - auc: 0.7684 - val_loss: 0.6918 - val_accuracy: 0.4908 - val_val_binary_crossentropy: 0.6919 - val_auc: 0.5715\n",
      "Epoch 82/200\n",
      "77/77 - 14s - loss: 0.6223 - accuracy: 0.7005 - val_binary_crossentropy: 0.6223 - auc: 0.7689 - val_loss: 0.6921 - val_accuracy: 0.4924 - val_val_binary_crossentropy: 0.6921 - val_auc: 0.5747\n",
      "Epoch 83/200\n",
      "77/77 - 14s - loss: 0.6222 - accuracy: 0.7010 - val_binary_crossentropy: 0.6222 - auc: 0.7691 - val_loss: 0.6920 - val_accuracy: 0.4915 - val_val_binary_crossentropy: 0.6920 - val_auc: 0.5740\n",
      "Epoch 84/200\n",
      "77/77 - 14s - loss: 0.6220 - accuracy: 0.7013 - val_binary_crossentropy: 0.6220 - auc: 0.7697 - val_loss: 0.6916 - val_accuracy: 0.4924 - val_val_binary_crossentropy: 0.6916 - val_auc: 0.5774\n",
      "Epoch 85/200\n",
      "77/77 - 14s - loss: 0.6219 - accuracy: 0.7016 - val_binary_crossentropy: 0.6219 - auc: 0.7699 - val_loss: 0.6912 - val_accuracy: 0.4922 - val_val_binary_crossentropy: 0.6912 - val_auc: 0.5755\n",
      "Epoch 86/200\n",
      "77/77 - 14s - loss: 0.6218 - accuracy: 0.7020 - val_binary_crossentropy: 0.6218 - auc: 0.7704 - val_loss: 0.6914 - val_accuracy: 0.4936 - val_val_binary_crossentropy: 0.6914 - val_auc: 0.5794\n",
      "Epoch 87/200\n",
      "77/77 - 14s - loss: 0.6216 - accuracy: 0.7025 - val_binary_crossentropy: 0.6216 - auc: 0.7709 - val_loss: 0.6913 - val_accuracy: 0.4932 - val_val_binary_crossentropy: 0.6914 - val_auc: 0.5789\n",
      "Epoch 88/200\n",
      "77/77 - 14s - loss: 0.6215 - accuracy: 0.7027 - val_binary_crossentropy: 0.6215 - auc: 0.7711 - val_loss: 0.6913 - val_accuracy: 0.4932 - val_val_binary_crossentropy: 0.6913 - val_auc: 0.5801\n",
      "Epoch 89/200\n",
      "77/77 - 14s - loss: 0.6213 - accuracy: 0.7030 - val_binary_crossentropy: 0.6213 - auc: 0.7715 - val_loss: 0.6912 - val_accuracy: 0.4921 - val_val_binary_crossentropy: 0.6912 - val_auc: 0.5775\n",
      "Epoch 90/200\n",
      "77/77 - 14s - loss: 0.6213 - accuracy: 0.7032 - val_binary_crossentropy: 0.6213 - auc: 0.7718 - val_loss: 0.6910 - val_accuracy: 0.4921 - val_val_binary_crossentropy: 0.6910 - val_auc: 0.5784\n",
      "Epoch 91/200\n",
      "77/77 - 14s - loss: 0.6211 - accuracy: 0.7037 - val_binary_crossentropy: 0.6211 - auc: 0.7722 - val_loss: 0.6908 - val_accuracy: 0.4955 - val_val_binary_crossentropy: 0.6908 - val_auc: 0.5842\n",
      "Epoch 92/200\n",
      "77/77 - 14s - loss: 0.6211 - accuracy: 0.7038 - val_binary_crossentropy: 0.6211 - auc: 0.7725 - val_loss: 0.6901 - val_accuracy: 0.4961 - val_val_binary_crossentropy: 0.6902 - val_auc: 0.5856\n",
      "Epoch 93/200\n",
      "77/77 - 14s - loss: 0.6209 - accuracy: 0.7042 - val_binary_crossentropy: 0.6209 - auc: 0.7728 - val_loss: 0.6901 - val_accuracy: 0.4963 - val_val_binary_crossentropy: 0.6901 - val_auc: 0.5837\n",
      "Epoch 94/200\n",
      "77/77 - 14s - loss: 0.6208 - accuracy: 0.7046 - val_binary_crossentropy: 0.6208 - auc: 0.7732 - val_loss: 0.6900 - val_accuracy: 0.4970 - val_val_binary_crossentropy: 0.6900 - val_auc: 0.5865\n",
      "Epoch 95/200\n",
      "77/77 - 14s - loss: 0.6207 - accuracy: 0.7049 - val_binary_crossentropy: 0.6207 - auc: 0.7735 - val_loss: 0.6905 - val_accuracy: 0.4951 - val_val_binary_crossentropy: 0.6905 - val_auc: 0.5856\n",
      "Epoch 96/200\n",
      "77/77 - 14s - loss: 0.6205 - accuracy: 0.7052 - val_binary_crossentropy: 0.6205 - auc: 0.7738 - val_loss: 0.6899 - val_accuracy: 0.4987 - val_val_binary_crossentropy: 0.6899 - val_auc: 0.5879\n",
      "Epoch 97/200\n",
      "77/77 - 14s - loss: 0.6204 - accuracy: 0.7055 - val_binary_crossentropy: 0.6204 - auc: 0.7743 - val_loss: 0.6901 - val_accuracy: 0.4973 - val_val_binary_crossentropy: 0.6901 - val_auc: 0.5882\n",
      "Epoch 98/200\n",
      "77/77 - 14s - loss: 0.6203 - accuracy: 0.7059 - val_binary_crossentropy: 0.6203 - auc: 0.7746 - val_loss: 0.6899 - val_accuracy: 0.4980 - val_val_binary_crossentropy: 0.6899 - val_auc: 0.5883\n",
      "Epoch 99/200\n",
      "77/77 - 14s - loss: 0.6202 - accuracy: 0.7062 - val_binary_crossentropy: 0.6202 - auc: 0.7750 - val_loss: 0.6900 - val_accuracy: 0.4971 - val_val_binary_crossentropy: 0.6900 - val_auc: 0.5878\n",
      "Epoch 100/200\n",
      "77/77 - 14s - loss: 0.6201 - accuracy: 0.7066 - val_binary_crossentropy: 0.6201 - auc: 0.7752 - val_loss: 0.6893 - val_accuracy: 0.4985 - val_val_binary_crossentropy: 0.6893 - val_auc: 0.5897\n",
      "Epoch 101/200\n",
      "77/77 - 14s - loss: 0.6200 - accuracy: 0.7067 - val_binary_crossentropy: 0.6200 - auc: 0.7756 - val_loss: 0.6896 - val_accuracy: 0.4956 - val_val_binary_crossentropy: 0.6896 - val_auc: 0.5882\n",
      "Epoch 102/200\n",
      "77/77 - 14s - loss: 0.6199 - accuracy: 0.7070 - val_binary_crossentropy: 0.6199 - auc: 0.7759 - val_loss: 0.6894 - val_accuracy: 0.4974 - val_val_binary_crossentropy: 0.6894 - val_auc: 0.5910\n",
      "Epoch 103/200\n",
      "77/77 - 14s - loss: 0.6197 - accuracy: 0.7073 - val_binary_crossentropy: 0.6197 - auc: 0.7762 - val_loss: 0.6889 - val_accuracy: 0.4971 - val_val_binary_crossentropy: 0.6890 - val_auc: 0.5896\n",
      "Epoch 104/200\n",
      "77/77 - 14s - loss: 0.6196 - accuracy: 0.7079 - val_binary_crossentropy: 0.6196 - auc: 0.7764 - val_loss: 0.6893 - val_accuracy: 0.4955 - val_val_binary_crossentropy: 0.6893 - val_auc: 0.5899\n",
      "Epoch 105/200\n",
      "77/77 - 14s - loss: 0.6195 - accuracy: 0.7079 - val_binary_crossentropy: 0.6195 - auc: 0.7767 - val_loss: 0.6892 - val_accuracy: 0.4957 - val_val_binary_crossentropy: 0.6892 - val_auc: 0.5905\n",
      "Epoch 106/200\n",
      "77/77 - 14s - loss: 0.6195 - accuracy: 0.7080 - val_binary_crossentropy: 0.6195 - auc: 0.7770 - val_loss: 0.6888 - val_accuracy: 0.4972 - val_val_binary_crossentropy: 0.6888 - val_auc: 0.5930\n",
      "Epoch 107/200\n",
      "77/77 - 14s - loss: 0.6193 - accuracy: 0.7085 - val_binary_crossentropy: 0.6193 - auc: 0.7774 - val_loss: 0.6890 - val_accuracy: 0.4962 - val_val_binary_crossentropy: 0.6890 - val_auc: 0.5915\n",
      "Epoch 108/200\n",
      "77/77 - 14s - loss: 0.6192 - accuracy: 0.7089 - val_binary_crossentropy: 0.6192 - auc: 0.7776 - val_loss: 0.6888 - val_accuracy: 0.4995 - val_val_binary_crossentropy: 0.6888 - val_auc: 0.5959\n",
      "Epoch 109/200\n",
      "77/77 - 14s - loss: 0.6191 - accuracy: 0.7092 - val_binary_crossentropy: 0.6191 - auc: 0.7780 - val_loss: 0.6884 - val_accuracy: 0.4982 - val_val_binary_crossentropy: 0.6885 - val_auc: 0.5948\n",
      "Epoch 110/200\n",
      "77/77 - 14s - loss: 0.6190 - accuracy: 0.7093 - val_binary_crossentropy: 0.6190 - auc: 0.7781 - val_loss: 0.6874 - val_accuracy: 0.5034 - val_val_binary_crossentropy: 0.6875 - val_auc: 0.5985\n",
      "Epoch 111/200\n",
      "77/77 - 14s - loss: 0.6189 - accuracy: 0.7095 - val_binary_crossentropy: 0.6189 - auc: 0.7784 - val_loss: 0.6883 - val_accuracy: 0.4967 - val_val_binary_crossentropy: 0.6883 - val_auc: 0.5955\n",
      "Epoch 112/200\n",
      "77/77 - 14s - loss: 0.6188 - accuracy: 0.7098 - val_binary_crossentropy: 0.6188 - auc: 0.7787 - val_loss: 0.6886 - val_accuracy: 0.4969 - val_val_binary_crossentropy: 0.6887 - val_auc: 0.5971\n",
      "Epoch 113/200\n",
      "77/77 - 14s - loss: 0.6188 - accuracy: 0.7098 - val_binary_crossentropy: 0.6188 - auc: 0.7786 - val_loss: 0.6878 - val_accuracy: 0.5009 - val_val_binary_crossentropy: 0.6878 - val_auc: 0.5993\n",
      "Epoch 114/200\n",
      "77/77 - 14s - loss: 0.6187 - accuracy: 0.7103 - val_binary_crossentropy: 0.6187 - auc: 0.7790 - val_loss: 0.6883 - val_accuracy: 0.4991 - val_val_binary_crossentropy: 0.6883 - val_auc: 0.5976\n",
      "Epoch 115/200\n",
      "77/77 - 14s - loss: 0.6185 - accuracy: 0.7106 - val_binary_crossentropy: 0.6185 - auc: 0.7792 - val_loss: 0.6873 - val_accuracy: 0.5020 - val_val_binary_crossentropy: 0.6873 - val_auc: 0.6018\n",
      "Epoch 116/200\n",
      "77/77 - 14s - loss: 0.6185 - accuracy: 0.7109 - val_binary_crossentropy: 0.6185 - auc: 0.7795 - val_loss: 0.6884 - val_accuracy: 0.4972 - val_val_binary_crossentropy: 0.6884 - val_auc: 0.5991\n",
      "Epoch 117/200\n",
      "77/77 - 14s - loss: 0.6184 - accuracy: 0.7109 - val_binary_crossentropy: 0.6184 - auc: 0.7796 - val_loss: 0.6877 - val_accuracy: 0.5025 - val_val_binary_crossentropy: 0.6877 - val_auc: 0.6032\n",
      "Epoch 118/200\n",
      "77/77 - 14s - loss: 0.6183 - accuracy: 0.7111 - val_binary_crossentropy: 0.6183 - auc: 0.7797 - val_loss: 0.6880 - val_accuracy: 0.4992 - val_val_binary_crossentropy: 0.6880 - val_auc: 0.6015\n",
      "Epoch 119/200\n",
      "77/77 - 14s - loss: 0.6182 - accuracy: 0.7115 - val_binary_crossentropy: 0.6182 - auc: 0.7801 - val_loss: 0.6878 - val_accuracy: 0.4999 - val_val_binary_crossentropy: 0.6878 - val_auc: 0.6024\n",
      "Epoch 120/200\n",
      "77/77 - 14s - loss: 0.6182 - accuracy: 0.7116 - val_binary_crossentropy: 0.6182 - auc: 0.7803 - val_loss: 0.6882 - val_accuracy: 0.4963 - val_val_binary_crossentropy: 0.6882 - val_auc: 0.6006\n",
      "Epoch 121/200\n",
      "77/77 - 14s - loss: 0.6181 - accuracy: 0.7118 - val_binary_crossentropy: 0.6181 - auc: 0.7804 - val_loss: 0.6878 - val_accuracy: 0.4995 - val_val_binary_crossentropy: 0.6878 - val_auc: 0.6047\n",
      "Epoch 122/200\n",
      "77/77 - 14s - loss: 0.6179 - accuracy: 0.7120 - val_binary_crossentropy: 0.6179 - auc: 0.7808 - val_loss: 0.6870 - val_accuracy: 0.5059 - val_val_binary_crossentropy: 0.6870 - val_auc: 0.6076\n",
      "Epoch 123/200\n",
      "77/77 - 14s - loss: 0.6179 - accuracy: 0.7122 - val_binary_crossentropy: 0.6179 - auc: 0.7809 - val_loss: 0.6873 - val_accuracy: 0.5046 - val_val_binary_crossentropy: 0.6873 - val_auc: 0.6065\n",
      "Epoch 124/200\n",
      "77/77 - 14s - loss: 0.6178 - accuracy: 0.7124 - val_binary_crossentropy: 0.6178 - auc: 0.7810 - val_loss: 0.6859 - val_accuracy: 0.5134 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.6106\n",
      "Epoch 125/200\n",
      "77/77 - 14s - loss: 0.6178 - accuracy: 0.7127 - val_binary_crossentropy: 0.6178 - auc: 0.7812 - val_loss: 0.6874 - val_accuracy: 0.5018 - val_val_binary_crossentropy: 0.6874 - val_auc: 0.6069\n",
      "Epoch 126/200\n",
      "77/77 - 14s - loss: 0.6177 - accuracy: 0.7130 - val_binary_crossentropy: 0.6177 - auc: 0.7814 - val_loss: 0.6875 - val_accuracy: 0.4994 - val_val_binary_crossentropy: 0.6876 - val_auc: 0.6061\n",
      "Epoch 127/200\n",
      "77/77 - 14s - loss: 0.6176 - accuracy: 0.7129 - val_binary_crossentropy: 0.6176 - auc: 0.7815 - val_loss: 0.6867 - val_accuracy: 0.5051 - val_val_binary_crossentropy: 0.6867 - val_auc: 0.6089\n",
      "Epoch 128/200\n",
      "77/77 - 14s - loss: 0.6176 - accuracy: 0.7132 - val_binary_crossentropy: 0.6176 - auc: 0.7817 - val_loss: 0.6874 - val_accuracy: 0.5013 - val_val_binary_crossentropy: 0.6874 - val_auc: 0.6062\n",
      "Epoch 129/200\n",
      "77/77 - 14s - loss: 0.6175 - accuracy: 0.7134 - val_binary_crossentropy: 0.6175 - auc: 0.7819 - val_loss: 0.6865 - val_accuracy: 0.5067 - val_val_binary_crossentropy: 0.6865 - val_auc: 0.6093\n",
      "Epoch 130/200\n",
      "77/77 - 14s - loss: 0.6174 - accuracy: 0.7136 - val_binary_crossentropy: 0.6174 - auc: 0.7821 - val_loss: 0.6867 - val_accuracy: 0.5065 - val_val_binary_crossentropy: 0.6867 - val_auc: 0.6097\n",
      "Epoch 131/200\n",
      "77/77 - 14s - loss: 0.6173 - accuracy: 0.7138 - val_binary_crossentropy: 0.6173 - auc: 0.7823 - val_loss: 0.6869 - val_accuracy: 0.5026 - val_val_binary_crossentropy: 0.6870 - val_auc: 0.6094\n",
      "Epoch 132/200\n",
      "77/77 - 14s - loss: 0.6172 - accuracy: 0.7139 - val_binary_crossentropy: 0.6172 - auc: 0.7825 - val_loss: 0.6870 - val_accuracy: 0.5042 - val_val_binary_crossentropy: 0.6870 - val_auc: 0.6099\n",
      "Epoch 133/200\n",
      "77/77 - 14s - loss: 0.6172 - accuracy: 0.7142 - val_binary_crossentropy: 0.6172 - auc: 0.7825 - val_loss: 0.6870 - val_accuracy: 0.5037 - val_val_binary_crossentropy: 0.6870 - val_auc: 0.6091\n",
      "Epoch 134/200\n",
      "77/77 - 14s - loss: 0.6171 - accuracy: 0.7143 - val_binary_crossentropy: 0.6171 - auc: 0.7827 - val_loss: 0.6867 - val_accuracy: 0.5061 - val_val_binary_crossentropy: 0.6867 - val_auc: 0.6114\n",
      "Epoch 135/200\n",
      "77/77 - 14s - loss: 0.6171 - accuracy: 0.7145 - val_binary_crossentropy: 0.6171 - auc: 0.7828 - val_loss: 0.6862 - val_accuracy: 0.5095 - val_val_binary_crossentropy: 0.6862 - val_auc: 0.6129\n",
      "Epoch 136/200\n",
      "77/77 - 14s - loss: 0.6170 - accuracy: 0.7146 - val_binary_crossentropy: 0.6170 - auc: 0.7831 - val_loss: 0.6867 - val_accuracy: 0.5055 - val_val_binary_crossentropy: 0.6867 - val_auc: 0.6120\n",
      "Epoch 137/200\n",
      "77/77 - 14s - loss: 0.6170 - accuracy: 0.7148 - val_binary_crossentropy: 0.6170 - auc: 0.7832 - val_loss: 0.6863 - val_accuracy: 0.5092 - val_val_binary_crossentropy: 0.6863 - val_auc: 0.6133\n",
      "Epoch 138/200\n",
      "77/77 - 14s - loss: 0.6169 - accuracy: 0.7149 - val_binary_crossentropy: 0.6169 - auc: 0.7833 - val_loss: 0.6866 - val_accuracy: 0.5079 - val_val_binary_crossentropy: 0.6866 - val_auc: 0.6131\n",
      "Epoch 139/200\n",
      "77/77 - 14s - loss: 0.6168 - accuracy: 0.7151 - val_binary_crossentropy: 0.6168 - auc: 0.7836 - val_loss: 0.6877 - val_accuracy: 0.5005 - val_val_binary_crossentropy: 0.6878 - val_auc: 0.6099\n",
      "Epoch 140/200\n",
      "77/77 - 14s - loss: 0.6168 - accuracy: 0.7151 - val_binary_crossentropy: 0.6168 - auc: 0.7836 - val_loss: 0.6857 - val_accuracy: 0.5129 - val_val_binary_crossentropy: 0.6858 - val_auc: 0.6162\n",
      "Epoch 141/200\n",
      "77/77 - 14s - loss: 0.6167 - accuracy: 0.7153 - val_binary_crossentropy: 0.6167 - auc: 0.7838 - val_loss: 0.6856 - val_accuracy: 0.5126 - val_val_binary_crossentropy: 0.6856 - val_auc: 0.6159\n",
      "Epoch 142/200\n",
      "77/77 - 14s - loss: 0.6166 - accuracy: 0.7156 - val_binary_crossentropy: 0.6166 - auc: 0.7841 - val_loss: 0.6861 - val_accuracy: 0.5116 - val_val_binary_crossentropy: 0.6861 - val_auc: 0.6145\n",
      "Epoch 143/200\n",
      "77/77 - 14s - loss: 0.6166 - accuracy: 0.7158 - val_binary_crossentropy: 0.6166 - auc: 0.7842 - val_loss: 0.6862 - val_accuracy: 0.5109 - val_val_binary_crossentropy: 0.6862 - val_auc: 0.6144\n",
      "Epoch 144/200\n",
      "77/77 - 14s - loss: 0.6165 - accuracy: 0.7159 - val_binary_crossentropy: 0.6165 - auc: 0.7842 - val_loss: 0.6862 - val_accuracy: 0.5088 - val_val_binary_crossentropy: 0.6862 - val_auc: 0.6141\n",
      "Epoch 145/200\n",
      "77/77 - 14s - loss: 0.6165 - accuracy: 0.7160 - val_binary_crossentropy: 0.6165 - auc: 0.7844 - val_loss: 0.6855 - val_accuracy: 0.5156 - val_val_binary_crossentropy: 0.6856 - val_auc: 0.6169\n",
      "Epoch 146/200\n",
      "77/77 - 14s - loss: 0.6164 - accuracy: 0.7162 - val_binary_crossentropy: 0.6164 - auc: 0.7845 - val_loss: 0.6856 - val_accuracy: 0.5189 - val_val_binary_crossentropy: 0.6856 - val_auc: 0.6171\n",
      "Epoch 147/200\n",
      "77/77 - 14s - loss: 0.6164 - accuracy: 0.7163 - val_binary_crossentropy: 0.6164 - auc: 0.7848 - val_loss: 0.6863 - val_accuracy: 0.5099 - val_val_binary_crossentropy: 0.6864 - val_auc: 0.6142\n",
      "Epoch 148/200\n",
      "77/77 - 14s - loss: 0.6163 - accuracy: 0.7164 - val_binary_crossentropy: 0.6163 - auc: 0.7849 - val_loss: 0.6864 - val_accuracy: 0.5097 - val_val_binary_crossentropy: 0.6864 - val_auc: 0.6154\n",
      "Epoch 149/200\n",
      "77/77 - 14s - loss: 0.6163 - accuracy: 0.7166 - val_binary_crossentropy: 0.6163 - auc: 0.7849 - val_loss: 0.6862 - val_accuracy: 0.5098 - val_val_binary_crossentropy: 0.6863 - val_auc: 0.6152\n",
      "Epoch 150/200\n",
      "77/77 - 14s - loss: 0.6162 - accuracy: 0.7166 - val_binary_crossentropy: 0.6162 - auc: 0.7850 - val_loss: 0.6861 - val_accuracy: 0.5167 - val_val_binary_crossentropy: 0.6861 - val_auc: 0.6170\n",
      "Epoch 151/200\n",
      "77/77 - 14s - loss: 0.6162 - accuracy: 0.7168 - val_binary_crossentropy: 0.6162 - auc: 0.7852 - val_loss: 0.6854 - val_accuracy: 0.5204 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6185\n",
      "Epoch 152/200\n",
      "77/77 - 14s - loss: 0.6161 - accuracy: 0.7168 - val_binary_crossentropy: 0.6161 - auc: 0.7854 - val_loss: 0.6868 - val_accuracy: 0.5054 - val_val_binary_crossentropy: 0.6868 - val_auc: 0.6140\n",
      "Epoch 153/200\n",
      "77/77 - 14s - loss: 0.6160 - accuracy: 0.7172 - val_binary_crossentropy: 0.6160 - auc: 0.7855 - val_loss: 0.6860 - val_accuracy: 0.5143 - val_val_binary_crossentropy: 0.6860 - val_auc: 0.6169\n",
      "Epoch 154/200\n",
      "77/77 - 14s - loss: 0.6160 - accuracy: 0.7172 - val_binary_crossentropy: 0.6160 - auc: 0.7855 - val_loss: 0.6859 - val_accuracy: 0.5134 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.6167\n",
      "Epoch 155/200\n",
      "77/77 - 14s - loss: 0.6160 - accuracy: 0.7172 - val_binary_crossentropy: 0.6160 - auc: 0.7857 - val_loss: 0.6860 - val_accuracy: 0.5157 - val_val_binary_crossentropy: 0.6860 - val_auc: 0.6169\n",
      "Epoch 156/200\n",
      "77/77 - 14s - loss: 0.6159 - accuracy: 0.7173 - val_binary_crossentropy: 0.6159 - auc: 0.7860 - val_loss: 0.6843 - val_accuracy: 0.5287 - val_val_binary_crossentropy: 0.6843 - val_auc: 0.6214\n",
      "Epoch 157/200\n",
      "77/77 - 14s - loss: 0.6159 - accuracy: 0.7174 - val_binary_crossentropy: 0.6159 - auc: 0.7859 - val_loss: 0.6862 - val_accuracy: 0.5117 - val_val_binary_crossentropy: 0.6862 - val_auc: 0.6175\n",
      "Epoch 158/200\n",
      "77/77 - 14s - loss: 0.6158 - accuracy: 0.7178 - val_binary_crossentropy: 0.6158 - auc: 0.7862 - val_loss: 0.6852 - val_accuracy: 0.5219 - val_val_binary_crossentropy: 0.6852 - val_auc: 0.6199\n",
      "Epoch 159/200\n",
      "77/77 - 14s - loss: 0.6158 - accuracy: 0.7176 - val_binary_crossentropy: 0.6158 - auc: 0.7861 - val_loss: 0.6860 - val_accuracy: 0.5163 - val_val_binary_crossentropy: 0.6860 - val_auc: 0.6176\n",
      "Epoch 160/200\n",
      "77/77 - 14s - loss: 0.6157 - accuracy: 0.7179 - val_binary_crossentropy: 0.6157 - auc: 0.7863 - val_loss: 0.6855 - val_accuracy: 0.5215 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6185\n",
      "Epoch 161/200\n",
      "77/77 - 14s - loss: 0.6157 - accuracy: 0.7181 - val_binary_crossentropy: 0.6157 - auc: 0.7865 - val_loss: 0.6858 - val_accuracy: 0.5171 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.6182\n",
      "Epoch 162/200\n",
      "77/77 - 14s - loss: 0.6156 - accuracy: 0.7182 - val_binary_crossentropy: 0.6156 - auc: 0.7867 - val_loss: 0.6856 - val_accuracy: 0.5218 - val_val_binary_crossentropy: 0.6856 - val_auc: 0.6187\n",
      "Epoch 163/200\n",
      "77/77 - 14s - loss: 0.6156 - accuracy: 0.7183 - val_binary_crossentropy: 0.6156 - auc: 0.7867 - val_loss: 0.6861 - val_accuracy: 0.5160 - val_val_binary_crossentropy: 0.6861 - val_auc: 0.6177\n",
      "Epoch 164/200\n",
      "77/77 - 14s - loss: 0.6155 - accuracy: 0.7183 - val_binary_crossentropy: 0.6155 - auc: 0.7867 - val_loss: 0.6850 - val_accuracy: 0.5231 - val_val_binary_crossentropy: 0.6850 - val_auc: 0.6200\n",
      "Epoch 165/200\n",
      "77/77 - 14s - loss: 0.6155 - accuracy: 0.7186 - val_binary_crossentropy: 0.6155 - auc: 0.7868 - val_loss: 0.6861 - val_accuracy: 0.5174 - val_val_binary_crossentropy: 0.6861 - val_auc: 0.6173\n",
      "Epoch 166/200\n",
      "77/77 - 14s - loss: 0.6154 - accuracy: 0.7186 - val_binary_crossentropy: 0.6154 - auc: 0.7869 - val_loss: 0.6850 - val_accuracy: 0.5243 - val_val_binary_crossentropy: 0.6851 - val_auc: 0.6204\n",
      "Epoch 167/200\n",
      "77/77 - 14s - loss: 0.6154 - accuracy: 0.7185 - val_binary_crossentropy: 0.6154 - auc: 0.7871 - val_loss: 0.6859 - val_accuracy: 0.5209 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.6190\n",
      "Epoch 168/200\n",
      "77/77 - 14s - loss: 0.6153 - accuracy: 0.7188 - val_binary_crossentropy: 0.6153 - auc: 0.7874 - val_loss: 0.6849 - val_accuracy: 0.5265 - val_val_binary_crossentropy: 0.6850 - val_auc: 0.6207\n",
      "Epoch 169/200\n",
      "77/77 - 14s - loss: 0.6153 - accuracy: 0.7189 - val_binary_crossentropy: 0.6153 - auc: 0.7873 - val_loss: 0.6859 - val_accuracy: 0.5220 - val_val_binary_crossentropy: 0.6859 - val_auc: 0.6197\n",
      "Epoch 170/200\n",
      "77/77 - 14s - loss: 0.6153 - accuracy: 0.7189 - val_binary_crossentropy: 0.6153 - auc: 0.7874 - val_loss: 0.6851 - val_accuracy: 0.5292 - val_val_binary_crossentropy: 0.6852 - val_auc: 0.6211\n",
      "Epoch 171/200\n",
      "77/77 - 14s - loss: 0.6152 - accuracy: 0.7191 - val_binary_crossentropy: 0.6152 - auc: 0.7876 - val_loss: 0.6855 - val_accuracy: 0.5212 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6200\n",
      "Epoch 172/200\n",
      "77/77 - 14s - loss: 0.6152 - accuracy: 0.7190 - val_binary_crossentropy: 0.6152 - auc: 0.7876 - val_loss: 0.6852 - val_accuracy: 0.5264 - val_val_binary_crossentropy: 0.6852 - val_auc: 0.6204\n",
      "Epoch 173/200\n",
      "77/77 - 14s - loss: 0.6151 - accuracy: 0.7192 - val_binary_crossentropy: 0.6151 - auc: 0.7877 - val_loss: 0.6852 - val_accuracy: 0.5281 - val_val_binary_crossentropy: 0.6853 - val_auc: 0.6207\n",
      "Epoch 174/200\n",
      "77/77 - 14s - loss: 0.6151 - accuracy: 0.7194 - val_binary_crossentropy: 0.6151 - auc: 0.7879 - val_loss: 0.6852 - val_accuracy: 0.5293 - val_val_binary_crossentropy: 0.6852 - val_auc: 0.6207\n",
      "Epoch 175/200\n",
      "77/77 - 14s - loss: 0.6151 - accuracy: 0.7193 - val_binary_crossentropy: 0.6151 - auc: 0.7879 - val_loss: 0.6854 - val_accuracy: 0.5256 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6200\n",
      "Epoch 176/200\n",
      "77/77 - 14s - loss: 0.6150 - accuracy: 0.7198 - val_binary_crossentropy: 0.6150 - auc: 0.7882 - val_loss: 0.6846 - val_accuracy: 0.5374 - val_val_binary_crossentropy: 0.6847 - val_auc: 0.6216\n",
      "Epoch 177/200\n",
      "77/77 - 14s - loss: 0.6150 - accuracy: 0.7196 - val_binary_crossentropy: 0.6150 - auc: 0.7881 - val_loss: 0.6856 - val_accuracy: 0.5289 - val_val_binary_crossentropy: 0.6856 - val_auc: 0.6195\n",
      "Epoch 178/200\n",
      "77/77 - 14s - loss: 0.6150 - accuracy: 0.7197 - val_binary_crossentropy: 0.6150 - auc: 0.7881 - val_loss: 0.6860 - val_accuracy: 0.5237 - val_val_binary_crossentropy: 0.6860 - val_auc: 0.6187\n",
      "Epoch 179/200\n",
      "77/77 - 14s - loss: 0.6149 - accuracy: 0.7199 - val_binary_crossentropy: 0.6149 - auc: 0.7884 - val_loss: 0.6853 - val_accuracy: 0.5287 - val_val_binary_crossentropy: 0.6853 - val_auc: 0.6203\n",
      "Epoch 180/200\n",
      "77/77 - 14s - loss: 0.6148 - accuracy: 0.7201 - val_binary_crossentropy: 0.6148 - auc: 0.7886 - val_loss: 0.6855 - val_accuracy: 0.5264 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6196\n",
      "Epoch 181/200\n",
      "77/77 - 14s - loss: 0.6148 - accuracy: 0.7202 - val_binary_crossentropy: 0.6148 - auc: 0.7887 - val_loss: 0.6863 - val_accuracy: 0.5200 - val_val_binary_crossentropy: 0.6864 - val_auc: 0.6168\n",
      "Epoch 182/200\n",
      "77/77 - 14s - loss: 0.6148 - accuracy: 0.7202 - val_binary_crossentropy: 0.6148 - auc: 0.7887 - val_loss: 0.6858 - val_accuracy: 0.5291 - val_val_binary_crossentropy: 0.6858 - val_auc: 0.6195\n",
      "Epoch 183/200\n",
      "77/77 - 14s - loss: 0.6147 - accuracy: 0.7203 - val_binary_crossentropy: 0.6147 - auc: 0.7888 - val_loss: 0.6856 - val_accuracy: 0.5265 - val_val_binary_crossentropy: 0.6857 - val_auc: 0.6188\n",
      "Epoch 184/200\n",
      "77/77 - 14s - loss: 0.6147 - accuracy: 0.7204 - val_binary_crossentropy: 0.6147 - auc: 0.7889 - val_loss: 0.6854 - val_accuracy: 0.5294 - val_val_binary_crossentropy: 0.6854 - val_auc: 0.6197\n",
      "Epoch 185/200\n",
      "77/77 - 14s - loss: 0.6147 - accuracy: 0.7203 - val_binary_crossentropy: 0.6147 - auc: 0.7890 - val_loss: 0.6859 - val_accuracy: 0.5280 - val_val_binary_crossentropy: 0.6860 - val_auc: 0.6188\n",
      "Epoch 186/200\n",
      "77/77 - 14s - loss: 0.6147 - accuracy: 0.7205 - val_binary_crossentropy: 0.6147 - auc: 0.7890 - val_loss: 0.6861 - val_accuracy: 0.5281 - val_val_binary_crossentropy: 0.6861 - val_auc: 0.6184\n",
      "Epoch 187/200\n",
      "77/77 - 14s - loss: 0.6146 - accuracy: 0.7207 - val_binary_crossentropy: 0.6146 - auc: 0.7891 - val_loss: 0.6857 - val_accuracy: 0.5278 - val_val_binary_crossentropy: 0.6858 - val_auc: 0.6192\n",
      "Epoch 188/200\n",
      "77/77 - 14s - loss: 0.6145 - accuracy: 0.7207 - val_binary_crossentropy: 0.6145 - auc: 0.7892 - val_loss: 0.6855 - val_accuracy: 0.5270 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6200\n",
      "Epoch 189/200\n",
      "77/77 - 14s - loss: 0.6145 - accuracy: 0.7209 - val_binary_crossentropy: 0.6145 - auc: 0.7893 - val_loss: 0.6850 - val_accuracy: 0.5345 - val_val_binary_crossentropy: 0.6850 - val_auc: 0.6212\n",
      "Epoch 190/200\n",
      "77/77 - 14s - loss: 0.6145 - accuracy: 0.7208 - val_binary_crossentropy: 0.6145 - auc: 0.7892 - val_loss: 0.6858 - val_accuracy: 0.5299 - val_val_binary_crossentropy: 0.6858 - val_auc: 0.6192\n",
      "Epoch 191/200\n",
      "77/77 - 14s - loss: 0.6145 - accuracy: 0.7209 - val_binary_crossentropy: 0.6145 - auc: 0.7893 - val_loss: 0.6855 - val_accuracy: 0.5310 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6196\n",
      "Epoch 192/200\n",
      "77/77 - 14s - loss: 0.6144 - accuracy: 0.7211 - val_binary_crossentropy: 0.6144 - auc: 0.7895 - val_loss: 0.6847 - val_accuracy: 0.5400 - val_val_binary_crossentropy: 0.6847 - val_auc: 0.6212\n",
      "Epoch 193/200\n",
      "77/77 - 14s - loss: 0.6144 - accuracy: 0.7211 - val_binary_crossentropy: 0.6144 - auc: 0.7896 - val_loss: 0.6852 - val_accuracy: 0.5387 - val_val_binary_crossentropy: 0.6852 - val_auc: 0.6204\n",
      "Epoch 194/200\n",
      "77/77 - 14s - loss: 0.6144 - accuracy: 0.7211 - val_binary_crossentropy: 0.6144 - auc: 0.7896 - val_loss: 0.6846 - val_accuracy: 0.5440 - val_val_binary_crossentropy: 0.6847 - val_auc: 0.6217\n",
      "Epoch 195/200\n",
      "77/77 - 14s - loss: 0.6143 - accuracy: 0.7211 - val_binary_crossentropy: 0.6143 - auc: 0.7897 - val_loss: 0.6850 - val_accuracy: 0.5335 - val_val_binary_crossentropy: 0.6851 - val_auc: 0.6205\n",
      "Epoch 196/200\n",
      "77/77 - 14s - loss: 0.6143 - accuracy: 0.7213 - val_binary_crossentropy: 0.6143 - auc: 0.7897 - val_loss: 0.6855 - val_accuracy: 0.5303 - val_val_binary_crossentropy: 0.6855 - val_auc: 0.6200\n",
      "Epoch 197/200\n",
      "77/77 - 14s - loss: 0.6143 - accuracy: 0.7212 - val_binary_crossentropy: 0.6143 - auc: 0.7899 - val_loss: 0.6864 - val_accuracy: 0.5276 - val_val_binary_crossentropy: 0.6864 - val_auc: 0.6173\n",
      "Epoch 198/200\n",
      "77/77 - 14s - loss: 0.6142 - accuracy: 0.7215 - val_binary_crossentropy: 0.6142 - auc: 0.7899 - val_loss: 0.6847 - val_accuracy: 0.5467 - val_val_binary_crossentropy: 0.6847 - val_auc: 0.6215\n",
      "Epoch 199/200\n",
      "77/77 - 14s - loss: 0.6142 - accuracy: 0.7216 - val_binary_crossentropy: 0.6142 - auc: 0.7901 - val_loss: 0.6860 - val_accuracy: 0.5274 - val_val_binary_crossentropy: 0.6860 - val_auc: 0.6179\n",
      "Epoch 200/200\n",
      "77/77 - 14s - loss: 0.6142 - accuracy: 0.7216 - val_binary_crossentropy: 0.6142 - auc: 0.7901 - val_loss: 0.6860 - val_accuracy: 0.5309 - val_val_binary_crossentropy: 0.6861 - val_auc: 0.6179\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.layers import LeakyReLU\n",
    "#import pandas as pd\n",
    "#import io\n",
    "#import os\n",
    "#import requests\n",
    "#import numpy as np\n",
    "#from sklearn import metrics\n",
    "#from tensorflow.keras import optimizers\n",
    "\n",
    "#Parms needed for case study  \n",
    "\n",
    "#We selected a five-layer neural network with 300 hidden units in each layer,\n",
    "#a learning rate of 0.05, and a weight decay coefficient of 1 × 10−5.\n",
    "# Hidden layer have tanh activation function\n",
    "#Gradient computations were made on mini-batches of size 100\n",
    "#The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10^−6,\n",
    "\n",
    "def define_predictor(n_input):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(300, kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), input_dim = n_input, activation='tanh')) # Hidden 1\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 2\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 3\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 4\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 5\n",
    "    model.add(tf.keras.layers.Dense(1,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None),activation='sigmoid')) # Output #1.2\n",
    "    #model.add(tf.keras.layers.Dense(1,activation='softmax')) # Output\n",
    "    #sgd = tf.keras.optimizers.SGD(lr=.05, decay = 1.0000002, momentum=0.99)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=get_optimizer() , \n",
    "                  metrics=['accuracy', \n",
    "                            tf.keras.losses.BinaryCrossentropy(\n",
    "                            from_logits=True, name='val_binary_crossentropy'), \n",
    "                            tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "    \n",
    "#setup scaler\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "dfx\n",
    "X = dfx.loc[:, 1:28]\n",
    "#X = dfx[features].copy()\n",
    "Y = dfx[0].copy()\n",
    "y = Y.values\n",
    "\n",
    "# the cv=cvx parameter sets the grid search to split the training and testing data 10 times. \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, test_size = 0.5, random_state = 101)\n",
    "#N_VALIDATION = int(1e3)\n",
    "N_TRAIN = len(X_train)\n",
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 100000\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
    "\n",
    "Att_model = define_predictor(X_train.shape[1])\n",
    "history = Att_model.fit(scaler.fit_transform(X_train),\n",
    "                        np.array(y_train),\n",
    "                        callbacks=get_callbacks(),\n",
    "                        verbose=2, \n",
    "#                        steps_per_epoch = 5, \n",
    "                        epochs=200, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data =  ( X_validate, y_validate ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 40s 2ms/step - loss: 0.6105 - accuracy: 0.7305 - val_binary_crossentropy: 0.6105 - auc: 0.7979\n"
     ]
    }
   ],
   "source": [
    "results = Att_model.evaluate(scaler.fit_transform(X_test),np.array(y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6105056405067444,\n",
       " 0.7304636240005493,\n",
       " 0.6105080842971802,\n",
       " 0.7978678941726685]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Att_model.predict(scaler.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9962246e-01],\n",
       "       [3.3872193e-01],\n",
       "       [2.7056452e-05],\n",
       "       ...,\n",
       "       [1.2375115e-02],\n",
       "       [5.8829914e-09],\n",
       "       [3.4241225e-03]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.338722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.999622\n",
       "1  0.338722\n",
       "2  0.000027\n",
       "3  0.000331\n",
       "4  0.098616"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame(predictions)\n",
    "preddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddf['round'] = preddf.round(preddf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.996225e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.387219e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.705645e-05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.314451e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.861650e-02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649995</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649996</th>\n",
       "      <td>9.999961e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649997</th>\n",
       "      <td>1.237512e-02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649998</th>\n",
       "      <td>5.882991e-09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649999</th>\n",
       "      <td>3.424122e-03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0  round\n",
       "0        9.996225e-01    1.0\n",
       "1        3.387219e-01    0.0\n",
       "2        2.705645e-05    0.0\n",
       "3        3.314451e-04    0.0\n",
       "4        9.861650e-02    0.0\n",
       "...               ...    ...\n",
       "1649995  1.000000e+00    1.0\n",
       "1649996  9.999961e-01    1.0\n",
       "1649997  1.237512e-02    0.0\n",
       "1649998  5.882991e-09    0.0\n",
       "1649999  3.424122e-03    0.0\n",
       "\n",
       "[1650000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "yresultsround = preddf['round'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yresultsround[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1650000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybinary = label_binarize(y_test, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = label_binarize(yresultsround, classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Power Used to Train and Test Replicated Model\n",
    "\n",
    "We used an AWS Deep Learning AMI. The AMIs provide machine learning practitioners and researchers with the infrastructure and tools to accelerate deep learning in the cloud, at any scale. You can quickly launch Amazon EC2 instances(servers) pre-installed with popular deep learning frameworks and interfaces such as TensorFlow, PyTorch, Apache MXNet, Chainer, Gluon, Horovod, and Keras to train sophisticated, custom AI models, experiment with new algorithms, or to learn new skills and techniques.  (https://aws.amazon.com/machine-learning/amis/)\n",
    "\n",
    "The server we utlized was an Amazon EC2 G3. The G3 instances are the latest generation of Amazon EC2 GPU graphics instances that deliver a powerful combination of CPU, host memory, and GPU capacity. G3 instances are ideal for graphics-intensive applications such as 3D visualizations, mid to high-end virtual workstations, virtual application software, 3D rendering, application streaming, video encoding, gaming, and other server-side graphics workloads.  \n",
    "\n",
    "Our specific instance  has 32 vCPUs based on custom 2.7 GHz Intel Xeon E5 2686 v4 processors and 244 GiB of DRAM host memory. Backed by2 NVIDIA Tesla M60 GPUs, with each GPU delivering up to 2,048 parallel processing cores and 8 GiB of GPU memory. (https://aws.amazon.com/ec2/instance-types/g3/)\n",
    "\n",
    "| Name | GPUs | vCPU | Memory (GiB) | GPU Memory (GiB) |\n",
    "|-|-|-|-|-|\n",
    "| g3.8xlarge | 2 | 32 | 244 | 16 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, Baldi and team produced Receiver Operating Characteristic (ROC) curves to illustrate performance of the models, using the area under the ROC curve (AUC) as the measurement metric. This metric is directly related to classiciation accuracy and correlated to other metrics. Thus is a solid choice for comparison in this scenario. \n",
    "\n",
    "With AUC, larger values indicate higher classification accuracy, so our goal is to obtain higher values. From the paper, it appears they achieved an **AUC score of 0.88** for both the low-level feature set and the complete feature set when utilizing the dropout training algorithm (stochastically dropping neurons in the top hidden layer with 50% probablity during training). (We are most interested in the complete feature set as we used all the features in our comparative analysis). For our results, we were able to obtain an **AUC score of 0.7979 on the training data** and an **AUC score of 0.7373 on test data**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_classes = ybinary.shape[1]\n",
    "#X1_train, X1_test, y1_train, y1_test\n",
    "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(ybinary[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ybinary.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "#Plot of a ROC curve for a specific class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.16204025, 1.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7366014061868519, 'micro': 0.7366014061868519}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic Higgs')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               8700      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 370,201\n",
      "Trainable params: 370,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Att_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6971189379692078,\n",
       "  0.6730819940567017,\n",
       "  0.6684746742248535,\n",
       "  0.6671062707901001,\n",
       "  0.666298508644104,\n",
       "  0.6656732559204102,\n",
       "  0.6652130484580994,\n",
       "  0.6647259593009949,\n",
       "  0.6642757654190063,\n",
       "  0.6638593077659607,\n",
       "  0.6634681820869446,\n",
       "  0.6630859375,\n",
       "  0.6625930070877075,\n",
       "  0.662186861038208,\n",
       "  0.6616360545158386,\n",
       "  0.6610156893730164,\n",
       "  0.6603534817695618,\n",
       "  0.6593383550643921,\n",
       "  0.6580010652542114,\n",
       "  0.6563535332679749,\n",
       "  0.654678225517273,\n",
       "  0.6531782746315002,\n",
       "  0.6519178748130798,\n",
       "  0.6508277058601379,\n",
       "  0.6496748328208923,\n",
       "  0.6484094858169556,\n",
       "  0.6468399167060852,\n",
       "  0.6449366211891174,\n",
       "  0.6428344249725342,\n",
       "  0.6408801674842834,\n",
       "  0.6393832564353943,\n",
       "  0.6382941007614136,\n",
       "  0.6374547481536865,\n",
       "  0.6366705298423767,\n",
       "  0.6360081434249878,\n",
       "  0.6353750228881836,\n",
       "  0.6347832083702087,\n",
       "  0.6342276930809021,\n",
       "  0.6336597800254822,\n",
       "  0.633163332939148,\n",
       "  0.6326498985290527,\n",
       "  0.6321606040000916,\n",
       "  0.6316586136817932,\n",
       "  0.6311439275741577,\n",
       "  0.6307176351547241,\n",
       "  0.6302891969680786,\n",
       "  0.6299024224281311,\n",
       "  0.6295543909072876,\n",
       "  0.6291630864143372,\n",
       "  0.6288115382194519,\n",
       "  0.6285091042518616,\n",
       "  0.6281789541244507,\n",
       "  0.6279516816139221,\n",
       "  0.6275691986083984,\n",
       "  0.627332866191864,\n",
       "  0.627059280872345,\n",
       "  0.6268214583396912,\n",
       "  0.6265002489089966,\n",
       "  0.6262749433517456,\n",
       "  0.6260592341423035,\n",
       "  0.625834584236145,\n",
       "  0.6256322264671326,\n",
       "  0.6253266930580139,\n",
       "  0.6251846551895142,\n",
       "  0.6249701380729675,\n",
       "  0.6247748136520386,\n",
       "  0.624646008014679,\n",
       "  0.6244521141052246,\n",
       "  0.6242634654045105,\n",
       "  0.6240978240966797,\n",
       "  0.6239177584648132,\n",
       "  0.6237286925315857,\n",
       "  0.6236218214035034,\n",
       "  0.6234667301177979,\n",
       "  0.623298704624176,\n",
       "  0.6231255531311035,\n",
       "  0.6229870915412903,\n",
       "  0.6228405237197876,\n",
       "  0.6227377653121948,\n",
       "  0.622611939907074,\n",
       "  0.6224650144577026,\n",
       "  0.6222832202911377,\n",
       "  0.6221920847892761,\n",
       "  0.6220117211341858,\n",
       "  0.6219077706336975,\n",
       "  0.6217692494392395,\n",
       "  0.6216374039649963,\n",
       "  0.6215200424194336,\n",
       "  0.6213387846946716,\n",
       "  0.6212570071220398,\n",
       "  0.6211410760879517,\n",
       "  0.6210644841194153,\n",
       "  0.6208816170692444,\n",
       "  0.6207984089851379,\n",
       "  0.6206817626953125,\n",
       "  0.6205238699913025,\n",
       "  0.620418906211853,\n",
       "  0.6203164458274841,\n",
       "  0.6201866865158081,\n",
       "  0.6200892925262451,\n",
       "  0.6200069189071655,\n",
       "  0.6198576092720032,\n",
       "  0.6197388172149658,\n",
       "  0.6196213960647583,\n",
       "  0.6195181608200073,\n",
       "  0.6194971799850464,\n",
       "  0.6193094253540039,\n",
       "  0.6191975474357605,\n",
       "  0.6190769076347351,\n",
       "  0.6190064549446106,\n",
       "  0.6189345717430115,\n",
       "  0.6188346147537231,\n",
       "  0.6187831163406372,\n",
       "  0.6186848282814026,\n",
       "  0.6185473799705505,\n",
       "  0.6184923052787781,\n",
       "  0.618413507938385,\n",
       "  0.6183468103408813,\n",
       "  0.6182100772857666,\n",
       "  0.6181759238243103,\n",
       "  0.618083119392395,\n",
       "  0.6179468631744385,\n",
       "  0.6179324984550476,\n",
       "  0.6178298592567444,\n",
       "  0.6177652478218079,\n",
       "  0.617666482925415,\n",
       "  0.6176308393478394,\n",
       "  0.6175630688667297,\n",
       "  0.6174719929695129,\n",
       "  0.6173866987228394,\n",
       "  0.6173478960990906,\n",
       "  0.617236316204071,\n",
       "  0.6172033548355103,\n",
       "  0.6171084642410278,\n",
       "  0.6170734167098999,\n",
       "  0.6169916391372681,\n",
       "  0.6169541478157043,\n",
       "  0.6168755292892456,\n",
       "  0.6168034672737122,\n",
       "  0.6167665719985962,\n",
       "  0.6167429089546204,\n",
       "  0.6166449785232544,\n",
       "  0.6165590286254883,\n",
       "  0.6165114641189575,\n",
       "  0.6164882183074951,\n",
       "  0.6164019107818604,\n",
       "  0.6163551211357117,\n",
       "  0.6162857413291931,\n",
       "  0.6162847280502319,\n",
       "  0.6162225008010864,\n",
       "  0.6161538362503052,\n",
       "  0.6160904169082642,\n",
       "  0.6159895062446594,\n",
       "  0.6159617304801941,\n",
       "  0.6159736514091492,\n",
       "  0.6158943176269531,\n",
       "  0.615857720375061,\n",
       "  0.6157600283622742,\n",
       "  0.6157701015472412,\n",
       "  0.6156829595565796,\n",
       "  0.6156500577926636,\n",
       "  0.6155930757522583,\n",
       "  0.615564227104187,\n",
       "  0.6154941320419312,\n",
       "  0.6154875755310059,\n",
       "  0.6154438257217407,\n",
       "  0.6153955459594727,\n",
       "  0.6153202056884766,\n",
       "  0.6152750253677368,\n",
       "  0.615278959274292,\n",
       "  0.6152026653289795,\n",
       "  0.6152323484420776,\n",
       "  0.6151270866394043,\n",
       "  0.6150878071784973,\n",
       "  0.6150980591773987,\n",
       "  0.6149642467498779,\n",
       "  0.6149810552597046,\n",
       "  0.6149503588676453,\n",
       "  0.6148749589920044,\n",
       "  0.6148059964179993,\n",
       "  0.6147952675819397,\n",
       "  0.6147621870040894,\n",
       "  0.6147183179855347,\n",
       "  0.6146699786186218,\n",
       "  0.614680290222168,\n",
       "  0.6146532893180847,\n",
       "  0.6146020889282227,\n",
       "  0.6145492196083069,\n",
       "  0.614526629447937,\n",
       "  0.6145367622375488,\n",
       "  0.6144739389419556,\n",
       "  0.6144088506698608,\n",
       "  0.614379346370697,\n",
       "  0.6144052147865295,\n",
       "  0.6143428683280945,\n",
       "  0.6143494248390198,\n",
       "  0.6143319606781006,\n",
       "  0.6142409443855286,\n",
       "  0.6142151951789856,\n",
       "  0.6141892075538635],\n",
       " 'accuracy': [0.4947968125343323,\n",
       "  0.5516712665557861,\n",
       "  0.5698255300521851,\n",
       "  0.5758996605873108,\n",
       "  0.579478919506073,\n",
       "  0.5821706056594849,\n",
       "  0.5838346481323242,\n",
       "  0.5859668254852295,\n",
       "  0.5877459049224854,\n",
       "  0.5898017883300781,\n",
       "  0.5913030505180359,\n",
       "  0.5928162932395935,\n",
       "  0.5944556593894958,\n",
       "  0.5963100790977478,\n",
       "  0.5984119176864624,\n",
       "  0.6006395816802979,\n",
       "  0.6031977534294128,\n",
       "  0.6066477298736572,\n",
       "  0.6112387776374817,\n",
       "  0.6169297099113464,\n",
       "  0.6225000619888306,\n",
       "  0.6269639730453491,\n",
       "  0.6307068467140198,\n",
       "  0.6338152885437012,\n",
       "  0.6367931962013245,\n",
       "  0.6402868628501892,\n",
       "  0.643815815448761,\n",
       "  0.648123562335968,\n",
       "  0.6527629494667053,\n",
       "  0.6568551659584045,\n",
       "  0.6601806282997131,\n",
       "  0.6623695492744446,\n",
       "  0.6640669703483582,\n",
       "  0.6655392050743103,\n",
       "  0.6670507192611694,\n",
       "  0.6683695912361145,\n",
       "  0.6697030663490295,\n",
       "  0.6708590388298035,\n",
       "  0.6723009943962097,\n",
       "  0.6732377409934998,\n",
       "  0.6746293306350708,\n",
       "  0.6756613850593567,\n",
       "  0.6769902110099792,\n",
       "  0.6782116293907166,\n",
       "  0.6793785095214844,\n",
       "  0.68040931224823,\n",
       "  0.6810694336891174,\n",
       "  0.6820360422134399,\n",
       "  0.6830174922943115,\n",
       "  0.6840020418167114,\n",
       "  0.6846110224723816,\n",
       "  0.6852821707725525,\n",
       "  0.6858599781990051,\n",
       "  0.6870700716972351,\n",
       "  0.6874735951423645,\n",
       "  0.6881417632102966,\n",
       "  0.6887753009796143,\n",
       "  0.6894660592079163,\n",
       "  0.6900846362113953,\n",
       "  0.690657377243042,\n",
       "  0.6914685368537903,\n",
       "  0.6917339563369751,\n",
       "  0.6925108432769775,\n",
       "  0.6930608749389648,\n",
       "  0.6933823227882385,\n",
       "  0.6938387751579285,\n",
       "  0.6943855285644531,\n",
       "  0.6949692964553833,\n",
       "  0.6954641342163086,\n",
       "  0.695822536945343,\n",
       "  0.6963075995445251,\n",
       "  0.6966812610626221,\n",
       "  0.6968845129013062,\n",
       "  0.6973647475242615,\n",
       "  0.6978641152381897,\n",
       "  0.6982926726341248,\n",
       "  0.69866943359375,\n",
       "  0.6990278959274292,\n",
       "  0.6993181705474854,\n",
       "  0.6996923089027405,\n",
       "  0.7000043988227844,\n",
       "  0.7004803419113159,\n",
       "  0.7009561657905579,\n",
       "  0.7012916207313538,\n",
       "  0.7016168236732483,\n",
       "  0.7019944787025452,\n",
       "  0.7024540901184082,\n",
       "  0.7026849985122681,\n",
       "  0.7030279040336609,\n",
       "  0.7032137513160706,\n",
       "  0.7036690711975098,\n",
       "  0.7037856578826904,\n",
       "  0.704234778881073,\n",
       "  0.7046472430229187,\n",
       "  0.704889714717865,\n",
       "  0.7051869630813599,\n",
       "  0.705538272857666,\n",
       "  0.7058777809143066,\n",
       "  0.7062045335769653,\n",
       "  0.7066271305084229,\n",
       "  0.7066776156425476,\n",
       "  0.7069897055625916,\n",
       "  0.7073314189910889,\n",
       "  0.7078605890274048,\n",
       "  0.7079392075538635,\n",
       "  0.7079774737358093,\n",
       "  0.7085241079330444,\n",
       "  0.7089083790779114,\n",
       "  0.7092373371124268,\n",
       "  0.7092503309249878,\n",
       "  0.7095163464546204,\n",
       "  0.7098138332366943,\n",
       "  0.7098320126533508,\n",
       "  0.7102527022361755,\n",
       "  0.7105810046195984,\n",
       "  0.7109150290489197,\n",
       "  0.7108590602874756,\n",
       "  0.7110772132873535,\n",
       "  0.7115331888198853,\n",
       "  0.7115808725357056,\n",
       "  0.7117840051651001,\n",
       "  0.7120350003242493,\n",
       "  0.7121942639350891,\n",
       "  0.7124086618423462,\n",
       "  0.7127352952957153,\n",
       "  0.7129530906677246,\n",
       "  0.7128757834434509,\n",
       "  0.7132104635238647,\n",
       "  0.7133926749229431,\n",
       "  0.7136453986167908,\n",
       "  0.7137638330459595,\n",
       "  0.7139027118682861,\n",
       "  0.7141655683517456,\n",
       "  0.7142794728279114,\n",
       "  0.714475691318512,\n",
       "  0.714622437953949,\n",
       "  0.7147974967956543,\n",
       "  0.7148503661155701,\n",
       "  0.7150712609291077,\n",
       "  0.7150565981864929,\n",
       "  0.7152948975563049,\n",
       "  0.7155711054801941,\n",
       "  0.7157834768295288,\n",
       "  0.7158776521682739,\n",
       "  0.7159540057182312,\n",
       "  0.7161648869514465,\n",
       "  0.7162755727767944,\n",
       "  0.716378390789032,\n",
       "  0.7166024446487427,\n",
       "  0.716556966304779,\n",
       "  0.7167589068412781,\n",
       "  0.7167686820030212,\n",
       "  0.7172455191612244,\n",
       "  0.717207133769989,\n",
       "  0.7171865701675415,\n",
       "  0.7172853946685791,\n",
       "  0.7174112796783447,\n",
       "  0.7178029417991638,\n",
       "  0.71757972240448,\n",
       "  0.7178968191146851,\n",
       "  0.7180541157722473,\n",
       "  0.7181578874588013,\n",
       "  0.7182924151420593,\n",
       "  0.7183222770690918,\n",
       "  0.7185768485069275,\n",
       "  0.7186028361320496,\n",
       "  0.7184929251670837,\n",
       "  0.7188214063644409,\n",
       "  0.7189152836799622,\n",
       "  0.7189246416091919,\n",
       "  0.7191206216812134,\n",
       "  0.7190110087394714,\n",
       "  0.7191587686538696,\n",
       "  0.7193648815155029,\n",
       "  0.7192996740341187,\n",
       "  0.7197566032409668,\n",
       "  0.7195801138877869,\n",
       "  0.7196877598762512,\n",
       "  0.7198817729949951,\n",
       "  0.72008216381073,\n",
       "  0.7202273607254028,\n",
       "  0.7202374935150146,\n",
       "  0.7203107476234436,\n",
       "  0.7204383015632629,\n",
       "  0.7202557921409607,\n",
       "  0.7204880118370056,\n",
       "  0.7206732034683228,\n",
       "  0.7207397222518921,\n",
       "  0.7208926677703857,\n",
       "  0.7208057045936584,\n",
       "  0.7208706140518188,\n",
       "  0.7210556864738464,\n",
       "  0.7210647463798523,\n",
       "  0.7210831046104431,\n",
       "  0.7211304903030396,\n",
       "  0.7212789058685303,\n",
       "  0.7211945056915283,\n",
       "  0.721473217010498,\n",
       "  0.7216059565544128,\n",
       "  0.7215611338615417],\n",
       " 'val_binary_crossentropy': [0.6971190571784973,\n",
       "  0.6730819940567017,\n",
       "  0.6684747338294983,\n",
       "  0.6671063899993896,\n",
       "  0.6662982702255249,\n",
       "  0.6656732559204102,\n",
       "  0.6652131080627441,\n",
       "  0.6647261381149292,\n",
       "  0.6642760038375854,\n",
       "  0.6638593077659607,\n",
       "  0.6634681820869446,\n",
       "  0.6630860567092896,\n",
       "  0.6625931262969971,\n",
       "  0.6621868014335632,\n",
       "  0.6616359353065491,\n",
       "  0.6610155701637268,\n",
       "  0.660353422164917,\n",
       "  0.6593382358551025,\n",
       "  0.6580010056495667,\n",
       "  0.6563537120819092,\n",
       "  0.6546783447265625,\n",
       "  0.653178334236145,\n",
       "  0.6519179940223694,\n",
       "  0.6508277058601379,\n",
       "  0.6496745944023132,\n",
       "  0.6484093070030212,\n",
       "  0.6468397974967957,\n",
       "  0.6449369192123413,\n",
       "  0.642834484577179,\n",
       "  0.6408800482749939,\n",
       "  0.6393833756446838,\n",
       "  0.6382941007614136,\n",
       "  0.6374548077583313,\n",
       "  0.6366705298423767,\n",
       "  0.6360081434249878,\n",
       "  0.6353749632835388,\n",
       "  0.6347832083702087,\n",
       "  0.6342276334762573,\n",
       "  0.6336597800254822,\n",
       "  0.6331634521484375,\n",
       "  0.6326499581336975,\n",
       "  0.6321607232093811,\n",
       "  0.631658673286438,\n",
       "  0.6311438679695129,\n",
       "  0.6307176947593689,\n",
       "  0.6302891969680786,\n",
       "  0.6299023628234863,\n",
       "  0.6295543313026428,\n",
       "  0.6291631460189819,\n",
       "  0.6288114786148071,\n",
       "  0.6285090446472168,\n",
       "  0.6281790733337402,\n",
       "  0.6279517412185669,\n",
       "  0.6275691390037537,\n",
       "  0.6273327469825745,\n",
       "  0.627059280872345,\n",
       "  0.6268213391304016,\n",
       "  0.6265001893043518,\n",
       "  0.6262747645378113,\n",
       "  0.6260592341423035,\n",
       "  0.6258345246315002,\n",
       "  0.6256322860717773,\n",
       "  0.6253266930580139,\n",
       "  0.6251847147941589,\n",
       "  0.6249702572822571,\n",
       "  0.6247747540473938,\n",
       "  0.6246459484100342,\n",
       "  0.6244519948959351,\n",
       "  0.6242635846138,\n",
       "  0.6240979433059692,\n",
       "  0.6239176392555237,\n",
       "  0.6237288117408752,\n",
       "  0.6236218810081482,\n",
       "  0.6234666109085083,\n",
       "  0.6232987642288208,\n",
       "  0.6231256127357483,\n",
       "  0.6229870319366455,\n",
       "  0.6228403449058533,\n",
       "  0.6227376461029053,\n",
       "  0.6226121187210083,\n",
       "  0.6224649548530579,\n",
       "  0.6222832798957825,\n",
       "  0.6221920847892761,\n",
       "  0.6220117211341858,\n",
       "  0.6219075918197632,\n",
       "  0.6217692494392395,\n",
       "  0.6216375231742859,\n",
       "  0.6215202212333679,\n",
       "  0.6213388442993164,\n",
       "  0.6212568283081055,\n",
       "  0.6211411952972412,\n",
       "  0.6210643649101257,\n",
       "  0.6208816766738892,\n",
       "  0.6207983493804932,\n",
       "  0.6206817626953125,\n",
       "  0.6205237507820129,\n",
       "  0.6204187870025635,\n",
       "  0.6203165054321289,\n",
       "  0.6201866269111633,\n",
       "  0.6200894117355347,\n",
       "  0.6200069785118103,\n",
       "  0.6198577880859375,\n",
       "  0.6197389960289001,\n",
       "  0.6196213960647583,\n",
       "  0.6195181608200073,\n",
       "  0.6194970011711121,\n",
       "  0.6193094849586487,\n",
       "  0.619197428226471,\n",
       "  0.6190767884254456,\n",
       "  0.6190064549446106,\n",
       "  0.6189346313476562,\n",
       "  0.6188346147537231,\n",
       "  0.6187830567359924,\n",
       "  0.6186848878860474,\n",
       "  0.6185474395751953,\n",
       "  0.6184924840927124,\n",
       "  0.6184136271476746,\n",
       "  0.6183469891548157,\n",
       "  0.618209958076477,\n",
       "  0.6181758642196655,\n",
       "  0.6180832982063293,\n",
       "  0.6179470419883728,\n",
       "  0.6179323792457581,\n",
       "  0.6178298592567444,\n",
       "  0.6177652478218079,\n",
       "  0.6176665425300598,\n",
       "  0.617630660533905,\n",
       "  0.6175632476806641,\n",
       "  0.6174718141555786,\n",
       "  0.6173868775367737,\n",
       "  0.6173478364944458,\n",
       "  0.6172363758087158,\n",
       "  0.617203414440155,\n",
       "  0.6171086430549622,\n",
       "  0.6170733571052551,\n",
       "  0.6169919371604919,\n",
       "  0.6169540882110596,\n",
       "  0.616875410079956,\n",
       "  0.616803765296936,\n",
       "  0.6167665123939514,\n",
       "  0.6167430281639099,\n",
       "  0.6166447401046753,\n",
       "  0.6165590882301331,\n",
       "  0.6165115237236023,\n",
       "  0.6164880990982056,\n",
       "  0.6164019703865051,\n",
       "  0.6163551211357117,\n",
       "  0.6162856221199036,\n",
       "  0.6162846684455872,\n",
       "  0.6162225604057312,\n",
       "  0.6161539554595947,\n",
       "  0.6160903573036194,\n",
       "  0.6159894466400146,\n",
       "  0.6159618496894836,\n",
       "  0.6159737706184387,\n",
       "  0.6158941984176636,\n",
       "  0.6158574223518372,\n",
       "  0.6157600283622742,\n",
       "  0.6157702803611755,\n",
       "  0.6156830191612244,\n",
       "  0.6156500577926636,\n",
       "  0.6155931353569031,\n",
       "  0.6155642867088318,\n",
       "  0.6154941916465759,\n",
       "  0.6154874563217163,\n",
       "  0.6154438257217407,\n",
       "  0.6153954267501831,\n",
       "  0.6153202652931213,\n",
       "  0.6152748465538025,\n",
       "  0.6152788996696472,\n",
       "  0.6152025461196899,\n",
       "  0.6152321696281433,\n",
       "  0.6151270270347595,\n",
       "  0.6150875091552734,\n",
       "  0.6150980591773987,\n",
       "  0.6149642467498779,\n",
       "  0.6149809956550598,\n",
       "  0.6149502992630005,\n",
       "  0.6148749589920044,\n",
       "  0.6148059368133545,\n",
       "  0.6147952675819397,\n",
       "  0.6147621870040894,\n",
       "  0.6147183775901794,\n",
       "  0.614669919013977,\n",
       "  0.6146802306175232,\n",
       "  0.6146531701087952,\n",
       "  0.6146019697189331,\n",
       "  0.614548921585083,\n",
       "  0.6145266890525818,\n",
       "  0.614536702632904,\n",
       "  0.614473819732666,\n",
       "  0.6144087910652161,\n",
       "  0.614379346370697,\n",
       "  0.6144052147865295,\n",
       "  0.6143430471420288,\n",
       "  0.6143496036529541,\n",
       "  0.6143321990966797,\n",
       "  0.614240825176239,\n",
       "  0.614215075969696,\n",
       "  0.6141892671585083],\n",
       " 'auc': [0.5677555203437805,\n",
       "  0.6371926069259644,\n",
       "  0.6514118909835815,\n",
       "  0.6555649638175964,\n",
       "  0.6581222414970398,\n",
       "  0.6603056192398071,\n",
       "  0.6624830961227417,\n",
       "  0.6642783284187317,\n",
       "  0.6662340760231018,\n",
       "  0.6680269241333008,\n",
       "  0.6695045232772827,\n",
       "  0.6711512207984924,\n",
       "  0.6728118658065796,\n",
       "  0.6744032502174377,\n",
       "  0.6763134002685547,\n",
       "  0.678363025188446,\n",
       "  0.680558979511261,\n",
       "  0.6837359666824341,\n",
       "  0.6877318024635315,\n",
       "  0.692358136177063,\n",
       "  0.6965491771697998,\n",
       "  0.6997110843658447,\n",
       "  0.7018585801124573,\n",
       "  0.7035253643989563,\n",
       "  0.7055718302726746,\n",
       "  0.708105206489563,\n",
       "  0.7115676403045654,\n",
       "  0.716097891330719,\n",
       "  0.7211074829101562,\n",
       "  0.7254777550697327,\n",
       "  0.7285118103027344,\n",
       "  0.7302037477493286,\n",
       "  0.7311158180236816,\n",
       "  0.7322319746017456,\n",
       "  0.7331863641738892,\n",
       "  0.7341248989105225,\n",
       "  0.7353853583335876,\n",
       "  0.7364518642425537,\n",
       "  0.7376519441604614,\n",
       "  0.7388603687286377,\n",
       "  0.7402624487876892,\n",
       "  0.7417193055152893,\n",
       "  0.7431339621543884,\n",
       "  0.7445870041847229,\n",
       "  0.745974600315094,\n",
       "  0.7472256422042847,\n",
       "  0.7481123805046082,\n",
       "  0.7492067813873291,\n",
       "  0.7503203749656677,\n",
       "  0.7512878179550171,\n",
       "  0.7520737648010254,\n",
       "  0.7529906630516052,\n",
       "  0.7536764144897461,\n",
       "  0.7546122670173645,\n",
       "  0.7552447319030762,\n",
       "  0.7559000253677368,\n",
       "  0.7566266655921936,\n",
       "  0.757361650466919,\n",
       "  0.7579841613769531,\n",
       "  0.7586541771888733,\n",
       "  0.7593849301338196,\n",
       "  0.7597330808639526,\n",
       "  0.7605271935462952,\n",
       "  0.7608332633972168,\n",
       "  0.7613667249679565,\n",
       "  0.7619825601577759,\n",
       "  0.7624790072441101,\n",
       "  0.7628370523452759,\n",
       "  0.7632454633712769,\n",
       "  0.7637125849723816,\n",
       "  0.764256477355957,\n",
       "  0.7648817896842957,\n",
       "  0.7650123834609985,\n",
       "  0.765505850315094,\n",
       "  0.7659745812416077,\n",
       "  0.7664581537246704,\n",
       "  0.7668890953063965,\n",
       "  0.7673269510269165,\n",
       "  0.7676101922988892,\n",
       "  0.7680225372314453,\n",
       "  0.7683906555175781,\n",
       "  0.7689281105995178,\n",
       "  0.7690805792808533,\n",
       "  0.7696875929832458,\n",
       "  0.7699429988861084,\n",
       "  0.7703654766082764,\n",
       "  0.7708718776702881,\n",
       "  0.7710907459259033,\n",
       "  0.7715370059013367,\n",
       "  0.7718141078948975,\n",
       "  0.7721540927886963,\n",
       "  0.7724547982215881,\n",
       "  0.772806704044342,\n",
       "  0.7731718420982361,\n",
       "  0.7735374569892883,\n",
       "  0.7738003730773926,\n",
       "  0.7742959260940552,\n",
       "  0.7745687365531921,\n",
       "  0.7749841809272766,\n",
       "  0.7751950025558472,\n",
       "  0.7756313681602478,\n",
       "  0.7758716344833374,\n",
       "  0.7762123942375183,\n",
       "  0.7764360904693604,\n",
       "  0.7767193913459778,\n",
       "  0.7770150303840637,\n",
       "  0.777391254901886,\n",
       "  0.777629017829895,\n",
       "  0.7779785990715027,\n",
       "  0.7781029343605042,\n",
       "  0.7784033417701721,\n",
       "  0.7786587476730347,\n",
       "  0.7785730361938477,\n",
       "  0.779009997844696,\n",
       "  0.779232919216156,\n",
       "  0.7795341610908508,\n",
       "  0.7795674204826355,\n",
       "  0.7796967625617981,\n",
       "  0.7800979614257812,\n",
       "  0.7803454399108887,\n",
       "  0.7804274559020996,\n",
       "  0.7807983160018921,\n",
       "  0.7809486985206604,\n",
       "  0.7810124158859253,\n",
       "  0.7812466621398926,\n",
       "  0.7813665270805359,\n",
       "  0.7814743518829346,\n",
       "  0.7816969156265259,\n",
       "  0.7819390892982483,\n",
       "  0.7820541858673096,\n",
       "  0.7823306918144226,\n",
       "  0.7824831008911133,\n",
       "  0.7825254797935486,\n",
       "  0.7826672196388245,\n",
       "  0.7827658653259277,\n",
       "  0.7830987572669983,\n",
       "  0.7831823825836182,\n",
       "  0.7832620143890381,\n",
       "  0.7836089730262756,\n",
       "  0.7836295366287231,\n",
       "  0.7837849855422974,\n",
       "  0.7840588688850403,\n",
       "  0.7841835021972656,\n",
       "  0.7842299342155457,\n",
       "  0.7843816876411438,\n",
       "  0.7845076322555542,\n",
       "  0.7847500443458557,\n",
       "  0.7848857641220093,\n",
       "  0.7849156260490417,\n",
       "  0.7849763035774231,\n",
       "  0.785197377204895,\n",
       "  0.7853511571884155,\n",
       "  0.7854986190795898,\n",
       "  0.7855435013771057,\n",
       "  0.7857319712638855,\n",
       "  0.7859753966331482,\n",
       "  0.7859156131744385,\n",
       "  0.7861703038215637,\n",
       "  0.7861383557319641,\n",
       "  0.7862761616706848,\n",
       "  0.7865231037139893,\n",
       "  0.7867120504379272,\n",
       "  0.7867157459259033,\n",
       "  0.7867251634597778,\n",
       "  0.7868426442146301,\n",
       "  0.7869367599487305,\n",
       "  0.7870615720748901,\n",
       "  0.7874274253845215,\n",
       "  0.7873377799987793,\n",
       "  0.7874459624290466,\n",
       "  0.787564218044281,\n",
       "  0.7875847220420837,\n",
       "  0.7877311110496521,\n",
       "  0.7878935933113098,\n",
       "  0.787897527217865,\n",
       "  0.7882378697395325,\n",
       "  0.7881273031234741,\n",
       "  0.788105845451355,\n",
       "  0.7883780002593994,\n",
       "  0.7885634899139404,\n",
       "  0.7886748313903809,\n",
       "  0.7887257933616638,\n",
       "  0.7888361811637878,\n",
       "  0.7888908982276917,\n",
       "  0.7889505624771118,\n",
       "  0.7889789342880249,\n",
       "  0.7890733480453491,\n",
       "  0.7891793847084045,\n",
       "  0.7892838716506958,\n",
       "  0.7892168164253235,\n",
       "  0.7892909646034241,\n",
       "  0.7895380258560181,\n",
       "  0.7896326780319214,\n",
       "  0.789558470249176,\n",
       "  0.7896933555603027,\n",
       "  0.7897418737411499,\n",
       "  0.7899023294448853,\n",
       "  0.7899332046508789,\n",
       "  0.7901275753974915,\n",
       "  0.7901052236557007],\n",
       " 'val_loss': [0.6842573285102844,\n",
       "  0.6805412173271179,\n",
       "  0.6789506673812866,\n",
       "  0.6779372096061707,\n",
       "  0.676797091960907,\n",
       "  0.676638126373291,\n",
       "  0.6760881543159485,\n",
       "  0.675510823726654,\n",
       "  0.6753823757171631,\n",
       "  0.6749960780143738,\n",
       "  0.6749008893966675,\n",
       "  0.6744024157524109,\n",
       "  0.6746075749397278,\n",
       "  0.6746140718460083,\n",
       "  0.6745357513427734,\n",
       "  0.6742324829101562,\n",
       "  0.6753820180892944,\n",
       "  0.6762667298316956,\n",
       "  0.6782101392745972,\n",
       "  0.6794577836990356,\n",
       "  0.6814930438995361,\n",
       "  0.6830958127975464,\n",
       "  0.6844441890716553,\n",
       "  0.6859186291694641,\n",
       "  0.6868900060653687,\n",
       "  0.6872271299362183,\n",
       "  0.6869574785232544,\n",
       "  0.6855888366699219,\n",
       "  0.6840688586235046,\n",
       "  0.6843740344047546,\n",
       "  0.6849080324172974,\n",
       "  0.6859143972396851,\n",
       "  0.6868252158164978,\n",
       "  0.6874785423278809,\n",
       "  0.6880995631217957,\n",
       "  0.6892220377922058,\n",
       "  0.6897419691085815,\n",
       "  0.6902876496315002,\n",
       "  0.6909460425376892,\n",
       "  0.6916292905807495,\n",
       "  0.6922399401664734,\n",
       "  0.6923660039901733,\n",
       "  0.6929609179496765,\n",
       "  0.693078339099884,\n",
       "  0.6936474442481995,\n",
       "  0.6934119462966919,\n",
       "  0.6932224035263062,\n",
       "  0.6933980584144592,\n",
       "  0.6936917901039124,\n",
       "  0.6937048435211182,\n",
       "  0.6936995387077332,\n",
       "  0.6936732530593872,\n",
       "  0.6937165260314941,\n",
       "  0.6933979392051697,\n",
       "  0.6941554546356201,\n",
       "  0.6939016580581665,\n",
       "  0.6940841674804688,\n",
       "  0.6945368051528931,\n",
       "  0.6941158175468445,\n",
       "  0.6942586302757263,\n",
       "  0.6940183043479919,\n",
       "  0.6942743062973022,\n",
       "  0.6940622925758362,\n",
       "  0.6939549446105957,\n",
       "  0.6939407587051392,\n",
       "  0.6940365433692932,\n",
       "  0.6938881874084473,\n",
       "  0.6936789155006409,\n",
       "  0.6934277415275574,\n",
       "  0.6930847764015198,\n",
       "  0.6933070421218872,\n",
       "  0.6929656267166138,\n",
       "  0.6932092905044556,\n",
       "  0.6925583481788635,\n",
       "  0.6927189230918884,\n",
       "  0.6925294399261475,\n",
       "  0.6926178932189941,\n",
       "  0.6926087141036987,\n",
       "  0.6919202208518982,\n",
       "  0.6918959617614746,\n",
       "  0.69184809923172,\n",
       "  0.6921018362045288,\n",
       "  0.6919767260551453,\n",
       "  0.6915872097015381,\n",
       "  0.691213846206665,\n",
       "  0.6913983821868896,\n",
       "  0.6913443803787231,\n",
       "  0.6912655830383301,\n",
       "  0.6911628842353821,\n",
       "  0.6909558176994324,\n",
       "  0.6908109188079834,\n",
       "  0.6901413798332214,\n",
       "  0.690091609954834,\n",
       "  0.6899632811546326,\n",
       "  0.690523087978363,\n",
       "  0.6899083256721497,\n",
       "  0.6900732517242432,\n",
       "  0.6899212002754211,\n",
       "  0.6899844408035278,\n",
       "  0.6892673969268799,\n",
       "  0.6895764470100403,\n",
       "  0.6893889904022217,\n",
       "  0.6889329552650452,\n",
       "  0.689296543598175,\n",
       "  0.6892106533050537,\n",
       "  0.6887946128845215,\n",
       "  0.6890065670013428,\n",
       "  0.6888088583946228,\n",
       "  0.6884491443634033,\n",
       "  0.6874440312385559,\n",
       "  0.6882879734039307,\n",
       "  0.6886481642723083,\n",
       "  0.6878063678741455,\n",
       "  0.6882601976394653,\n",
       "  0.6873236298561096,\n",
       "  0.6883813142776489,\n",
       "  0.687686562538147,\n",
       "  0.6880141496658325,\n",
       "  0.6878044009208679,\n",
       "  0.6881541013717651,\n",
       "  0.6877938508987427,\n",
       "  0.6869577169418335,\n",
       "  0.6872538924217224,\n",
       "  0.6859174370765686,\n",
       "  0.6874181032180786,\n",
       "  0.6875499486923218,\n",
       "  0.6867247223854065,\n",
       "  0.6874085068702698,\n",
       "  0.6865102052688599,\n",
       "  0.686697244644165,\n",
       "  0.6869443655014038,\n",
       "  0.6869545578956604,\n",
       "  0.6869878768920898,\n",
       "  0.686672568321228,\n",
       "  0.6861616373062134,\n",
       "  0.686668872833252,\n",
       "  0.6862682700157166,\n",
       "  0.686619222164154,\n",
       "  0.6877391934394836,\n",
       "  0.6857460141181946,\n",
       "  0.6855974197387695,\n",
       "  0.686127245426178,\n",
       "  0.6861684322357178,\n",
       "  0.6861664652824402,\n",
       "  0.6855432391166687,\n",
       "  0.6855806112289429,\n",
       "  0.6863488554954529,\n",
       "  0.6864185333251953,\n",
       "  0.6862319707870483,\n",
       "  0.6860911846160889,\n",
       "  0.6854346990585327,\n",
       "  0.6867802143096924,\n",
       "  0.686013400554657,\n",
       "  0.6858574151992798,\n",
       "  0.685988187789917,\n",
       "  0.6842748522758484,\n",
       "  0.6861864924430847,\n",
       "  0.6851766109466553,\n",
       "  0.6859671473503113,\n",
       "  0.6855168342590332,\n",
       "  0.6858431100845337,\n",
       "  0.6855881214141846,\n",
       "  0.6860647201538086,\n",
       "  0.6849843859672546,\n",
       "  0.6861257553100586,\n",
       "  0.6850378513336182,\n",
       "  0.6858716011047363,\n",
       "  0.6849379539489746,\n",
       "  0.685886561870575,\n",
       "  0.6851407289505005,\n",
       "  0.685470461845398,\n",
       "  0.6851542592048645,\n",
       "  0.6852322220802307,\n",
       "  0.6851710677146912,\n",
       "  0.6854459047317505,\n",
       "  0.6846460700035095,\n",
       "  0.6855630874633789,\n",
       "  0.6860215663909912,\n",
       "  0.68528151512146,\n",
       "  0.6854739189147949,\n",
       "  0.6863303780555725,\n",
       "  0.685794472694397,\n",
       "  0.6856423020362854,\n",
       "  0.6853622794151306,\n",
       "  0.6859437823295593,\n",
       "  0.6860597729682922,\n",
       "  0.6857413053512573,\n",
       "  0.6855180263519287,\n",
       "  0.6849650740623474,\n",
       "  0.6857850551605225,\n",
       "  0.6854925155639648,\n",
       "  0.6846832633018494,\n",
       "  0.6851751804351807,\n",
       "  0.6846296787261963,\n",
       "  0.6850403547286987,\n",
       "  0.685518741607666,\n",
       "  0.6863642930984497,\n",
       "  0.6846978068351746,\n",
       "  0.6860002279281616,\n",
       "  0.6860466003417969],\n",
       " 'val_accuracy': [0.4735957682132721,\n",
       "  0.48888787627220154,\n",
       "  0.49449092149734497,\n",
       "  0.497635155916214,\n",
       "  0.5013830065727234,\n",
       "  0.5017733573913574,\n",
       "  0.5031896829605103,\n",
       "  0.5047078728675842,\n",
       "  0.504819393157959,\n",
       "  0.505636990070343,\n",
       "  0.5054908990859985,\n",
       "  0.5066545605659485,\n",
       "  0.5053030252456665,\n",
       "  0.5046157836914062,\n",
       "  0.5041133165359497,\n",
       "  0.5045678615570068,\n",
       "  0.5001581907272339,\n",
       "  0.49723029136657715,\n",
       "  0.4919327199459076,\n",
       "  0.49011334776878357,\n",
       "  0.4869818091392517,\n",
       "  0.4849860668182373,\n",
       "  0.48289090394973755,\n",
       "  0.480459988117218,\n",
       "  0.4789709150791168,\n",
       "  0.4790933430194855,\n",
       "  0.4813963770866394,\n",
       "  0.4869539439678192,\n",
       "  0.4940175712108612,\n",
       "  0.49511879682540894,\n",
       "  0.4958175718784332,\n",
       "  0.49340301752090454,\n",
       "  0.49173152446746826,\n",
       "  0.49114060401916504,\n",
       "  0.49065878987312317,\n",
       "  0.4883551597595215,\n",
       "  0.48759332299232483,\n",
       "  0.4863327145576477,\n",
       "  0.4846036434173584,\n",
       "  0.483630895614624,\n",
       "  0.4838060736656189,\n",
       "  0.4827660620212555,\n",
       "  0.4822484850883484,\n",
       "  0.4828503131866455,\n",
       "  0.48242121934890747,\n",
       "  0.4816509187221527,\n",
       "  0.4825630187988281,\n",
       "  0.4818721115589142,\n",
       "  0.48099637031555176,\n",
       "  0.4821375906467438,\n",
       "  0.4822278916835785,\n",
       "  0.48283272981643677,\n",
       "  0.4828903079032898,\n",
       "  0.4831545352935791,\n",
       "  0.48325514793395996,\n",
       "  0.4833115041255951,\n",
       "  0.48445695638656616,\n",
       "  0.48437514901161194,\n",
       "  0.4838084876537323,\n",
       "  0.4852381944656372,\n",
       "  0.4854509234428406,\n",
       "  0.4861412048339844,\n",
       "  0.48667213320732117,\n",
       "  0.4857248365879059,\n",
       "  0.48808425664901733,\n",
       "  0.48619818687438965,\n",
       "  0.48638060688972473,\n",
       "  0.4874321222305298,\n",
       "  0.48745453357696533,\n",
       "  0.4875987768173218,\n",
       "  0.48841392993927,\n",
       "  0.48702728748321533,\n",
       "  0.489080011844635,\n",
       "  0.4902448356151581,\n",
       "  0.48922181129455566,\n",
       "  0.490230917930603,\n",
       "  0.49014848470687866,\n",
       "  0.4910242557525635,\n",
       "  0.49013152718544006,\n",
       "  0.4911987781524658,\n",
       "  0.4907696843147278,\n",
       "  0.49235212802886963,\n",
       "  0.4914509057998657,\n",
       "  0.49236181378364563,\n",
       "  0.49215152859687805,\n",
       "  0.4936436414718628,\n",
       "  0.4932381808757782,\n",
       "  0.49317818880081177,\n",
       "  0.49211031198501587,\n",
       "  0.4921096861362457,\n",
       "  0.4955484867095947,\n",
       "  0.4961351454257965,\n",
       "  0.49631819128990173,\n",
       "  0.4970211982727051,\n",
       "  0.4950539469718933,\n",
       "  0.49869757890701294,\n",
       "  0.4972884953022003,\n",
       "  0.49795395135879517,\n",
       "  0.4970775842666626,\n",
       "  0.498518168926239,\n",
       "  0.4955678880214691,\n",
       "  0.4973866641521454,\n",
       "  0.497127890586853,\n",
       "  0.49546241760253906,\n",
       "  0.49567151069641113,\n",
       "  0.4972030222415924,\n",
       "  0.49617213010787964,\n",
       "  0.4994739294052124,\n",
       "  0.4982048571109772,\n",
       "  0.5034363865852356,\n",
       "  0.4967169761657715,\n",
       "  0.4968533217906952,\n",
       "  0.5008551478385925,\n",
       "  0.49909695982933044,\n",
       "  0.5019587874412537,\n",
       "  0.4971678853034973,\n",
       "  0.50250244140625,\n",
       "  0.4991587996482849,\n",
       "  0.4999218285083771,\n",
       "  0.49630847573280334,\n",
       "  0.4994993805885315,\n",
       "  0.5058866739273071,\n",
       "  0.5046194195747375,\n",
       "  0.5133593678474426,\n",
       "  0.5017890930175781,\n",
       "  0.4994339346885681,\n",
       "  0.5051072835922241,\n",
       "  0.501349687576294,\n",
       "  0.5066624283790588,\n",
       "  0.5064666867256165,\n",
       "  0.5026103258132935,\n",
       "  0.5042181611061096,\n",
       "  0.5036963820457458,\n",
       "  0.5060672760009766,\n",
       "  0.5095393657684326,\n",
       "  0.505487859249115,\n",
       "  0.5091642141342163,\n",
       "  0.5078690648078918,\n",
       "  0.5004763603210449,\n",
       "  0.5128775835037231,\n",
       "  0.5126382112503052,\n",
       "  0.5115563869476318,\n",
       "  0.5109291076660156,\n",
       "  0.5087854266166687,\n",
       "  0.5156078934669495,\n",
       "  0.518858790397644,\n",
       "  0.5099151730537415,\n",
       "  0.509655773639679,\n",
       "  0.5097678899765015,\n",
       "  0.5167478919029236,\n",
       "  0.5204454660415649,\n",
       "  0.5053963661193848,\n",
       "  0.5143284797668457,\n",
       "  0.5133509039878845,\n",
       "  0.515668511390686,\n",
       "  0.5287351608276367,\n",
       "  0.5116551518440247,\n",
       "  0.5219084620475769,\n",
       "  0.5163345336914062,\n",
       "  0.5214902758598328,\n",
       "  0.5170660614967346,\n",
       "  0.5217896699905396,\n",
       "  0.5160054564476013,\n",
       "  0.5231387615203857,\n",
       "  0.5174036622047424,\n",
       "  0.5243211984634399,\n",
       "  0.5209497213363647,\n",
       "  0.5264751315116882,\n",
       "  0.5219842195510864,\n",
       "  0.529236376285553,\n",
       "  0.5211975574493408,\n",
       "  0.5263878703117371,\n",
       "  0.528087854385376,\n",
       "  0.5293315052986145,\n",
       "  0.5255587697029114,\n",
       "  0.5373939275741577,\n",
       "  0.5289182066917419,\n",
       "  0.5237060785293579,\n",
       "  0.5286697149276733,\n",
       "  0.5264060497283936,\n",
       "  0.5200085043907166,\n",
       "  0.5290951728820801,\n",
       "  0.5265181660652161,\n",
       "  0.5294478535652161,\n",
       "  0.5279878973960876,\n",
       "  0.5280812382698059,\n",
       "  0.5277509093284607,\n",
       "  0.5270187854766846,\n",
       "  0.5345442295074463,\n",
       "  0.5298769474029541,\n",
       "  0.5310291051864624,\n",
       "  0.5400182008743286,\n",
       "  0.5387345552444458,\n",
       "  0.5439606308937073,\n",
       "  0.5335278511047363,\n",
       "  0.5303430557250977,\n",
       "  0.5276424288749695,\n",
       "  0.5467254519462585,\n",
       "  0.5274314880371094,\n",
       "  0.5308709144592285],\n",
       " 'val_val_binary_crossentropy': [0.6842477917671204,\n",
       "  0.6805392503738403,\n",
       "  0.678950309753418,\n",
       "  0.6779376268386841,\n",
       "  0.6767969727516174,\n",
       "  0.6766379475593567,\n",
       "  0.6760879755020142,\n",
       "  0.6755102276802063,\n",
       "  0.6753818988800049,\n",
       "  0.6749958992004395,\n",
       "  0.6749001145362854,\n",
       "  0.6744015216827393,\n",
       "  0.6746072173118591,\n",
       "  0.6746132373809814,\n",
       "  0.6745345592498779,\n",
       "  0.6742309331893921,\n",
       "  0.6753815412521362,\n",
       "  0.6762660145759583,\n",
       "  0.6782102584838867,\n",
       "  0.6794591546058655,\n",
       "  0.6814956665039062,\n",
       "  0.6830995678901672,\n",
       "  0.6844480037689209,\n",
       "  0.6859226822853088,\n",
       "  0.6868935227394104,\n",
       "  0.6872305274009705,\n",
       "  0.6869612336158752,\n",
       "  0.6855921149253845,\n",
       "  0.6840710639953613,\n",
       "  0.6843742728233337,\n",
       "  0.6849064826965332,\n",
       "  0.6859123110771179,\n",
       "  0.6868221163749695,\n",
       "  0.6874735951423645,\n",
       "  0.6880927681922913,\n",
       "  0.689214825630188,\n",
       "  0.6897343993186951,\n",
       "  0.6902808547019958,\n",
       "  0.690939724445343,\n",
       "  0.6916228532791138,\n",
       "  0.692234218120575,\n",
       "  0.6923596858978271,\n",
       "  0.692955493927002,\n",
       "  0.6930742263793945,\n",
       "  0.6936440467834473,\n",
       "  0.6934080123901367,\n",
       "  0.6932182312011719,\n",
       "  0.6933945417404175,\n",
       "  0.6936883926391602,\n",
       "  0.6937025785446167,\n",
       "  0.6936991810798645,\n",
       "  0.6936736106872559,\n",
       "  0.6937164664268494,\n",
       "  0.6933977007865906,\n",
       "  0.6941578388214111,\n",
       "  0.6939040422439575,\n",
       "  0.6940883994102478,\n",
       "  0.6945422887802124,\n",
       "  0.6941216588020325,\n",
       "  0.6942647099494934,\n",
       "  0.6940260529518127,\n",
       "  0.6942824125289917,\n",
       "  0.6940726041793823,\n",
       "  0.6939659118652344,\n",
       "  0.6939530372619629,\n",
       "  0.6940464973449707,\n",
       "  0.6939005851745605,\n",
       "  0.6936917304992676,\n",
       "  0.6934393644332886,\n",
       "  0.6930959820747375,\n",
       "  0.6933206915855408,\n",
       "  0.6929768919944763,\n",
       "  0.6932246685028076,\n",
       "  0.6925735473632812,\n",
       "  0.6927334666252136,\n",
       "  0.6925455331802368,\n",
       "  0.6926330924034119,\n",
       "  0.6926263570785522,\n",
       "  0.6919348239898682,\n",
       "  0.6919112205505371,\n",
       "  0.6918622851371765,\n",
       "  0.692118763923645,\n",
       "  0.6919937133789062,\n",
       "  0.6916041374206543,\n",
       "  0.6912298798561096,\n",
       "  0.6914166212081909,\n",
       "  0.6913630962371826,\n",
       "  0.6912834048271179,\n",
       "  0.6911795735359192,\n",
       "  0.6909720301628113,\n",
       "  0.6908296346664429,\n",
       "  0.6901593804359436,\n",
       "  0.6901106834411621,\n",
       "  0.6899821162223816,\n",
       "  0.6905421018600464,\n",
       "  0.6899275779724121,\n",
       "  0.6900929808616638,\n",
       "  0.6899412870407104,\n",
       "  0.6900045871734619,\n",
       "  0.6892873048782349,\n",
       "  0.6895965337753296,\n",
       "  0.6894097924232483,\n",
       "  0.6889525055885315,\n",
       "  0.6893157958984375,\n",
       "  0.689231276512146,\n",
       "  0.6888151168823242,\n",
       "  0.6890279054641724,\n",
       "  0.6888294816017151,\n",
       "  0.6884702444076538,\n",
       "  0.687465250492096,\n",
       "  0.688308835029602,\n",
       "  0.6886695623397827,\n",
       "  0.6878281235694885,\n",
       "  0.6882821917533875,\n",
       "  0.6873465776443481,\n",
       "  0.6884031891822815,\n",
       "  0.6877086162567139,\n",
       "  0.688037097454071,\n",
       "  0.6878266334533691,\n",
       "  0.688174843788147,\n",
       "  0.6878162026405334,\n",
       "  0.6869795918464661,\n",
       "  0.6872764825820923,\n",
       "  0.6859407424926758,\n",
       "  0.6874415278434753,\n",
       "  0.6875718235969543,\n",
       "  0.686747133731842,\n",
       "  0.6874305605888367,\n",
       "  0.6865336894989014,\n",
       "  0.6867187023162842,\n",
       "  0.6869667768478394,\n",
       "  0.686976969242096,\n",
       "  0.6870109438896179,\n",
       "  0.6866959929466248,\n",
       "  0.6861855983734131,\n",
       "  0.6866910457611084,\n",
       "  0.6862912178039551,\n",
       "  0.686642050743103,\n",
       "  0.6877617835998535,\n",
       "  0.6857678294181824,\n",
       "  0.6856197118759155,\n",
       "  0.6861498355865479,\n",
       "  0.6861909627914429,\n",
       "  0.6861889362335205,\n",
       "  0.6855668425559998,\n",
       "  0.685603141784668,\n",
       "  0.6863716840744019,\n",
       "  0.6864405870437622,\n",
       "  0.6862537860870361,\n",
       "  0.6861157417297363,\n",
       "  0.6854583024978638,\n",
       "  0.6868012547492981,\n",
       "  0.686036229133606,\n",
       "  0.6858800053596497,\n",
       "  0.6860102415084839,\n",
       "  0.684299111366272,\n",
       "  0.6862079501152039,\n",
       "  0.6851999163627625,\n",
       "  0.6859909296035767,\n",
       "  0.6855418086051941,\n",
       "  0.6858667135238647,\n",
       "  0.68561190366745,\n",
       "  0.6860887408256531,\n",
       "  0.6850091218948364,\n",
       "  0.6861492991447449,\n",
       "  0.6850636601448059,\n",
       "  0.6858964562416077,\n",
       "  0.6849626302719116,\n",
       "  0.6859118342399597,\n",
       "  0.6851674318313599,\n",
       "  0.6854939460754395,\n",
       "  0.6851789355278015,\n",
       "  0.6852582693099976,\n",
       "  0.6851983070373535,\n",
       "  0.6854718327522278,\n",
       "  0.6846743226051331,\n",
       "  0.685590922832489,\n",
       "  0.6860468983650208,\n",
       "  0.6853076815605164,\n",
       "  0.6854979991912842,\n",
       "  0.686353325843811,\n",
       "  0.685823380947113,\n",
       "  0.6856677532196045,\n",
       "  0.6853904724121094,\n",
       "  0.6859714388847351,\n",
       "  0.6860872507095337,\n",
       "  0.6857695579528809,\n",
       "  0.6855455636978149,\n",
       "  0.6849936842918396,\n",
       "  0.6858121156692505,\n",
       "  0.6855215430259705,\n",
       "  0.684714138507843,\n",
       "  0.6852076053619385,\n",
       "  0.6846625804901123,\n",
       "  0.6850698590278625,\n",
       "  0.6855476498603821,\n",
       "  0.6863942742347717,\n",
       "  0.6847320199012756,\n",
       "  0.6860266923904419,\n",
       "  0.6860756874084473],\n",
       " 'val_auc': [0.6219770312309265,\n",
       "  0.6452390551567078,\n",
       "  0.6537286639213562,\n",
       "  0.6579475998878479,\n",
       "  0.6599478125572205,\n",
       "  0.6619526743888855,\n",
       "  0.6640335917472839,\n",
       "  0.6658503413200378,\n",
       "  0.6673164963722229,\n",
       "  0.6689161062240601,\n",
       "  0.6703037619590759,\n",
       "  0.671728253364563,\n",
       "  0.6731854677200317,\n",
       "  0.6738928556442261,\n",
       "  0.6749635338783264,\n",
       "  0.6752933263778687,\n",
       "  0.6746999025344849,\n",
       "  0.6720559000968933,\n",
       "  0.6663315892219543,\n",
       "  0.6552909016609192,\n",
       "  0.6397854685783386,\n",
       "  0.6259788870811462,\n",
       "  0.6161534190177917,\n",
       "  0.6050959825515747,\n",
       "  0.5939371585845947,\n",
       "  0.5810237526893616,\n",
       "  0.5725762248039246,\n",
       "  0.569186806678772,\n",
       "  0.5696643590927124,\n",
       "  0.5644686818122864,\n",
       "  0.5569955706596375,\n",
       "  0.5479315519332886,\n",
       "  0.5413808822631836,\n",
       "  0.5386895537376404,\n",
       "  0.5377600193023682,\n",
       "  0.5327970385551453,\n",
       "  0.5324394702911377,\n",
       "  0.5305137634277344,\n",
       "  0.5265228748321533,\n",
       "  0.5252347588539124,\n",
       "  0.5277588367462158,\n",
       "  0.5243401527404785,\n",
       "  0.5244099497795105,\n",
       "  0.5270341634750366,\n",
       "  0.5298808813095093,\n",
       "  0.5277947187423706,\n",
       "  0.5298903584480286,\n",
       "  0.5303086042404175,\n",
       "  0.5289950966835022,\n",
       "  0.5348536968231201,\n",
       "  0.5367746353149414,\n",
       "  0.539354681968689,\n",
       "  0.5398929715156555,\n",
       "  0.5398143529891968,\n",
       "  0.5441082119941711,\n",
       "  0.5426496863365173,\n",
       "  0.5468370318412781,\n",
       "  0.5473053455352783,\n",
       "  0.5454204678535461,\n",
       "  0.5490046143531799,\n",
       "  0.5507790446281433,\n",
       "  0.5530644655227661,\n",
       "  0.5544445514678955,\n",
       "  0.5522546172142029,\n",
       "  0.5596946477890015,\n",
       "  0.5554222464561462,\n",
       "  0.5550113916397095,\n",
       "  0.559145987033844,\n",
       "  0.5575606226921082,\n",
       "  0.5576856136322021,\n",
       "  0.5623680949211121,\n",
       "  0.5573021769523621,\n",
       "  0.5642893314361572,\n",
       "  0.5666597485542297,\n",
       "  0.5665614604949951,\n",
       "  0.567460834980011,\n",
       "  0.5692100524902344,\n",
       "  0.570907711982727,\n",
       "  0.5679582357406616,\n",
       "  0.572717010974884,\n",
       "  0.5714773535728455,\n",
       "  0.5746752619743347,\n",
       "  0.5739650726318359,\n",
       "  0.5774388313293457,\n",
       "  0.5755405426025391,\n",
       "  0.5794193744659424,\n",
       "  0.5788899064064026,\n",
       "  0.5801470279693604,\n",
       "  0.5774645805358887,\n",
       "  0.5784057974815369,\n",
       "  0.584246039390564,\n",
       "  0.5855903029441833,\n",
       "  0.5837194919586182,\n",
       "  0.5865033268928528,\n",
       "  0.585577666759491,\n",
       "  0.5879483222961426,\n",
       "  0.5882131457328796,\n",
       "  0.5882549285888672,\n",
       "  0.5878084897994995,\n",
       "  0.5897491574287415,\n",
       "  0.5881535410881042,\n",
       "  0.5910354852676392,\n",
       "  0.5895522832870483,\n",
       "  0.5899356007575989,\n",
       "  0.590528666973114,\n",
       "  0.59303218126297,\n",
       "  0.5914565324783325,\n",
       "  0.5959459543228149,\n",
       "  0.594750702381134,\n",
       "  0.5985450148582458,\n",
       "  0.5954980850219727,\n",
       "  0.5970683693885803,\n",
       "  0.5992802977561951,\n",
       "  0.5975797176361084,\n",
       "  0.6018086671829224,\n",
       "  0.599062442779541,\n",
       "  0.6031562685966492,\n",
       "  0.6014761328697205,\n",
       "  0.602436363697052,\n",
       "  0.6006475687026978,\n",
       "  0.604692816734314,\n",
       "  0.6076198220252991,\n",
       "  0.6065402626991272,\n",
       "  0.6106445789337158,\n",
       "  0.6068933010101318,\n",
       "  0.606101930141449,\n",
       "  0.6089285016059875,\n",
       "  0.6061640977859497,\n",
       "  0.609342098236084,\n",
       "  0.6096559166908264,\n",
       "  0.6093553304672241,\n",
       "  0.609917402267456,\n",
       "  0.6090711951255798,\n",
       "  0.6113918423652649,\n",
       "  0.6128790974617004,\n",
       "  0.6119961738586426,\n",
       "  0.6133427619934082,\n",
       "  0.613089919090271,\n",
       "  0.609880805015564,\n",
       "  0.6161684393882751,\n",
       "  0.6158592104911804,\n",
       "  0.6145304441452026,\n",
       "  0.614449143409729,\n",
       "  0.6140832901000977,\n",
       "  0.6168925762176514,\n",
       "  0.6171032190322876,\n",
       "  0.6141939759254456,\n",
       "  0.6153902411460876,\n",
       "  0.6151865720748901,\n",
       "  0.6169597506523132,\n",
       "  0.6185178756713867,\n",
       "  0.6140077114105225,\n",
       "  0.6168691515922546,\n",
       "  0.6166855096817017,\n",
       "  0.616926372051239,\n",
       "  0.6213606595993042,\n",
       "  0.617536723613739,\n",
       "  0.6198724508285522,\n",
       "  0.6175749897956848,\n",
       "  0.6185088157653809,\n",
       "  0.6182453036308289,\n",
       "  0.6187071800231934,\n",
       "  0.6176553964614868,\n",
       "  0.6199948191642761,\n",
       "  0.6173304319381714,\n",
       "  0.6204412579536438,\n",
       "  0.6190453767776489,\n",
       "  0.6206985116004944,\n",
       "  0.6196895241737366,\n",
       "  0.6211432218551636,\n",
       "  0.6199870109558105,\n",
       "  0.6203691363334656,\n",
       "  0.6206542253494263,\n",
       "  0.6207427978515625,\n",
       "  0.619958758354187,\n",
       "  0.6215853095054626,\n",
       "  0.6194601058959961,\n",
       "  0.6187112927436829,\n",
       "  0.6203498840332031,\n",
       "  0.6195626854896545,\n",
       "  0.6167936325073242,\n",
       "  0.6194913387298584,\n",
       "  0.6188269257545471,\n",
       "  0.619732141494751,\n",
       "  0.6187540292739868,\n",
       "  0.6184470057487488,\n",
       "  0.6191946864128113,\n",
       "  0.6200147271156311,\n",
       "  0.6212177276611328,\n",
       "  0.6192120909690857,\n",
       "  0.6196376085281372,\n",
       "  0.6211634874343872,\n",
       "  0.6203899383544922,\n",
       "  0.6216800808906555,\n",
       "  0.6205389499664307,\n",
       "  0.6199690699577332,\n",
       "  0.6173331141471863,\n",
       "  0.621455192565918,\n",
       "  0.617926836013794,\n",
       "  0.6179028749465942],\n",
       " 'lr': [0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now vs Then: Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - New Python packages such as TensorFlow and Keras. \n",
    "Today there are newer packages that require a lot less effort and are more efficient such as Tensorflow and Keras. TensorFlow is an end to end open-source library used for machine learning and neural networks.  It facilitates easy model building and is quite robust for all machine learning applications. It has a rather simple and flexible architecture making it much easier to code, develop models and deploy them quiker.\n",
    "\n",
    "Keras is a neural network library providing high level APIs and is built in Python making it more user friendly.  It facilitates the rapid building and testing of a neural network with minimal lines of code.\n",
    "\n",
    "\n",
    "#### Activation Functions: We would explore using the newer activation functions of ReLU, Softwmax and Softplus vs tanh or Sigmoid.\n",
    "\n",
    "\n",
    "**Softmax activation** – \"This function converts numbers to probabilities that sum up to one. It outputs a vector that denotes the probability distributions of a list of potenmtial outcomes\"\n",
    "\n",
    "Reference: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d#:~:text=Specifically%20trying%20out%20neural%20networks,a%20list%20of%20potential%20outcomes.\n",
    "\n",
    "**Sigmoid activation** - \"Sigmoid activation functions use a second non-linearity for large inputs. Being in the range from zero and one, sigmoid activations can be interpreted as probabilities. If a range from -1 to 1 is desired, the sigmoid can be scaled and shifted to yield the hyperbolic tangent activation function\"\n",
    "\n",
    "Reference: https://en.wikipedia.org/wiki/Activation_function#:~:text=Sigmoid%20activation%20functions%20use%20a,the%20hyperbolic%20tangent%20activation%20function%3A%20.\n",
    "\n",
    "**tanh** - \"tanh is similar to a logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped). The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\"\n",
    "\n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6#:~:text=tanh%20is%20also%20like%20logistic,sigmoidal%20(s%20%2D%20shaped).&text=The%20advantage%20is%20that%20the,zero%20in%20the%20tanh%20graph.\n",
    "\n",
    "\n",
    "**ReLU activation** – Rectified Linear Unit activation function, is a piecewise linear function that outputs the input directly if positive (otherwise, output 0). Default activation, easy train, better performance, activation is threshold at 0, can accelerate SGD, implemented by simply thresholding matrix of activations at 0, can be fragile where weights could update in a way for neuron to not activate again (Leaky ReLU attempts to fix dying problem)\n",
    "\n",
    "\n",
    "Training and Testing using different splits of the data. Baldi et al used 99% of the data for training and a 1% for testing.  We recommend using an 80% train and 20% test setbut other options such as 70/30 should also be explored.\n",
    "\n",
    "**ADAM Optimizer** - \"Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iteratively based on the training data.\"  We leveraged the Adam optimizer for our model but clearly given our accuracyt scores, more refinement is required to obtian the most optimal parameter settings.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20an%20optimization%20algorithm,iterative%20based%20in%20training%20data.&text=The%20algorithm%20is%20called%20Adam.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXJxtwBWIhjG"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "While we were able to leverage the use of different parameters anbd functions within Tensoflow and Keras, we were not able to achieve the same ROC scores per Baldi et al in their original paper. It is also evident that neural networks provide extensive functionality and power to build models of high complexity with relative ease and coding.  The key learning for us through this exercise was the importance of **tuning** the parameters and therefore understanding their impact on model performance and execution.\n",
    "\n",
    "## Steven & Joe - You may want to add some color at a high level as to which parameters were important and delivered the greatest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjfnkEeQyAFG"
   },
   "source": [
    "To recap: here are the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Get more training data.\n",
    "* Reduce the capacity of the network.\n",
    "* Add weight regularization.\n",
    "* Add dropout.\n",
    "\n",
    "Two important approaches not covered in this guide are:\n",
    "\n",
    "* data-augmentation\n",
    "* batch normalization\n",
    "\n",
    "Remember that each method can help on its own, but often combining them can be even more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "NW will add "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "overfit_and_underfit.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
