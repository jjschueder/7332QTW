{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE 3/CASE 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary/Introduction blurb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing \n",
    "\n",
    "We received the data in an \"R data format\" file. Since we opted to do this analysis in Python, the Python package \"pyreadr\" was used to read the data into a Python Pandas dataframe; we then converted the dataframe to a comma separated file (CSV) to store on GitHub for ease of use. Each execution of this Jupyter notebook reads the data from the CSV file on GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jjschueder/7333QTW/blob/master/Case%20Study%203/Py%20read%20R.JPG -- DO WE NEED THIS LINK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyreadr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e6574da70878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import all packages needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyreadr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyreadr'"
     ]
    }
   ],
   "source": [
    "#import all packages needed\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset - ONLY DONE ONCE, included for reference\n",
    "# result = pyreadr.read_r(r'C:\\Users\\jjsch\\Downloads\\Week_5_Materials_2\\data.Rda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a dataframe - ONLY DONE ONCE, for reference\n",
    "# df = result[\"emailDFrp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe to CSV - ONLY DONE ONCE, for reference\n",
    "# df.to_csv(r'C:\\Users\\jjsch\\Downloads\\Week_5_Materials_2\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>isSpam</th>\n",
       "      <th>isRe</th>\n",
       "      <th>underscore</th>\n",
       "      <th>priority</th>\n",
       "      <th>isInReplyTo</th>\n",
       "      <th>sortedRec</th>\n",
       "      <th>subPunc</th>\n",
       "      <th>multipartText</th>\n",
       "      <th>hasImages</th>\n",
       "      <th>...</th>\n",
       "      <th>subQuesCt</th>\n",
       "      <th>numAtt</th>\n",
       "      <th>numRec</th>\n",
       "      <th>perCaps</th>\n",
       "      <th>hour</th>\n",
       "      <th>perHTML</th>\n",
       "      <th>subBlanks</th>\n",
       "      <th>forwards</th>\n",
       "      <th>avgWordLen</th>\n",
       "      <th>numDlr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.451039</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.376623</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.491289</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.436096</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.817164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.918919</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.116643</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.217391</td>\n",
       "      <td>6.451613</td>\n",
       "      <td>4.234940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  isSpam isRe underscore priority isInReplyTo sortedRec subPunc  \\\n",
       "0           0       0    T          F        F           T         T       F   \n",
       "1           1       0    F          F        F           F         T       F   \n",
       "2           2       0    F          F        F           F         T       F   \n",
       "3           3       0    F          F        F           F         T       F   \n",
       "4           4       0    T          F        F           F         T       F   \n",
       "\n",
       "  multipartText hasImages  ... subQuesCt numAtt numRec   perCaps  hour  \\\n",
       "0             F         F  ...       0.0    0.0    2.0  4.451039  11.0   \n",
       "1             F         F  ...       0.0    0.0    1.0  7.491289  11.0   \n",
       "2             F         F  ...       0.0    0.0    1.0  7.436096  12.0   \n",
       "3             F         F  ...       0.0    0.0    0.0  5.090909  13.0   \n",
       "4             F         F  ...       0.0    0.0    1.0  6.116643  13.0   \n",
       "\n",
       "  perHTML  subBlanks  forwards  avgWordLen  numDlr  \n",
       "0     0.0  12.500000  0.000000    4.376623       3  \n",
       "1     0.0   8.000000  0.000000    4.555556       0  \n",
       "2     0.0   8.000000  0.000000    4.817164       0  \n",
       "3     0.0  18.918919  3.125000    4.714286       0  \n",
       "4     0.0  15.217391  6.451613    4.234940       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data \n",
    "df = pd.read_csv(r'https://raw.githubusercontent.com/jjschueder/7333QTW/master/Case%20Study%203/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial review of the data shows many columns are formated as a \"logical\" data type of TRUE or FALSE. Machine learning classificaton algorithms need numeric variables. Therefore, we converted the TRUE/FALSE variables to a numeric representation: TRUE == 1; FALSE == 0. The remaining variables are numeric as they represent percentages, counts, and hours of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform logical data to numeric data\n",
    "# df['isSpam'] = df['isSpam'].map({'T': 1, 'F': 0})\n",
    "df['isRe'] = df['isRe'].map({'T': 1, 'F': 0})             \n",
    "df['underscore'] = df['underscore'].map({'T': 1, 'F': 0})       \n",
    "df['priority'] = df['priority'].map({'T': 1, 'F': 0})         \n",
    "df['isInReplyTo'] = df['isInReplyTo'].map({'T': 1, 'F': 0})      \n",
    "df['sortedRec'] = df['sortedRec'].map({'T': 1, 'F': 0})        \n",
    "df['subPunc'] = df['subPunc'].map({'T': 1, 'F': 0})          \n",
    "df['multipartText'] = df['multipartText'].map({'T': 1, 'F': 0})    \n",
    "df['hasImages'] = df['hasImages'].map({'T': 1, 'F': 0})        \n",
    "df['isPGPsigned'] = df['isPGPsigned'].map({'T': 1, 'F': 0})       \n",
    "df['subSpamWords'] = df['subSpamWords'].map({'T': 1, 'F': 0})      \n",
    "df['noHost'] = df['noHost'].map({'T': 1, 'F': 0})            \n",
    "df['numEnd'] = df['numEnd'].map({'T': 1, 'F': 0})            \n",
    "df['isYelling'] = df['isYelling'].map({'T': 1, 'F': 0})         \n",
    "df['isOrigMsg'] = df['isOrigMsg'].map({'T': 1, 'F': 0})         \n",
    "df['isDear'] = df['isDear'].map({'T': 1, 'F': 0})            \n",
    "df['isWrote'] = df['isWrote'].map({'T': 1, 'F': 0})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming proper the proper type of data frame, we conducted exploratory data analysis to better understand the dataset. Our first step was to understand the variables and their associated meaning in relation to an email message. The data dictionary below provides a summary. Note, the target variable to predict - IsSpam - is included. There is a record id - Unnamed: 0 - which is not necessary for our analysi,s but we retained it for reference as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'isSpam', 'isRe', 'underscore', 'priority', 'isInReplyTo',\n",
       "       'sortedRec', 'subPunc', 'multipartText', 'hasImages', 'isPGPsigned',\n",
       "       'subSpamWords', 'noHost', 'numEnd', 'isYelling', 'isOrigMsg', 'isDear',\n",
       "       'isWrote', 'numLines', 'bodyCharCt', 'subExcCt', 'subQuesCt', 'numAtt',\n",
       "       'numRec', 'perCaps', 'hour', 'perHTML', 'subBlanks', 'forwards',\n",
       "       'avgWordLen', 'numDlr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| VARIABLE | DEFINITION |\n",
    "|-|-|\n",
    "| isSpam | TRUE if email classified as spam |\n",
    "| isRe | TRUE if Re: appears at the start of subject line |\n",
    "| numLines | number of lines in body of message |\n",
    "| bodyCharCt | number of characters in the body of message |\n",
    "| underscore | TRUE if email address in From field of header contains underscore |\n",
    "| subExcCt | number of exclamation marks in subject |\n",
    "| subQuesCt | number of question marks in subject |\n",
    "| numAtt | number of attachments in message |\n",
    "| priority | TRUE if Priority key is present in header |\n",
    "| numRec | number of recipients of message, including CCs |\n",
    "| perCaps | percentage of capitals among all letters in message |\n",
    "| isInReplyTo | TRUE if the In-Reply-To key is present in header |\n",
    "| sortedRec | TRUE if recipients' email addresses are sorted |\n",
    "| subPunc | TRUE if words in subject have punctuation or numbers embedded (i.e. w!se) |\n",
    "| hour | hour of the day in the Date field |\n",
    "| multipartText | TRUE if MIME type is multipart/text |\n",
    "| hasImages | TRUE if message contains images |\n",
    "| isPGPsigned | TRUE if message contains a PGP (encryption) signature |\n",
    "| perMTML | percentage of characters in HTML tags in message compared to all characters |\n",
    "| subSpamWords | TRUE if subject contains one of the words in spam word vector |\n",
    "| subBlanks | percentage of blanks in subject |\n",
    "| noHost | TRUE if there is no hostname in Message-Id key in header |\n",
    "| numEnd | TRUE if sender's email address (before @) ends in number |\n",
    "| isYelling | TRUE if subject is in all capital letters |\n",
    "| forwards | number of forward symbols in a line of the body (i.e. >>> xxx contains 3) |\n",
    "| isOrigMsg | TRUE if message body contains phrase \"original message\" |\n",
    "| isDear | TRUE if message body contains word \"dear\" |\n",
    "| isWrote | TRUE if message contains phrase \"wrote:\" |\n",
    "| avgWordLen | average length of words in message |\n",
    "| numDlr | number of dollar signs in message |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data profiling\n",
    "\n",
    "Next, we dug deeper into the exploratory analysis to understand descriptive statistics, data distrutions, and corrleation between our variables. Using standard Python command and a Python package called Pandas Profiling, we found some notable characters about select variables.\n",
    "\n",
    "Missing data: \n",
    "* subSpam (7 missing), noHost (1 missing), isYelling (7 missing), subExcCt (20 missing), subQuesCt (20 missing), numRec (20 missing), and subBlanks (20 missing). \n",
    "* Missing values and imputation strategy outlined below.\n",
    "\n",
    "Outliers / Odd data:\n",
    "* Outliers correlated with Spam: numRec with 311 receipents; 100% capital letters in email (>80% was spam threshold for perCaps); 100% HTML characters (>97% was spam threshold for perHTML); 86% blanks in subject (>25% was spam threshold for subBlanks), and average word length of 26 is a spam outlier.\n",
    "* Outliers not necessarily correlated with Spam: count of question marks in subject (8 and 12 outliers were not spam); greatest number of attachments at 18 (not spam); number of forwards at 99 (not spam); and large number of dollar signs at 1977 (not spam).\n",
    "\n",
    "WHAT OTHER INTERESTING THINGS TO NOTE FROM PROFILE\n",
    "* Distributions\n",
    "* Other thoughts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>isSpam</th>\n",
       "      <th>isRe</th>\n",
       "      <th>underscore</th>\n",
       "      <th>priority</th>\n",
       "      <th>isInReplyTo</th>\n",
       "      <th>sortedRec</th>\n",
       "      <th>subPunc</th>\n",
       "      <th>multipartText</th>\n",
       "      <th>hasImages</th>\n",
       "      <th>...</th>\n",
       "      <th>subQuesCt</th>\n",
       "      <th>numAtt</th>\n",
       "      <th>numRec</th>\n",
       "      <th>perCaps</th>\n",
       "      <th>hour</th>\n",
       "      <th>perHTML</th>\n",
       "      <th>subBlanks</th>\n",
       "      <th>forwards</th>\n",
       "      <th>avgWordLen</th>\n",
       "      <th>numDlr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9328.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9066.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9328.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "      <td>9348.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4673.500000</td>\n",
       "      <td>0.256418</td>\n",
       "      <td>0.321459</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.298674</td>\n",
       "      <td>0.898588</td>\n",
       "      <td>0.028134</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>1.929407</td>\n",
       "      <td>8.850371</td>\n",
       "      <td>12.210847</td>\n",
       "      <td>6.517082</td>\n",
       "      <td>13.866939</td>\n",
       "      <td>10.445086</td>\n",
       "      <td>4.487222</td>\n",
       "      <td>1.781558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2698.679492</td>\n",
       "      <td>0.436679</td>\n",
       "      <td>0.467062</td>\n",
       "      <td>0.115319</td>\n",
       "      <td>0.075788</td>\n",
       "      <td>0.457701</td>\n",
       "      <td>0.301890</td>\n",
       "      <td>0.165365</td>\n",
       "      <td>0.184011</td>\n",
       "      <td>0.048458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507685</td>\n",
       "      <td>0.324879</td>\n",
       "      <td>5.242396</td>\n",
       "      <td>9.583415</td>\n",
       "      <td>6.623932</td>\n",
       "      <td>19.135266</td>\n",
       "      <td>7.431938</td>\n",
       "      <td>18.263576</td>\n",
       "      <td>0.568582</td>\n",
       "      <td>30.380455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.363072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2336.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.255319</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.526316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.208257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4673.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.055473</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.253012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7010.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.059399</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.686275</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>4.728507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9347.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>86.419753</td>\n",
       "      <td>99.058270</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1977.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       isSpam         isRe   underscore     priority  \\\n",
       "count  9348.000000  9348.000000  9348.000000  9348.000000  9348.000000   \n",
       "mean   4673.500000     0.256418     0.321459     0.013479     0.005777   \n",
       "std    2698.679492     0.436679     0.467062     0.115319     0.075788   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%    2336.750000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%    4673.500000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%    7010.250000     1.000000     1.000000     0.000000     0.000000   \n",
       "max    9347.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       isInReplyTo    sortedRec      subPunc  multipartText    hasImages  ...  \\\n",
       "count  9348.000000  9348.000000  9348.000000    9348.000000  9348.000000  ...   \n",
       "mean      0.298674     0.898588     0.028134       0.035088     0.002353  ...   \n",
       "std       0.457701     0.301890     0.165365       0.184011     0.048458  ...   \n",
       "min       0.000000     0.000000     0.000000       0.000000     0.000000  ...   \n",
       "25%       0.000000     1.000000     0.000000       0.000000     0.000000  ...   \n",
       "50%       0.000000     1.000000     0.000000       0.000000     0.000000  ...   \n",
       "75%       1.000000     1.000000     0.000000       0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000       1.000000     1.000000  ...   \n",
       "\n",
       "         subQuesCt       numAtt       numRec      perCaps         hour  \\\n",
       "count  9328.000000  9348.000000  9066.000000  9348.000000  9348.000000   \n",
       "mean      0.137757     0.065789     1.929407     8.850371    12.210847   \n",
       "std       0.507685     0.324879     5.242396     9.583415     6.623932   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     1.000000     4.255319     8.000000   \n",
       "50%       0.000000     0.000000     1.000000     6.055473    13.000000   \n",
       "75%       0.000000     0.000000     1.000000     9.059399    18.000000   \n",
       "max      12.000000    18.000000   311.000000   100.000000    23.000000   \n",
       "\n",
       "           perHTML    subBlanks     forwards   avgWordLen       numDlr  \n",
       "count  9348.000000  9328.000000  9348.000000  9348.000000  9348.000000  \n",
       "mean      6.517082    13.866939    10.445086     4.487222     1.781558  \n",
       "std      19.135266     7.431938    18.263576     0.568582    30.380455  \n",
       "min       0.000000     0.000000     0.000000     1.363072     0.000000  \n",
       "25%       0.000000    10.526316     0.000000     4.208257     0.000000  \n",
       "50%       0.000000    13.253012     0.000000     4.454545     0.000000  \n",
       "75%       0.000000    15.686275    15.384615     4.728507     0.000000  \n",
       "max     100.000000    86.419753    99.058270    26.000000  1977.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProfileReport' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-05fe04409c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pandas Profiling Report'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplorative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ProfileReport' is not defined"
     ]
    }
   ],
   "source": [
    "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8bf19b848b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_notebook_iframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'profile' is not defined"
     ]
    }
   ],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values and Imputation Strategy\n",
    "As noted above, we addressed missing values in our dataset to improve our results. We opted to drop the 20 rows missing in the “subExcCt”, “subQuesCt”, and “subBlanks” columns. The missing data associated with these three columns was absent in the same rows; therefore, only 20 records were removed in addressing those columns. We felt this was an appropriate step given it represented only 0.2% of the data. Dropping these records also removed the seven missing values of “isYelling” and “subSpamWords.”\n",
    "\n",
    "The number of recipients (numRec) has the most missing values at 282. We decided to impute the missing values with the mean number of recipients. This impution was done after we split the data into training and testing datasets. The mean was calculated from the training dataset only. We felt this would most closely mimic a production environment and limit data leak between training and test sets. The remaining missing data point was one value for “noHost\"; we imputed this with the most frequently occurring value (mode) of False (0). **{IS THIS WHAT WE DID!??}{ OR WE CAN ASSIGN THE VALUE SINCE WE KNOW THE MISSING ONE IS SPAM ... I think all the SPAMs have a noHost of TRUE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              datatype  count of Nan\n",
      "Unnamed: 0       int64             0\n",
      "isSpam           int64             0\n",
      "isRe             int64             0\n",
      "underscore       int64             0\n",
      "priority         int64             0\n",
      "isInReplyTo      int64             0\n",
      "sortedRec        int64             0\n",
      "subPunc          int64             0\n",
      "multipartText    int64             0\n",
      "hasImages        int64             0\n",
      "isPGPsigned      int64             0\n",
      "subSpamWords   float64             7\n",
      "noHost         float64             1\n",
      "numEnd           int64             0\n",
      "isYelling      float64             7\n",
      "isOrigMsg        int64             0\n",
      "isDear           int64             0\n",
      "isWrote          int64             0\n",
      "numLines         int64             0\n",
      "bodyCharCt       int64             0\n",
      "subExcCt       float64            20\n",
      "subQuesCt      float64            20\n",
      "numAtt         float64             0\n",
      "numRec         float64           282\n",
      "perCaps        float64             0\n",
      "hour           float64             0\n",
      "perHTML        float64             0\n",
      "subBlanks      float64            20\n",
      "forwards       float64             0\n",
      "avgWordLen     float64             0\n",
      "numDlr           int64             0\n"
     ]
    }
   ],
   "source": [
    "# Missing Data Summary\n",
    "countOfNan = pd.Series(df.isnull().sum()) \n",
    "DataType = pd.Series(df.dtypes) \n",
    "  \n",
    "frame = { 'datatype': DataType, 'count of Nan': countOfNan } \n",
    "result = pd.DataFrame(frame) \n",
    "print(result)\n",
    "\n",
    "# remove NaNs\n",
    "dfNoNa=df[~df['subQuesCt'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Data Split, equivalent to R split()\n",
    "\n",
    "In the Python sklearn package, \"Statified Shuffle Split\" provides train/test indices to split data in train/test sets.\n",
    "This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html  \n",
    "\n",
    "The settings we use for training are 1 fold using eighty percent of the data for training. Twenty percent of the data will be used to test the initial model. \n",
    "\n",
    "** DID WE USE STRATIFIED SHUFFLE SPLIT OR JUST SHUFFLE SPLIT ?? It looks like we commented out Strat Shuffle Split. Why did we decide to do that? Why was Shuffle Split better than Strat Shuffle Split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=10, random_state=101, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# now divide the data into test and train using scikit learn built-ins\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cvx = ShuffleSplit(n_splits=10, test_size=0.20, random_state=101)\n",
    "#cv = StratifiedShuffleSplit(n_splits=10,train_size=0.8)\n",
    "print (cvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['isRe', 'underscore', 'priority', 'isInReplyTo',\n",
    "       'sortedRec', 'subPunc', 'multipartText', 'hasImages', 'isPGPsigned',\n",
    "       'subSpamWords', 'noHost', 'numEnd', 'isYelling', 'isOrigMsg', 'isDear',\n",
    "       'isWrote', 'numLines', 'bodyCharCt', 'subExcCt', 'subQuesCt', 'numAtt',\n",
    "       'numRec', 'perCaps', 'hour', 'perHTML', 'subBlanks', 'forwards',\n",
    "       'avgWordLen', 'numDlr']\n",
    "\n",
    "X = dfNoNa[features].copy()\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X2)\n",
    "\n",
    "#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n",
    "#Notice we scaled the data before the cross validation\n",
    "#X = scaler.transform(X2)\n",
    "\n",
    "Y= dfNoNa[['isSpam']].copy()\n",
    "y = Y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: BUILD/EVALUATE TREE-BASED MODEL FOR PREDICTING \"SPAM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline, complex model\n",
    "explain what we did, why we did this, summarize the findings.\n",
    "\n",
    "First the data is split into random train and test subsets, then a grid search mechanism is utilized to train models with various parmeters, score those models in terms of accuracy and return which model will provide the best accuracy.\n",
    "\n",
    "** RAN BASELINE MODEL -- with accuracy of 96%. can we get other stats about the baseline tree: how many leaves/levels, what were the results of the other metrics. \n",
    "\n",
    "** We could define the metrics to evaluate models here (before we run baseline) to be able to show the full change in results between base and our defined model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY BASELINE MODEL\n",
    "Accuracy 96.67% <br>\n",
    "WHAT ELSE -- how many layers? <br>\n",
    "What key variables? All used? <br>\n",
    "\n",
    "***WHAT IF WE USED GRID SEARCH AS OUR FIRST STEP TO UNDERSTAND WHAT PERFORMS BEST. USE THAT AS THE BASELINE, and say, great model but too complex. So we explored all the options output by grid search and created THIS model.\n",
    "\n",
    "If we go this route, we only have 2 models to describe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.9 ms\n",
      "accuracy: 0.9678456591639871\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\n",
    "\n",
    "#impute missing values for noHost and numRec using the mean of the training set\n",
    "X1_train.loc[X1_train['numRec'].isnull(),'numRec']=X1_train.numRec.mean(skipna = True)\n",
    "X1_test.loc[X1_test['numRec'].isnull(),'numRec']=X1_train.numRec.mean(skipna = True)\n",
    "X1_train.loc[X1_train['noHost'].isnull(),'noHost']=X1_train.noHost.mean(skipna = True)\n",
    "X1_test.loc[X1_test['noHost'].isnull(),'noHost']=X1_train.noHost.mean(skipna = True)\n",
    "\n",
    "#run base line decision tree classifier \n",
    "dt_clf = DecisionTreeClassifier()\n",
    "%time dt_clf.fit(X1_train,y1_train)\n",
    "yhat = dt_clf.predict(X1_test)\n",
    "print ('accuracy:', mt.accuracy_score(y1_test,yhat))\n",
    "print (dt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "min_samples_leaf_range = [2,3,4,5,6,7,8,9,10,11,15,20,30,40,50,75,100,150]\n",
    "# Create a dictionary of all the parameter options \n",
    "# Note has you can access the parameters of steps of a pipeline by using '__’\n",
    "\n",
    "\n",
    "\n",
    "parameters ={'classify__criterion':['gini','entropy'],\n",
    "             'classify__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "             'classify__min_samples_leaf': min_samples_leaf_range,\n",
    "             'classify__max_features': ['auto','auto', 'log2']}\n",
    "\n",
    "\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "                      (\"classify\",DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is doing a grid search, the cv=cvx is telling the grid search to split the training and testing data 10 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 8s\n",
      "accuracy: 0.9833869239013934\n",
      "{'classify__criterion': 'gini', 'classify__max_depth': 70, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "dt_clf = GridSearchCV(estimator, param_grid =parameters, cv=cvx)\n",
    "\n",
    "\n",
    "# train the decision tree algorithm\n",
    "%time dt_clf.fit(X,y)\n",
    "yhat = dt_clf.best_estimator_.predict(X1_test)\n",
    "print ('accuracy:', mt.accuracy_score(y1_test,yhat))\n",
    "print (dt_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commented as it was not doing the cv split. replaced with cell immediately above.\n",
    "#dt_clf = GridSearchCV(DecisionTreeClassifier(), param_grid =parameters)\n",
    "\n",
    "\n",
    "# train the decision tree algorithm\n",
    "#%time dt_clf.fit(X1_train,y1_train)\n",
    "#yhat = dt_clf.best_estimator_.predict(X1_test)\n",
    "#print ('accuracy:', mt.accuracy_score(y1_test,yhat))\n",
    "#print (dt_clf.best_params_)\n",
    "#Wall time: 1min 2s\n",
    "#accuracy: 0.9463792150359315\n",
    "#{'criterion': 'gini', 'max_depth': 120, 'max_features': 'auto', 'min_samples_leaf': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: EXPLAIN PARAMETERS INVOLVED IN \"TUNING\" MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain our decisions, how that impacted from base model\n",
    "Parameter definitions (if we need them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter                | Definition                                                                                                                                                                                  |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Criterion                | function to measure the quality of a split; supported criteria are \"gini\" for Gini impurity and \"entropy\" for information gain                                                              |\n",
    "| Splitter                 | strategy used to choose best split at each node; supported strategies are \"best\" to choose best split and \"random  to choose best random split                                              |\n",
    "| Max_depth                | maximum depth of tree; if None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples                                           |\n",
    "| Min_samples_split        | minimum number of samples required to split an internal node                                                                                                                                |\n",
    "| Min_samples_leaf         | minimum number of samples required at a leaf node; split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each left and right branches |\n",
    "| Min_weight_fraction_leaf | minimum weighted fraction of sum total of weights (for all input samples) required to be at a leaf node; samples have equal weight when sample_weight is not provided                       |\n",
    "| Max_features             | number of features to consider when looking for best split                                                                                                                                  |\n",
    "| Random_state             | controls randomness of estimator                                                                                                                                                            |\n",
    "| Max_leaf_nodes           | grow a tree with max_leaf_nodes in best-first fashion; best nodes defined as relative reduction in impurity                                                                                 |\n",
    "| Min_impurity_decrease    | node will be split if this split induces a decrease of impurity greater than/equal to this value                                                                                            |\n",
    "| Class_weight             | weights associated with classes                                                                                                                                                             |\n",
    "| Ccp_alpha                | complexity parameter used for Minimal Cost-Complexity Pruning                                                                                                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "estimatorsdf = pd.DataFrame(dt_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classify__criterion</th>\n",
       "      <th>param_classify__max_depth</th>\n",
       "      <th>param_classify__max_features</th>\n",
       "      <th>param_classify__min_samples_leaf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017106</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>4.001719e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859019</td>\n",
       "      <td>0.885687</td>\n",
       "      <td>0.843474</td>\n",
       "      <td>0.853122</td>\n",
       "      <td>0.847762</td>\n",
       "      <td>0.852452</td>\n",
       "      <td>0.860627</td>\n",
       "      <td>0.857947</td>\n",
       "      <td>0.852171</td>\n",
       "      <td>0.023556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>3.933949e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>3</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.878885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844814</td>\n",
       "      <td>0.861029</td>\n",
       "      <td>0.854463</td>\n",
       "      <td>0.854061</td>\n",
       "      <td>0.850978</td>\n",
       "      <td>0.871080</td>\n",
       "      <td>0.860225</td>\n",
       "      <td>0.797641</td>\n",
       "      <td>0.851233</td>\n",
       "      <td>0.019532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>4.427542e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.881029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806352</td>\n",
       "      <td>0.838381</td>\n",
       "      <td>0.831412</td>\n",
       "      <td>0.871750</td>\n",
       "      <td>0.821630</td>\n",
       "      <td>0.877111</td>\n",
       "      <td>0.848298</td>\n",
       "      <td>0.840391</td>\n",
       "      <td>0.842227</td>\n",
       "      <td>0.024725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014483</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>4.440328e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.881565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846824</td>\n",
       "      <td>0.875905</td>\n",
       "      <td>0.829402</td>\n",
       "      <td>0.855669</td>\n",
       "      <td>0.864380</td>\n",
       "      <td>0.820557</td>\n",
       "      <td>0.840257</td>\n",
       "      <td>0.859153</td>\n",
       "      <td>0.847682</td>\n",
       "      <td>0.019071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017057</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>4.377215e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>6</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.824759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869070</td>\n",
       "      <td>0.878719</td>\n",
       "      <td>0.863977</td>\n",
       "      <td>0.840525</td>\n",
       "      <td>0.832485</td>\n",
       "      <td>0.860627</td>\n",
       "      <td>0.871750</td>\n",
       "      <td>0.882605</td>\n",
       "      <td>0.858081</td>\n",
       "      <td>0.020361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016569</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>2.931534e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>7</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.858521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888904</td>\n",
       "      <td>0.788931</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.871348</td>\n",
       "      <td>0.867998</td>\n",
       "      <td>0.846288</td>\n",
       "      <td>0.851648</td>\n",
       "      <td>0.885687</td>\n",
       "      <td>0.857679</td>\n",
       "      <td>0.026431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>5.383609e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.858521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838649</td>\n",
       "      <td>0.854463</td>\n",
       "      <td>0.852586</td>\n",
       "      <td>0.839453</td>\n",
       "      <td>0.870812</td>\n",
       "      <td>0.866524</td>\n",
       "      <td>0.824578</td>\n",
       "      <td>0.859689</td>\n",
       "      <td>0.851126</td>\n",
       "      <td>0.013270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015854</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>3.951937e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>9</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.849946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879657</td>\n",
       "      <td>0.894130</td>\n",
       "      <td>0.830206</td>\n",
       "      <td>0.854329</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.869606</td>\n",
       "      <td>0.835835</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.852117</td>\n",
       "      <td>0.022857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.014967</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>4.982250e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.858521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884749</td>\n",
       "      <td>0.839989</td>\n",
       "      <td>0.856473</td>\n",
       "      <td>0.869070</td>\n",
       "      <td>0.856339</td>\n",
       "      <td>0.838247</td>\n",
       "      <td>0.811043</td>\n",
       "      <td>0.852586</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.018935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.022661</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>2.037102e-03</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>11</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.851018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820960</td>\n",
       "      <td>0.870410</td>\n",
       "      <td>0.863709</td>\n",
       "      <td>0.860627</td>\n",
       "      <td>0.868802</td>\n",
       "      <td>0.857277</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>0.824444</td>\n",
       "      <td>0.850750</td>\n",
       "      <td>0.019105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.028525</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>1.415460e-03</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>15</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.857449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875235</td>\n",
       "      <td>0.814527</td>\n",
       "      <td>0.825784</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>0.867730</td>\n",
       "      <td>0.869204</td>\n",
       "      <td>0.894666</td>\n",
       "      <td>0.844412</td>\n",
       "      <td>0.850348</td>\n",
       "      <td>0.028578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>1.559609e-03</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>20</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840123</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.867328</td>\n",
       "      <td>0.861297</td>\n",
       "      <td>0.852720</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.861029</td>\n",
       "      <td>0.852854</td>\n",
       "      <td>0.850804</td>\n",
       "      <td>0.020440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.029301</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>1.343031e-03</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>30</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.852090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867060</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.869204</td>\n",
       "      <td>0.799786</td>\n",
       "      <td>0.866256</td>\n",
       "      <td>0.857813</td>\n",
       "      <td>0.841061</td>\n",
       "      <td>0.864782</td>\n",
       "      <td>0.852573</td>\n",
       "      <td>0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.026728</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>1.354290e-03</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>40</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.847803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838649</td>\n",
       "      <td>0.838515</td>\n",
       "      <td>0.772447</td>\n",
       "      <td>0.848164</td>\n",
       "      <td>0.803404</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.875503</td>\n",
       "      <td>0.851246</td>\n",
       "      <td>0.836773</td>\n",
       "      <td>0.028017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.016263</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>6.304413e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.806002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837577</td>\n",
       "      <td>0.874832</td>\n",
       "      <td>0.844948</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.885419</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.842402</td>\n",
       "      <td>0.840123</td>\n",
       "      <td>0.844720</td>\n",
       "      <td>0.028548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>6.636965e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>75</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856205</td>\n",
       "      <td>0.835433</td>\n",
       "      <td>0.845752</td>\n",
       "      <td>0.864246</td>\n",
       "      <td>0.844010</td>\n",
       "      <td>0.809702</td>\n",
       "      <td>0.837711</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.833141</td>\n",
       "      <td>0.019739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>3.248066e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.859057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859555</td>\n",
       "      <td>0.836103</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.864782</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.858483</td>\n",
       "      <td>0.860091</td>\n",
       "      <td>0.838649</td>\n",
       "      <td>0.853243</td>\n",
       "      <td>0.010039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>5.216806e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>150</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.866024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.816939</td>\n",
       "      <td>0.885151</td>\n",
       "      <td>0.847896</td>\n",
       "      <td>0.837979</td>\n",
       "      <td>0.831815</td>\n",
       "      <td>0.886090</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.855535</td>\n",
       "      <td>0.022286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>2.940062e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.860129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863307</td>\n",
       "      <td>0.871616</td>\n",
       "      <td>0.855937</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.843340</td>\n",
       "      <td>0.845082</td>\n",
       "      <td>0.852452</td>\n",
       "      <td>0.866524</td>\n",
       "      <td>0.856580</td>\n",
       "      <td>0.012214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.014963</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>3.946889e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>3</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.868167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861565</td>\n",
       "      <td>0.882873</td>\n",
       "      <td>0.821094</td>\n",
       "      <td>0.863173</td>\n",
       "      <td>0.863441</td>\n",
       "      <td>0.822970</td>\n",
       "      <td>0.857947</td>\n",
       "      <td>0.855669</td>\n",
       "      <td>0.856138</td>\n",
       "      <td>0.018406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>4.598126e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.869239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842938</td>\n",
       "      <td>0.879523</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845350</td>\n",
       "      <td>0.802466</td>\n",
       "      <td>0.857813</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.876173</td>\n",
       "      <td>0.857210</td>\n",
       "      <td>0.022120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.015576</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>5.920224e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.856377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882739</td>\n",
       "      <td>0.870946</td>\n",
       "      <td>0.867864</td>\n",
       "      <td>0.864916</td>\n",
       "      <td>0.874564</td>\n",
       "      <td>0.889440</td>\n",
       "      <td>0.867328</td>\n",
       "      <td>0.854597</td>\n",
       "      <td>0.868252</td>\n",
       "      <td>0.012163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.014392</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>4.451606e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>6</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838515</td>\n",
       "      <td>0.870276</td>\n",
       "      <td>0.822032</td>\n",
       "      <td>0.830608</td>\n",
       "      <td>0.842938</td>\n",
       "      <td>0.869740</td>\n",
       "      <td>0.876575</td>\n",
       "      <td>0.818413</td>\n",
       "      <td>0.850335</td>\n",
       "      <td>0.021492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>2.975997e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>7</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.836013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816671</td>\n",
       "      <td>0.802064</td>\n",
       "      <td>0.847494</td>\n",
       "      <td>0.828598</td>\n",
       "      <td>0.863977</td>\n",
       "      <td>0.854329</td>\n",
       "      <td>0.833557</td>\n",
       "      <td>0.862771</td>\n",
       "      <td>0.840056</td>\n",
       "      <td>0.021895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.017121</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>5.978964e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.878349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880595</td>\n",
       "      <td>0.818011</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.877111</td>\n",
       "      <td>0.872018</td>\n",
       "      <td>0.865988</td>\n",
       "      <td>0.876575</td>\n",
       "      <td>0.868668</td>\n",
       "      <td>0.867140</td>\n",
       "      <td>0.016928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.017852</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>6.973513e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>9</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.862272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.786384</td>\n",
       "      <td>0.869740</td>\n",
       "      <td>0.861431</td>\n",
       "      <td>0.866256</td>\n",
       "      <td>0.861297</td>\n",
       "      <td>0.869338</td>\n",
       "      <td>0.862503</td>\n",
       "      <td>0.855481</td>\n",
       "      <td>0.023349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>3.974461e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854597</td>\n",
       "      <td>0.827928</td>\n",
       "      <td>0.851246</td>\n",
       "      <td>0.871214</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.855401</td>\n",
       "      <td>0.858483</td>\n",
       "      <td>0.861297</td>\n",
       "      <td>0.851260</td>\n",
       "      <td>0.014832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.015047</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>2.926768e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>11</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.853162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873492</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.845484</td>\n",
       "      <td>0.858215</td>\n",
       "      <td>0.862235</td>\n",
       "      <td>0.877111</td>\n",
       "      <td>0.848298</td>\n",
       "      <td>0.855856</td>\n",
       "      <td>0.016656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.015425</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>3.010398e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>15</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.870311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883945</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.861431</td>\n",
       "      <td>0.855267</td>\n",
       "      <td>0.808630</td>\n",
       "      <td>0.833021</td>\n",
       "      <td>0.870410</td>\n",
       "      <td>0.865318</td>\n",
       "      <td>0.851635</td>\n",
       "      <td>0.023643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>2.914565e-04</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>20</td>\n",
       "      <td>{'classify__criterion': 'gini', 'classify__max...</td>\n",
       "      <td>0.871383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839855</td>\n",
       "      <td>0.860493</td>\n",
       "      <td>0.863039</td>\n",
       "      <td>0.850844</td>\n",
       "      <td>0.888234</td>\n",
       "      <td>0.885821</td>\n",
       "      <td>0.861029</td>\n",
       "      <td>0.865050</td>\n",
       "      <td>0.863629</td>\n",
       "      <td>0.014066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>0.020146</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.990538e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951354</td>\n",
       "      <td>0.950817</td>\n",
       "      <td>0.947601</td>\n",
       "      <td>0.943447</td>\n",
       "      <td>0.942241</td>\n",
       "      <td>0.938354</td>\n",
       "      <td>0.940767</td>\n",
       "      <td>0.941437</td>\n",
       "      <td>0.944921</td>\n",
       "      <td>0.004172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>2.989452e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>9</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.929796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952292</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.939560</td>\n",
       "      <td>0.943313</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.949611</td>\n",
       "      <td>0.946663</td>\n",
       "      <td>0.941584</td>\n",
       "      <td>0.007327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>0.019856</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>9.464947e-07</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.923365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937014</td>\n",
       "      <td>0.945189</td>\n",
       "      <td>0.940901</td>\n",
       "      <td>0.937684</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.937952</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.946127</td>\n",
       "      <td>0.941195</td>\n",
       "      <td>0.003309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>0.019351</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.991498e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>11</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.932476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931654</td>\n",
       "      <td>0.939292</td>\n",
       "      <td>0.947601</td>\n",
       "      <td>0.936210</td>\n",
       "      <td>0.942107</td>\n",
       "      <td>0.939158</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.933262</td>\n",
       "      <td>0.938783</td>\n",
       "      <td>0.004375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>2.991681e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>15</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.922294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929241</td>\n",
       "      <td>0.925757</td>\n",
       "      <td>0.913294</td>\n",
       "      <td>0.939158</td>\n",
       "      <td>0.928035</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.937014</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.927808</td>\n",
       "      <td>0.006747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>2.980023e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>20</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.918006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931520</td>\n",
       "      <td>0.915706</td>\n",
       "      <td>0.922273</td>\n",
       "      <td>0.924819</td>\n",
       "      <td>0.933932</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.928437</td>\n",
       "      <td>0.925972</td>\n",
       "      <td>0.006148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>0.018155</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>5.852780e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>30</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.935155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.915304</td>\n",
       "      <td>0.915706</td>\n",
       "      <td>0.907531</td>\n",
       "      <td>0.904449</td>\n",
       "      <td>0.917716</td>\n",
       "      <td>0.891316</td>\n",
       "      <td>0.906459</td>\n",
       "      <td>0.910694</td>\n",
       "      <td>0.009879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0.017253</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>4.886653e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>40</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.924437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.896945</td>\n",
       "      <td>0.904717</td>\n",
       "      <td>0.910614</td>\n",
       "      <td>0.888234</td>\n",
       "      <td>0.893996</td>\n",
       "      <td>0.906593</td>\n",
       "      <td>0.906727</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.900817</td>\n",
       "      <td>0.010680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0.017565</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>1.236333e-06</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.904073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917046</td>\n",
       "      <td>0.909140</td>\n",
       "      <td>0.903511</td>\n",
       "      <td>0.900965</td>\n",
       "      <td>0.904181</td>\n",
       "      <td>0.899625</td>\n",
       "      <td>0.905923</td>\n",
       "      <td>0.915974</td>\n",
       "      <td>0.905803</td>\n",
       "      <td>0.006050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.989107e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>75</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.890139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901367</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.877513</td>\n",
       "      <td>0.891718</td>\n",
       "      <td>0.896676</td>\n",
       "      <td>0.879389</td>\n",
       "      <td>0.883543</td>\n",
       "      <td>0.886090</td>\n",
       "      <td>0.887537</td>\n",
       "      <td>0.007276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>3.902954e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.874598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872420</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>0.873492</td>\n",
       "      <td>0.900831</td>\n",
       "      <td>0.884079</td>\n",
       "      <td>0.889172</td>\n",
       "      <td>0.879389</td>\n",
       "      <td>0.873492</td>\n",
       "      <td>0.881171</td>\n",
       "      <td>0.010911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>0.016184</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>4.859624e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>auto</td>\n",
       "      <td>150</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.875670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875369</td>\n",
       "      <td>0.863977</td>\n",
       "      <td>0.842536</td>\n",
       "      <td>0.865452</td>\n",
       "      <td>0.829402</td>\n",
       "      <td>0.876977</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.896542</td>\n",
       "      <td>0.866685</td>\n",
       "      <td>0.017946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>4.982771e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.956056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.980836</td>\n",
       "      <td>0.979094</td>\n",
       "      <td>0.972795</td>\n",
       "      <td>0.980568</td>\n",
       "      <td>0.974538</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.972661</td>\n",
       "      <td>0.977901</td>\n",
       "      <td>0.003777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>0.019352</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>4.545529e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.929260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962209</td>\n",
       "      <td>0.961404</td>\n",
       "      <td>0.957116</td>\n",
       "      <td>0.965023</td>\n",
       "      <td>0.966765</td>\n",
       "      <td>0.963683</td>\n",
       "      <td>0.955776</td>\n",
       "      <td>0.961806</td>\n",
       "      <td>0.960614</td>\n",
       "      <td>0.004108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>0.019645</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>4.510455e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.929796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953230</td>\n",
       "      <td>0.962075</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.958188</td>\n",
       "      <td>0.960734</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.955508</td>\n",
       "      <td>0.951488</td>\n",
       "      <td>0.957451</td>\n",
       "      <td>0.003593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>4.885681e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.942122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953632</td>\n",
       "      <td>0.958724</td>\n",
       "      <td>0.953900</td>\n",
       "      <td>0.951756</td>\n",
       "      <td>0.950683</td>\n",
       "      <td>0.956848</td>\n",
       "      <td>0.950147</td>\n",
       "      <td>0.951488</td>\n",
       "      <td>0.953270</td>\n",
       "      <td>0.002981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>0.018849</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>6.308644e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>6</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.943194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952828</td>\n",
       "      <td>0.933664</td>\n",
       "      <td>0.937952</td>\n",
       "      <td>0.950683</td>\n",
       "      <td>0.950147</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.945859</td>\n",
       "      <td>0.949209</td>\n",
       "      <td>0.947025</td>\n",
       "      <td>0.006636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>2.980815e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.912111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936344</td>\n",
       "      <td>0.934468</td>\n",
       "      <td>0.942777</td>\n",
       "      <td>0.949477</td>\n",
       "      <td>0.943983</td>\n",
       "      <td>0.947735</td>\n",
       "      <td>0.945725</td>\n",
       "      <td>0.939292</td>\n",
       "      <td>0.941973</td>\n",
       "      <td>0.004896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>2.995651e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.919614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946529</td>\n",
       "      <td>0.938086</td>\n",
       "      <td>0.928973</td>\n",
       "      <td>0.941169</td>\n",
       "      <td>0.940901</td>\n",
       "      <td>0.924417</td>\n",
       "      <td>0.927633</td>\n",
       "      <td>0.941169</td>\n",
       "      <td>0.935862</td>\n",
       "      <td>0.007018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>0.017611</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>3.012886e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>9</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.920150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928973</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.942643</td>\n",
       "      <td>0.944921</td>\n",
       "      <td>0.928705</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.942107</td>\n",
       "      <td>0.930046</td>\n",
       "      <td>0.933704</td>\n",
       "      <td>0.007635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>0.018053</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>4.897300e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.929260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917582</td>\n",
       "      <td>0.941035</td>\n",
       "      <td>0.936076</td>\n",
       "      <td>0.925891</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.932726</td>\n",
       "      <td>0.941437</td>\n",
       "      <td>0.936880</td>\n",
       "      <td>0.933128</td>\n",
       "      <td>0.006754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>4.006762e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>11</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.940514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935272</td>\n",
       "      <td>0.934602</td>\n",
       "      <td>0.934736</td>\n",
       "      <td>0.927231</td>\n",
       "      <td>0.924149</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.932324</td>\n",
       "      <td>0.924685</td>\n",
       "      <td>0.932391</td>\n",
       "      <td>0.005546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>0.017230</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.989114e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>15</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.922294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913830</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.920665</td>\n",
       "      <td>0.931922</td>\n",
       "      <td>0.920933</td>\n",
       "      <td>0.915438</td>\n",
       "      <td>0.927633</td>\n",
       "      <td>0.931922</td>\n",
       "      <td>0.924350</td>\n",
       "      <td>0.006275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>0.017207</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>4.000271e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.906217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920263</td>\n",
       "      <td>0.894666</td>\n",
       "      <td>0.908470</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.903779</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.913026</td>\n",
       "      <td>0.913133</td>\n",
       "      <td>0.008620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>0.016907</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>3.997693e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>30</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.896034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913830</td>\n",
       "      <td>0.913830</td>\n",
       "      <td>0.913696</td>\n",
       "      <td>0.905253</td>\n",
       "      <td>0.905923</td>\n",
       "      <td>0.917984</td>\n",
       "      <td>0.920263</td>\n",
       "      <td>0.898687</td>\n",
       "      <td>0.909274</td>\n",
       "      <td>0.007232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>0.016593</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>4.590937e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>40</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.901393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921335</td>\n",
       "      <td>0.908604</td>\n",
       "      <td>0.905521</td>\n",
       "      <td>0.889306</td>\n",
       "      <td>0.914366</td>\n",
       "      <td>0.891048</td>\n",
       "      <td>0.909274</td>\n",
       "      <td>0.904717</td>\n",
       "      <td>0.905052</td>\n",
       "      <td>0.009198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>3.922540e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.891747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897615</td>\n",
       "      <td>0.883811</td>\n",
       "      <td>0.861297</td>\n",
       "      <td>0.894398</td>\n",
       "      <td>0.883409</td>\n",
       "      <td>0.897213</td>\n",
       "      <td>0.883409</td>\n",
       "      <td>0.897079</td>\n",
       "      <td>0.888998</td>\n",
       "      <td>0.011301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>0.016145</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>5.386765e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>75</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869874</td>\n",
       "      <td>0.869606</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>0.880863</td>\n",
       "      <td>0.888234</td>\n",
       "      <td>0.895872</td>\n",
       "      <td>0.883409</td>\n",
       "      <td>0.879657</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.007951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>0.015094</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>1.093364e-05</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.867631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898419</td>\n",
       "      <td>0.871348</td>\n",
       "      <td>0.854865</td>\n",
       "      <td>0.876709</td>\n",
       "      <td>0.831144</td>\n",
       "      <td>0.876173</td>\n",
       "      <td>0.888502</td>\n",
       "      <td>0.854061</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>0.014860</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>4.570694e-04</td>\n",
       "      <td>entropy</td>\n",
       "      <td>150</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>{'classify__criterion': 'entropy', 'classify__...</td>\n",
       "      <td>0.884780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843206</td>\n",
       "      <td>0.859153</td>\n",
       "      <td>0.847628</td>\n",
       "      <td>0.856205</td>\n",
       "      <td>0.864648</td>\n",
       "      <td>0.861833</td>\n",
       "      <td>0.846288</td>\n",
       "      <td>0.857277</td>\n",
       "      <td>0.859327</td>\n",
       "      <td>0.013122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1944 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0          0.017106      0.006873         0.001801    4.001719e-04   \n",
       "1          0.014724      0.000774         0.001795    3.933949e-04   \n",
       "2          0.015882      0.000814         0.001701    4.427542e-04   \n",
       "3          0.014483      0.001111         0.001700    4.440328e-04   \n",
       "4          0.017057      0.001445         0.002007    4.377215e-04   \n",
       "5          0.016569      0.001009         0.001907    2.931534e-04   \n",
       "6          0.015668      0.000795         0.001897    5.383609e-04   \n",
       "7          0.015854      0.001043         0.001804    3.951937e-04   \n",
       "8          0.014967      0.000777         0.001496    4.982250e-04   \n",
       "9          0.022661      0.007073         0.003296    2.037102e-03   \n",
       "10         0.028525      0.002326         0.003691    1.415460e-03   \n",
       "11         0.027826      0.001695         0.003393    1.559609e-03   \n",
       "12         0.029301      0.000988         0.003293    1.343031e-03   \n",
       "13         0.026728      0.002304         0.004391    1.354290e-03   \n",
       "14         0.016263      0.001787         0.001992    6.304413e-04   \n",
       "15         0.014373      0.000667         0.001594    6.636965e-04   \n",
       "16         0.015771      0.002261         0.002153    3.248066e-04   \n",
       "17         0.015870      0.000917         0.001893    5.216806e-04   \n",
       "18         0.016060      0.001037         0.001911    2.940062e-04   \n",
       "19         0.014963      0.001167         0.001803    3.946889e-04   \n",
       "20         0.014876      0.001506         0.001699    4.598126e-04   \n",
       "21         0.015576      0.002284         0.001797    5.920224e-04   \n",
       "22         0.014392      0.000793         0.001706    4.451606e-04   \n",
       "23         0.016229      0.002132         0.002101    2.975997e-04   \n",
       "24         0.017121      0.001350         0.002194    5.978964e-04   \n",
       "25         0.017852      0.002205         0.002095    6.973513e-04   \n",
       "26         0.015195      0.000969         0.001792    3.974461e-04   \n",
       "27         0.015047      0.001210         0.001906    2.926768e-04   \n",
       "28         0.015425      0.001100         0.001900    3.010398e-04   \n",
       "29         0.016755      0.001534         0.001902    2.914565e-04   \n",
       "...             ...           ...              ...             ...   \n",
       "1914       0.020146      0.001882         0.001796    3.990538e-04   \n",
       "1915       0.019747      0.000869         0.001896    2.989452e-04   \n",
       "1916       0.019856      0.001752         0.001996    9.464947e-07   \n",
       "1917       0.019351      0.001622         0.001796    3.991498e-04   \n",
       "1918       0.018450      0.001022         0.001896    2.991681e-04   \n",
       "1919       0.019152      0.000985         0.001892    2.980023e-04   \n",
       "1920       0.018155      0.001473         0.001796    5.852780e-04   \n",
       "1921       0.017253      0.001548         0.001597    4.886653e-04   \n",
       "1922       0.017565      0.001564         0.001995    1.236333e-06   \n",
       "1923       0.016854      0.001127         0.001796    3.989107e-04   \n",
       "1924       0.015758      0.001074         0.001801    3.902954e-04   \n",
       "1925       0.016184      0.001408         0.001401    4.859624e-04   \n",
       "1926       0.019060      0.000705         0.001503    4.982771e-04   \n",
       "1927       0.019352      0.000916         0.001702    4.545529e-04   \n",
       "1928       0.019645      0.001247         0.001999    4.510455e-04   \n",
       "1929       0.019647      0.002858         0.001597    4.885681e-04   \n",
       "1930       0.018849      0.001041         0.001996    6.308644e-04   \n",
       "1931       0.018753      0.000750         0.001892    2.980815e-04   \n",
       "1932       0.018750      0.001323         0.001896    2.995651e-04   \n",
       "1933       0.017611      0.001721         0.001900    3.012886e-04   \n",
       "1934       0.018053      0.001297         0.001598    4.897300e-04   \n",
       "1935       0.018450      0.000499         0.001800    4.006762e-04   \n",
       "1936       0.017230      0.001322         0.001796    3.989114e-04   \n",
       "1937       0.017207      0.000824         0.001798    4.000271e-04   \n",
       "1938       0.016907      0.001031         0.001797    3.997693e-04   \n",
       "1939       0.016593      0.001146         0.001709    4.590937e-04   \n",
       "1940       0.015713      0.001184         0.001799    3.922540e-04   \n",
       "1941       0.016145      0.000875         0.001656    5.386765e-04   \n",
       "1942       0.015094      0.000961         0.002001    1.093364e-05   \n",
       "1943       0.014860      0.000698         0.001696    4.570694e-04   \n",
       "\n",
       "     param_classify__criterion param_classify__max_depth  \\\n",
       "0                         gini                         4   \n",
       "1                         gini                         4   \n",
       "2                         gini                         4   \n",
       "3                         gini                         4   \n",
       "4                         gini                         4   \n",
       "5                         gini                         4   \n",
       "6                         gini                         4   \n",
       "7                         gini                         4   \n",
       "8                         gini                         4   \n",
       "9                         gini                         4   \n",
       "10                        gini                         4   \n",
       "11                        gini                         4   \n",
       "12                        gini                         4   \n",
       "13                        gini                         4   \n",
       "14                        gini                         4   \n",
       "15                        gini                         4   \n",
       "16                        gini                         4   \n",
       "17                        gini                         4   \n",
       "18                        gini                         4   \n",
       "19                        gini                         4   \n",
       "20                        gini                         4   \n",
       "21                        gini                         4   \n",
       "22                        gini                         4   \n",
       "23                        gini                         4   \n",
       "24                        gini                         4   \n",
       "25                        gini                         4   \n",
       "26                        gini                         4   \n",
       "27                        gini                         4   \n",
       "28                        gini                         4   \n",
       "29                        gini                         4   \n",
       "...                        ...                       ...   \n",
       "1914                   entropy                       150   \n",
       "1915                   entropy                       150   \n",
       "1916                   entropy                       150   \n",
       "1917                   entropy                       150   \n",
       "1918                   entropy                       150   \n",
       "1919                   entropy                       150   \n",
       "1920                   entropy                       150   \n",
       "1921                   entropy                       150   \n",
       "1922                   entropy                       150   \n",
       "1923                   entropy                       150   \n",
       "1924                   entropy                       150   \n",
       "1925                   entropy                       150   \n",
       "1926                   entropy                       150   \n",
       "1927                   entropy                       150   \n",
       "1928                   entropy                       150   \n",
       "1929                   entropy                       150   \n",
       "1930                   entropy                       150   \n",
       "1931                   entropy                       150   \n",
       "1932                   entropy                       150   \n",
       "1933                   entropy                       150   \n",
       "1934                   entropy                       150   \n",
       "1935                   entropy                       150   \n",
       "1936                   entropy                       150   \n",
       "1937                   entropy                       150   \n",
       "1938                   entropy                       150   \n",
       "1939                   entropy                       150   \n",
       "1940                   entropy                       150   \n",
       "1941                   entropy                       150   \n",
       "1942                   entropy                       150   \n",
       "1943                   entropy                       150   \n",
       "\n",
       "     param_classify__max_features param_classify__min_samples_leaf  \\\n",
       "0                            auto                                2   \n",
       "1                            auto                                3   \n",
       "2                            auto                                4   \n",
       "3                            auto                                5   \n",
       "4                            auto                                6   \n",
       "5                            auto                                7   \n",
       "6                            auto                                8   \n",
       "7                            auto                                9   \n",
       "8                            auto                               10   \n",
       "9                            auto                               11   \n",
       "10                           auto                               15   \n",
       "11                           auto                               20   \n",
       "12                           auto                               30   \n",
       "13                           auto                               40   \n",
       "14                           auto                               50   \n",
       "15                           auto                               75   \n",
       "16                           auto                              100   \n",
       "17                           auto                              150   \n",
       "18                           auto                                2   \n",
       "19                           auto                                3   \n",
       "20                           auto                                4   \n",
       "21                           auto                                5   \n",
       "22                           auto                                6   \n",
       "23                           auto                                7   \n",
       "24                           auto                                8   \n",
       "25                           auto                                9   \n",
       "26                           auto                               10   \n",
       "27                           auto                               11   \n",
       "28                           auto                               15   \n",
       "29                           auto                               20   \n",
       "...                           ...                              ...   \n",
       "1914                         auto                                8   \n",
       "1915                         auto                                9   \n",
       "1916                         auto                               10   \n",
       "1917                         auto                               11   \n",
       "1918                         auto                               15   \n",
       "1919                         auto                               20   \n",
       "1920                         auto                               30   \n",
       "1921                         auto                               40   \n",
       "1922                         auto                               50   \n",
       "1923                         auto                               75   \n",
       "1924                         auto                              100   \n",
       "1925                         auto                              150   \n",
       "1926                         log2                                2   \n",
       "1927                         log2                                3   \n",
       "1928                         log2                                4   \n",
       "1929                         log2                                5   \n",
       "1930                         log2                                6   \n",
       "1931                         log2                                7   \n",
       "1932                         log2                                8   \n",
       "1933                         log2                                9   \n",
       "1934                         log2                               10   \n",
       "1935                         log2                               11   \n",
       "1936                         log2                               15   \n",
       "1937                         log2                               20   \n",
       "1938                         log2                               30   \n",
       "1939                         log2                               40   \n",
       "1940                         log2                               50   \n",
       "1941                         log2                               75   \n",
       "1942                         log2                              100   \n",
       "1943                         log2                              150   \n",
       "\n",
       "                                                 params  split0_test_score  \\\n",
       "0     {'classify__criterion': 'gini', 'classify__max...           0.874598   \n",
       "1     {'classify__criterion': 'gini', 'classify__max...           0.878885   \n",
       "2     {'classify__criterion': 'gini', 'classify__max...           0.881029   \n",
       "3     {'classify__criterion': 'gini', 'classify__max...           0.881565   \n",
       "4     {'classify__criterion': 'gini', 'classify__max...           0.824759   \n",
       "5     {'classify__criterion': 'gini', 'classify__max...           0.858521   \n",
       "6     {'classify__criterion': 'gini', 'classify__max...           0.858521   \n",
       "7     {'classify__criterion': 'gini', 'classify__max...           0.849946   \n",
       "8     {'classify__criterion': 'gini', 'classify__max...           0.858521   \n",
       "9     {'classify__criterion': 'gini', 'classify__max...           0.851018   \n",
       "10    {'classify__criterion': 'gini', 'classify__max...           0.857449   \n",
       "11    {'classify__criterion': 'gini', 'classify__max...           0.863880   \n",
       "12    {'classify__criterion': 'gini', 'classify__max...           0.852090   \n",
       "13    {'classify__criterion': 'gini', 'classify__max...           0.847803   \n",
       "14    {'classify__criterion': 'gini', 'classify__max...           0.806002   \n",
       "15    {'classify__criterion': 'gini', 'classify__max...           0.833333   \n",
       "16    {'classify__criterion': 'gini', 'classify__max...           0.859057   \n",
       "17    {'classify__criterion': 'gini', 'classify__max...           0.866024   \n",
       "18    {'classify__criterion': 'gini', 'classify__max...           0.860129   \n",
       "19    {'classify__criterion': 'gini', 'classify__max...           0.868167   \n",
       "20    {'classify__criterion': 'gini', 'classify__max...           0.869239   \n",
       "21    {'classify__criterion': 'gini', 'classify__max...           0.856377   \n",
       "22    {'classify__criterion': 'gini', 'classify__max...           0.863880   \n",
       "23    {'classify__criterion': 'gini', 'classify__max...           0.836013   \n",
       "24    {'classify__criterion': 'gini', 'classify__max...           0.878349   \n",
       "25    {'classify__criterion': 'gini', 'classify__max...           0.862272   \n",
       "26    {'classify__criterion': 'gini', 'classify__max...           0.863880   \n",
       "27    {'classify__criterion': 'gini', 'classify__max...           0.853162   \n",
       "28    {'classify__criterion': 'gini', 'classify__max...           0.870311   \n",
       "29    {'classify__criterion': 'gini', 'classify__max...           0.871383   \n",
       "...                                                 ...                ...   \n",
       "1914  {'classify__criterion': 'entropy', 'classify__...           0.941586   \n",
       "1915  {'classify__criterion': 'entropy', 'classify__...           0.929796   \n",
       "1916  {'classify__criterion': 'entropy', 'classify__...           0.923365   \n",
       "1917  {'classify__criterion': 'entropy', 'classify__...           0.932476   \n",
       "1918  {'classify__criterion': 'entropy', 'classify__...           0.922294   \n",
       "1919  {'classify__criterion': 'entropy', 'classify__...           0.918006   \n",
       "1920  {'classify__criterion': 'entropy', 'classify__...           0.935155   \n",
       "1921  {'classify__criterion': 'entropy', 'classify__...           0.924437   \n",
       "1922  {'classify__criterion': 'entropy', 'classify__...           0.904073   \n",
       "1923  {'classify__criterion': 'entropy', 'classify__...           0.890139   \n",
       "1924  {'classify__criterion': 'entropy', 'classify__...           0.874598   \n",
       "1925  {'classify__criterion': 'entropy', 'classify__...           0.875670   \n",
       "1926  {'classify__criterion': 'entropy', 'classify__...           0.956056   \n",
       "1927  {'classify__criterion': 'entropy', 'classify__...           0.929260   \n",
       "1928  {'classify__criterion': 'entropy', 'classify__...           0.929796   \n",
       "1929  {'classify__criterion': 'entropy', 'classify__...           0.942122   \n",
       "1930  {'classify__criterion': 'entropy', 'classify__...           0.943194   \n",
       "1931  {'classify__criterion': 'entropy', 'classify__...           0.912111   \n",
       "1932  {'classify__criterion': 'entropy', 'classify__...           0.919614   \n",
       "1933  {'classify__criterion': 'entropy', 'classify__...           0.920150   \n",
       "1934  {'classify__criterion': 'entropy', 'classify__...           0.929260   \n",
       "1935  {'classify__criterion': 'entropy', 'classify__...           0.940514   \n",
       "1936  {'classify__criterion': 'entropy', 'classify__...           0.922294   \n",
       "1937  {'classify__criterion': 'entropy', 'classify__...           0.906217   \n",
       "1938  {'classify__criterion': 'entropy', 'classify__...           0.896034   \n",
       "1939  {'classify__criterion': 'entropy', 'classify__...           0.901393   \n",
       "1940  {'classify__criterion': 'entropy', 'classify__...           0.891747   \n",
       "1941  {'classify__criterion': 'entropy', 'classify__...           0.891211   \n",
       "1942  {'classify__criterion': 'entropy', 'classify__...           0.867631   \n",
       "1943  {'classify__criterion': 'entropy', 'classify__...           0.884780   \n",
       "\n",
       "      ...  split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0     ...            0.859019            0.885687            0.843474   \n",
       "1     ...            0.844814            0.861029            0.854463   \n",
       "2     ...            0.806352            0.838381            0.831412   \n",
       "3     ...            0.846824            0.875905            0.829402   \n",
       "4     ...            0.869070            0.878719            0.863977   \n",
       "5     ...            0.888904            0.788931            0.857143   \n",
       "6     ...            0.838649            0.854463            0.852586   \n",
       "7     ...            0.879657            0.894130            0.830206   \n",
       "8     ...            0.884749            0.839989            0.856473   \n",
       "9     ...            0.820960            0.870410            0.863709   \n",
       "10    ...            0.875235            0.814527            0.825784   \n",
       "11    ...            0.840123            0.862637            0.867328   \n",
       "12    ...            0.867060            0.858349            0.869204   \n",
       "13    ...            0.838649            0.838515            0.772447   \n",
       "14    ...            0.837577            0.874832            0.844948   \n",
       "15    ...            0.856205            0.835433            0.845752   \n",
       "16    ...            0.859555            0.836103            0.853659   \n",
       "17    ...            0.865854            0.816939            0.885151   \n",
       "18    ...            0.863307            0.871616            0.855937   \n",
       "19    ...            0.861565            0.882873            0.821094   \n",
       "20    ...            0.842938            0.879523            0.857143   \n",
       "21    ...            0.882739            0.870946            0.867864   \n",
       "22    ...            0.838515            0.870276            0.822032   \n",
       "23    ...            0.816671            0.802064            0.847494   \n",
       "24    ...            0.880595            0.818011            0.871884   \n",
       "25    ...            0.858349            0.786384            0.869740   \n",
       "26    ...            0.854597            0.827928            0.851246   \n",
       "27    ...            0.873492            0.829000            0.845886   \n",
       "28    ...            0.883945            0.815331            0.861431   \n",
       "29    ...            0.839855            0.860493            0.863039   \n",
       "...   ...                 ...                 ...                 ...   \n",
       "1914  ...            0.951354            0.950817            0.947601   \n",
       "1915  ...            0.952292            0.930314            0.928169   \n",
       "1916  ...            0.937014            0.945189            0.940901   \n",
       "1917  ...            0.931654            0.939292            0.947601   \n",
       "1918  ...            0.929241            0.925757            0.913294   \n",
       "1919  ...            0.931520            0.915706            0.922273   \n",
       "1920  ...            0.914634            0.915304            0.915706   \n",
       "1921  ...            0.896945            0.904717            0.910614   \n",
       "1922  ...            0.917046            0.909140            0.903511   \n",
       "1923  ...            0.901367            0.880729            0.877513   \n",
       "1924  ...            0.872420            0.868936            0.873492   \n",
       "1925  ...            0.875369            0.863977            0.842536   \n",
       "1926  ...            0.975610            0.980836            0.979094   \n",
       "1927  ...            0.962209            0.961404            0.957116   \n",
       "1928  ...            0.953230            0.962075            0.963415   \n",
       "1929  ...            0.953632            0.958724            0.953900   \n",
       "1930  ...            0.952828            0.933664            0.937952   \n",
       "1931  ...            0.936344            0.934468            0.942777   \n",
       "1932  ...            0.946529            0.938086            0.928973   \n",
       "1933  ...            0.928973            0.939024            0.942643   \n",
       "1934  ...            0.917582            0.941035            0.936076   \n",
       "1935  ...            0.935272            0.934602            0.934736   \n",
       "1936  ...            0.913830            0.928169            0.920665   \n",
       "1937  ...            0.920263            0.894666            0.908470   \n",
       "1938  ...            0.913830            0.913830            0.913696   \n",
       "1939  ...            0.921335            0.908604            0.905521   \n",
       "1940  ...            0.897615            0.883811            0.861297   \n",
       "1941  ...            0.869874            0.869606            0.887698   \n",
       "1942  ...            0.898419            0.871348            0.854865   \n",
       "1943  ...            0.843206            0.859153            0.847628   \n",
       "\n",
       "      split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0               0.853122            0.847762            0.852452   \n",
       "1               0.854061            0.850978            0.871080   \n",
       "2               0.871750            0.821630            0.877111   \n",
       "3               0.855669            0.864380            0.820557   \n",
       "4               0.840525            0.832485            0.860627   \n",
       "5               0.871348            0.867998            0.846288   \n",
       "6               0.839453            0.870812            0.866524   \n",
       "7               0.854329            0.853659            0.869606   \n",
       "8               0.869070            0.856339            0.838247   \n",
       "9               0.860627            0.868802            0.857277   \n",
       "10              0.797105            0.867730            0.869204   \n",
       "11              0.861297            0.852720            0.794559   \n",
       "12              0.799786            0.866256            0.857813   \n",
       "13              0.848164            0.803404            0.841463   \n",
       "14              0.802600            0.885419            0.862637   \n",
       "15              0.864246            0.844010            0.809702   \n",
       "16              0.864782            0.841463            0.858483   \n",
       "17              0.847896            0.837979            0.831815   \n",
       "18              0.878853            0.843340            0.845082   \n",
       "19              0.863173            0.863441            0.822970   \n",
       "20              0.845350            0.802466            0.857813   \n",
       "21              0.864916            0.874564            0.889440   \n",
       "22              0.830608            0.842938            0.869740   \n",
       "23              0.828598            0.863977            0.854329   \n",
       "24              0.877111            0.872018            0.865988   \n",
       "25              0.861431            0.866256            0.861297   \n",
       "26              0.871214            0.862637            0.855401   \n",
       "27              0.845484            0.858215            0.862235   \n",
       "28              0.855267            0.808630            0.833021   \n",
       "29              0.850844            0.888234            0.885821   \n",
       "...                  ...                 ...                 ...   \n",
       "1914            0.943447            0.942241            0.938354   \n",
       "1915            0.939560            0.943313            0.939024   \n",
       "1916            0.937684            0.944251            0.937952   \n",
       "1917            0.936210            0.942107            0.939158   \n",
       "1918            0.939158            0.928035            0.928571   \n",
       "1919            0.924819            0.933932            0.934066   \n",
       "1920            0.907531            0.904449            0.917716   \n",
       "1921            0.888234            0.893996            0.906593   \n",
       "1922            0.900965            0.904181            0.899625   \n",
       "1923            0.891718            0.896676            0.879389   \n",
       "1924            0.900831            0.884079            0.889172   \n",
       "1925            0.865452            0.829402            0.876977   \n",
       "1926            0.972795            0.980568            0.974538   \n",
       "1927            0.965023            0.966765            0.963683   \n",
       "1928            0.958188            0.960734            0.956044   \n",
       "1929            0.951756            0.950683            0.956848   \n",
       "1930            0.950683            0.950147            0.944251   \n",
       "1931            0.949477            0.943983            0.947735   \n",
       "1932            0.941169            0.940901            0.924417   \n",
       "1933            0.944921            0.928705            0.934066   \n",
       "1934            0.925891            0.934066            0.932726   \n",
       "1935            0.927231            0.924149            0.930314   \n",
       "1936            0.931922            0.920933            0.915438   \n",
       "1937            0.912490            0.912490            0.903779   \n",
       "1938            0.905253            0.905923            0.917984   \n",
       "1939            0.889306            0.914366            0.891048   \n",
       "1940            0.894398            0.883409            0.897213   \n",
       "1941            0.880863            0.888234            0.895872   \n",
       "1942            0.876709            0.831144            0.876173   \n",
       "1943            0.856205            0.864648            0.861833   \n",
       "\n",
       "      split8_train_score  split9_train_score  mean_train_score  \\\n",
       "0               0.860627            0.857947          0.852171   \n",
       "1               0.860225            0.797641          0.851233   \n",
       "2               0.848298            0.840391          0.842227   \n",
       "3               0.840257            0.859153          0.847682   \n",
       "4               0.871750            0.882605          0.858081   \n",
       "5               0.851648            0.885687          0.857679   \n",
       "6               0.824578            0.859689          0.851126   \n",
       "7               0.835835            0.813187          0.852117   \n",
       "8               0.811043            0.852586          0.849558   \n",
       "9               0.868936            0.824444          0.850750   \n",
       "10              0.894666            0.844412          0.850348   \n",
       "11              0.861029            0.852854          0.850804   \n",
       "12              0.841061            0.864782          0.852573   \n",
       "13              0.875503            0.851246          0.836773   \n",
       "14              0.842402            0.840123          0.844720   \n",
       "15              0.837711            0.802734          0.833141   \n",
       "16              0.860091            0.838649          0.853243   \n",
       "17              0.886090            0.846154          0.855535   \n",
       "18              0.852452            0.866524          0.856580   \n",
       "19              0.857947            0.855669          0.856138   \n",
       "20              0.880729            0.876173          0.857210   \n",
       "21              0.867328            0.854597          0.868252   \n",
       "22              0.876575            0.818413          0.850335   \n",
       "23              0.833557            0.862771          0.840056   \n",
       "24              0.876575            0.868668          0.867140   \n",
       "25              0.869338            0.862503          0.855481   \n",
       "26              0.858483            0.861297          0.851260   \n",
       "27              0.877111            0.848298          0.855856   \n",
       "28              0.870410            0.865318          0.851635   \n",
       "29              0.861029            0.865050          0.863629   \n",
       "...                  ...                 ...               ...   \n",
       "1914            0.940767            0.941437          0.944921   \n",
       "1915            0.949611            0.946663          0.941584   \n",
       "1916            0.944251            0.946127          0.941195   \n",
       "1917            0.939024            0.933262          0.938783   \n",
       "1918            0.937014            0.928169          0.927808   \n",
       "1919            0.918923            0.928437          0.925972   \n",
       "1920            0.891316            0.906459          0.910694   \n",
       "1921            0.906727            0.880729          0.900817   \n",
       "1922            0.905923            0.915974          0.905803   \n",
       "1923            0.883543            0.886090          0.887537   \n",
       "1924            0.879389            0.873492          0.881171   \n",
       "1925            0.871884            0.896542          0.866685   \n",
       "1926            0.977352            0.972661          0.977901   \n",
       "1927            0.955776            0.961806          0.960614   \n",
       "1928            0.955508            0.951488          0.957451   \n",
       "1929            0.950147            0.951488          0.953270   \n",
       "1930            0.945859            0.949209          0.947025   \n",
       "1931            0.945725            0.939292          0.941973   \n",
       "1932            0.927633            0.941169          0.935862   \n",
       "1933            0.942107            0.930046          0.933704   \n",
       "1934            0.941437            0.936880          0.933128   \n",
       "1935            0.932324            0.924685          0.932391   \n",
       "1936            0.927633            0.931922          0.924350   \n",
       "1937            0.923077            0.913026          0.913133   \n",
       "1938            0.920263            0.898687          0.909274   \n",
       "1939            0.909274            0.904717          0.905052   \n",
       "1940            0.883409            0.897079          0.888998   \n",
       "1941            0.883409            0.879657          0.882913   \n",
       "1942            0.888502            0.854061          0.870544   \n",
       "1943            0.846288            0.857277          0.859327   \n",
       "\n",
       "      std_train_score  \n",
       "0            0.023556  \n",
       "1            0.019532  \n",
       "2            0.024725  \n",
       "3            0.019071  \n",
       "4            0.020361  \n",
       "5            0.026431  \n",
       "6            0.013270  \n",
       "7            0.022857  \n",
       "8            0.018935  \n",
       "9            0.019105  \n",
       "10           0.028578  \n",
       "11           0.020440  \n",
       "12           0.020250  \n",
       "13           0.028017  \n",
       "14           0.028548  \n",
       "15           0.019739  \n",
       "16           0.010039  \n",
       "17           0.022286  \n",
       "18           0.012214  \n",
       "19           0.018406  \n",
       "20           0.022120  \n",
       "21           0.012163  \n",
       "22           0.021492  \n",
       "23           0.021895  \n",
       "24           0.016928  \n",
       "25           0.023349  \n",
       "26           0.014832  \n",
       "27           0.016656  \n",
       "28           0.023643  \n",
       "29           0.014066  \n",
       "...               ...  \n",
       "1914         0.004172  \n",
       "1915         0.007327  \n",
       "1916         0.003309  \n",
       "1917         0.004375  \n",
       "1918         0.006747  \n",
       "1919         0.006148  \n",
       "1920         0.009879  \n",
       "1921         0.010680  \n",
       "1922         0.006050  \n",
       "1923         0.007276  \n",
       "1924         0.010911  \n",
       "1925         0.017946  \n",
       "1926         0.003777  \n",
       "1927         0.004108  \n",
       "1928         0.003593  \n",
       "1929         0.002981  \n",
       "1930         0.006636  \n",
       "1931         0.004896  \n",
       "1932         0.007018  \n",
       "1933         0.007635  \n",
       "1934         0.006754  \n",
       "1935         0.005546  \n",
       "1936         0.006275  \n",
       "1937         0.008620  \n",
       "1938         0.007232  \n",
       "1939         0.009198  \n",
       "1940         0.011301  \n",
       "1941         0.007951  \n",
       "1942         0.018727  \n",
       "1943         0.013122  \n",
       "\n",
       "[1944 rows x 34 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimatorsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refit with the best params to use for ploting the tree viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorsdf.to_csv(r'C:\\Users\\kevinm\\Documents\\GitHub\\7333QTW\\Case Study 3\\estimators.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (revised model) BUILD/EVALUTE TREE-BASED MODEL TO PREDICT \"SPAM\"\n",
    "The grid search finds a model with very good accuracy of close to 98%. However, the model is very complex and has a depth of 120 branches. An evaluation of all the models evaluated shows that tree with a much smaller branch depth can produce a fairly accurate tree, with few branches.  This simpler tree can be used to describe some of the more important features of the data set in a more compact manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(**{'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 5})\n",
    "#clf.fit(X1_train,y1_train)\n",
    "#yhat = clf.predict(X1_test)\n",
    "#print ('accuracy:', mt.accuracy_score(y1_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will do a cv split of the data for testing and training 10 times and refit and evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "for fold, (train, test) in enumerate(cv.split(X,y)):\n",
    "    print ('Next Evaluation:)\n",
    "    # train the decision tree algorithm\n",
    "    clf.fit(X.iloc[train],y[train])\n",
    "    yhat = clf.predict(X.iloc[test])\n",
    "    print ('accuracy:', mt.accuracy_score(y[test],yhat),'\\n')\n",
    "    conf = mt.confusion_matrix(y[test],yhat)\n",
    "    print(\"confusion matrix\\n\",conf,'\\n')\n",
    "    print(\"Precision Score is: {}\" .format(precision_score(y[test],yhat, average='weighted')),'\\n')\n",
    "    print(\"Recall Score is: {}\" .format(recall_score(y[test],yhat, average='weighted')),'\\n')\n",
    "    print(\"F1 Score is: {}\" .format(f1_score(y[test],yhat, average='weighted')),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: WHICH VARIABLES \"MOST\" IMPORTANT?\n",
    "\n",
    "The features that are being considered in our model are:\n",
    "* perCaps\n",
    "* perHTML\n",
    "* bodyCharCt\n",
    "* forwards\n",
    "* numDlr\n",
    "* isInReplyTo\n",
    "* avgWordLen\n",
    "* isWrote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging field names and feature importances so they can be displayed together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(clf.feature_importances_, columns =['featimp'])\n",
    "featuresnames = pd.DataFrame(X1_test.columns.values.tolist(), columns =['fields'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featimpdf = pd.merge(featuresnames, fi, left_index=True, right_index=True)\n",
    "featimpdf = featimpdf.sort_values(by='featimp', ascending=False)\n",
    "featimpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.barplot(x=featimpdf.fields, y=featimpdf.featimp)\n",
    "ax.set_title('Feature Importance')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: PLOT/ANALYZE PATHS THROUGH TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain a path or two\n",
    "what did we learn?\n",
    "what do the splits tell us? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(clf);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = ['Spam', 'NotSpam']\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (3,3), dpi = 300)\n",
    "\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = features, \n",
    "               class_names=cn,\n",
    "               filled = True);\n",
    "# fig.savefig(r'C:\\Users\\jjsch\\Downloads\\Week_5_Materials_2\\plottreefncn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code will export the decsion tree first to a dot file, then reimport and export to PDF for easier zooming and viewing of the decsion tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to dot file\n",
    "tree.export_graphviz(clf,\n",
    "                     out_file=(r\"C:\\Users\\kevinm\\Documents\\GitHub\\7333QTW\\Case Study 3\\tree.dot\"),\n",
    "                               #(r\"C:\\Users\\jjsch\\Downloads\\Week_5_Materials_2\\tree.dot\"),\n",
    "                     feature_names = features, \n",
    "                     class_names=cn,\n",
    "                     filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from graphviz import Source\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotfile = StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pdf file and open\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=features)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"name of file\",view = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: EVALUATE PERFORMANCE OF MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix to show classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y1_test,yhat)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "#https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "\n",
    "ylist = Y.values.astype('int64')\n",
    "ylist\n",
    "from sklearn.preprocessing import label_binarize\n",
    "ybinary = label_binarize(ylist, classes=[0, 1])\n",
    "n_classes = ybinary.shape[1]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,ybinary, test_size=0.2)\n",
    "y_score = cross_val_predict(clf, X, ybinary, cv=10 ,method='predict')\n",
    "from sklearn.preprocessing import label_binarize\n",
    "ybinary = label_binarize(ylist, classes=[0, 1])\n",
    "n_classes = ybinary.shape[1]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,ybinary, test_size=0.2)\n",
    "y_score = cross_val_predict(clf, X, ybinary, cv=10 ,method='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1_train, X1_test, y1_train, y1_test\n",
    "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "\n",
    "    #y1_test,yhat    \n",
    "    fpr[i], tpr[i], _ = roc_curve(ybinary, y_score)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ybinary.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[i], tpr[i], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Spam')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY/CONCLUSION/FINAL THOUGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ROC curve, also known as a Reciever Operation Characteristic Curve is a plot that calculates and can be used to compare the classifiers we will used in our modeling along with the true positive rate (TPR) and false positive rate (FPR) they convey. From a topline level, you can use the Area Under the Curve (AUC) to help detirmine which can better pair an observation with the correct spam classification. An AUC score of 1.0 denotes a perfect classifier and an area of 0.5 represents a model is no better than a random guess.  \n",
    "\n",
    "In addition to the ROC-AUC value we will use to evaluate classifiers, we will be using other evaluators as well. \n",
    "Accuracy - Accuracy is the total number of correct predictions over the total number of predictions made. Accuracy will be plotted in our AUC curve and, while not perfect, is a good singlular measure to evaluate a model. Just be careful that we aren't overclassifying unbalalnced variables.   \n",
    "\n",
    "**Accuracy** = (TP + TN)/(TP + FP + FN + TN) \n",
    "\n",
    "**Precision** - Precision is the Proportions of true positives over the total number of positive outcomes whether accurately predicted or inaccurately predicted. Precision is useful to us because it will help us better understand which model is accurately picking correct classes and not resting on selecting incorrect classes.   \n",
    "Precision = (TP) / (TP + FP)  \n",
    "\n",
    "**Recall** - Also known as sensitivity, recall is the proportion of positive outcomes that were correctly classified by our model. Essentially it tells us how many values we incorrectly predicted while the precision can tell us more about what we correctly classified. Recall is good as a pair with precision, as it will help us tell if our model is overfit or selecting a single class and not training itself.   \n",
    "Recall/Sensitivity = (TP) / (TP + FN)  \n",
    "\n",
    "**F1 Score** - Another measure of accuracy that accounts for the true negatives and false positives. \n",
    "\n",
    "F1 score = 2(True Positive Rate * True Negatives)/(True Positives + True Negatives)  \n",
    "\n",
    "The aim of our modeling is to focus on maximizing our precision, recall, and accuracy scores in our models  \n",
    "\n",
    "Reference - https://github.com/jjschueder/7331DataMiningNotebooks/blob/master/lab2/Lab2_Daniel_Jeff_Armando_Joe(Final).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup evaluation of complex model\n",
    "The evaluation below shows cross validation of the deeper tree model with more branches.  Accuracy and other metrics are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf=DecisionTreeClassifier(**dt_clf.best_params_)\n",
    "dt_clf.fit(X1_train,y1_train)\n",
    "yhat = dt_clf.predict(X1_test)\n",
    "print ('accuracy:', mt.accuracy_score(y1_test,yhat))\n",
    "\n",
    "\n",
    "fi_dt = pd.DataFrame(dt_clf.feature_importances_, columns =['featimp'])\n",
    "featuresnames_dt = pd.DataFrame(X1_test.columns.values.tolist(), columns =['fields'])\n",
    "featimpdt_df = pd.merge(featuresnames_dt, fi_dt, left_index=True, right_index=True)\n",
    "featimpdt_df = featimpdf.sort_values(by='featimp', ascending=False)\n",
    "featimpdt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "for fold, (train, test) in enumerate(cv.split(X,y)):\n",
    "    print ('Next Evaluation:')\n",
    "    # train the decision tree algorithm\n",
    "    dt_clf.fit(X.iloc[train],y[train])\n",
    "    yhat = dt_clf.predict(X.iloc[test])\n",
    "    print ('accuracy:', mt.accuracy_score(y[test],yhat),'\\n')\n",
    "    conf = mt.confusion_matrix(y[test],yhat)\n",
    "    print(\"confusion matrix\\n\",conf,'\\n')\n",
    "    print(\"Precision Score is: {}\" .format(precision_score(y[test],yhat, average='weighted')),'\\n')\n",
    "    print(\"Recall Score is: {}\" .format(recall_score(y[test],yhat, average='weighted')),'\\n')\n",
    "    print(\"F1 Score is: {}\" .format(f1_score(y[test],yhat, average='weighted')),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "#https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "\n",
    "ylistd = Y.values.astype('int64')\n",
    "ylistd\n",
    "from sklearn.preprocessing import label_binarize\n",
    "ybinaryd = label_binarize(ylistd, classes=[0, 1])\n",
    "n_classes = ybinaryd.shape[1]\n",
    "X_train2d, X_test2d, y_train2d, y_test2d = train_test_split(X,ybinaryd, test_size=0.2)\n",
    "y_scored = cross_val_predict(dt_clf, X, ybinaryd, cv=10 ,method='predict')\n",
    "from sklearn.preprocessing import label_binarize\n",
    "ybinaryd = label_binarize(ylistd, classes=[0, 1])\n",
    "n_classes = ybinaryd.shape[1]\n",
    "X_train2d, X_test2d, y_train2d, y_test2d = train_test_split(X,ybinary, test_size=0.2)\n",
    "y_scored = cross_val_predict(dt_clf, X, ybinaryd, cv=10 ,method='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1_train, X1_test, y1_train, y1_test\n",
    "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fprd = dict()\n",
    "tprd = dict()\n",
    "roc_aucd = dict()\n",
    "for i in range(n_classes):\n",
    "\n",
    "#y1_test,yhat    \n",
    "    fprd[i], tprd[i], _ = roc_curve(ybinaryd, y_scored)\n",
    "    roc_aucd[i] = auc(fprd[i], tprd[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fprd[\"micro\"], tprd[\"micro\"], _ = roc_curve(ybinaryd.ravel(), y_scored.ravel())\n",
    "roc_aucd[\"micro\"] = auc(fprd[\"micro\"], tprd[\"micro\"])\n",
    "\n",
    "#Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fprd[i], tprd[i], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_aucd[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Spam')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
